{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Congress knowledge base. Right now these docs cover eight open source projects: cybernode - smart node manager and transaction crawler cyber-search - transaction parser for cybernode cyber-markets - toolchain for parsing of orders and trades cyb-js - cyber-search javascript library cyb - web3 browser chaingear - create your own Registry of general purpose entries on Ethereum blockchain cyberd - research on the cyber protocol cybercongress - community of of scientists, developers, engineers and craftsmen","title":"cyber\u2022Congress"},{"location":"contribute/","text":"Current wiki is built on top of mkdocs.org engine with Material for MkDocs extensions pack. Required Installations \u00b6 https://hub.docker.com/r/squidfunk/mkdocs-material/ Commands Cheat Sheet \u00b6 docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Contributing"},{"location":"contribute/#required-installations","text":"https://hub.docker.com/r/squidfunk/mkdocs-material/","title":"Required Installations"},{"location":"contribute/#commands-cheat-sheet","text":"docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs squidfunk/mkdocs-material gh-deploy","title":"Commands Cheat Sheet"},{"location":"contribute/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"cyb/app-guidelines/","text":"DApp guidelines \u00b6 @asadovka What is App? \u00b6 App in Cyb is a one or multi page application. It can be a simple html file as a frontend and may include smart contract logic as a backend. Apps should be available through IPFS hash. App structure \u00b6 Cyb CLI can automatically generate structure for your App. All you need to do is just type some commands, come up with a name and develop! App requirements \u00b6 We set limitations of computation resources usage for each app. less then 5% of CPU usage less then 10% of RAM usage App development \u00b6 Here we describe how to generate initial app structure. Install the latest version of CYB package npm install -g cyb Select the path where you want to store your app. For example /Desktop/My_apps cd username/Desktop/My_apps Enter the name and generate the structure of the app. A folder with all necessary components will be created cyb init appname Go to the app folder and link your app with your Cyb cyb link Open the Appstore in CYB. You will see your app on \u00abYour app\u00bb page You can also use our Help App (cyb://.help/create) for more info. App deployment \u00b6 When you are ready with development of your app you can easely publish it to the DApp Store so every user of Cyb will see it. Open your app, click on \"deploy\" button and enter IPFS hash of your app. Sign the transaction - your app will be registred in Chaingear. And that is it! App design \u00b6 App design process follows Web3 design principles. State rules \u00b6 We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green Data visualization \u00b6 Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead. Blockchain objects presenting \u00b6 Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.","title":"App guidelines"},{"location":"cyb/app-guidelines/#dapp-guidelines","text":"@asadovka","title":"DApp guidelines"},{"location":"cyb/app-guidelines/#what-is-app","text":"App in Cyb is a one or multi page application. It can be a simple html file as a frontend and may include smart contract logic as a backend. Apps should be available through IPFS hash.","title":"What is App?"},{"location":"cyb/app-guidelines/#app-structure","text":"Cyb CLI can automatically generate structure for your App. All you need to do is just type some commands, come up with a name and develop!","title":"App structure"},{"location":"cyb/app-guidelines/#app-requirements","text":"We set limitations of computation resources usage for each app. less then 5% of CPU usage less then 10% of RAM usage","title":"App requirements"},{"location":"cyb/app-guidelines/#app-development","text":"Here we describe how to generate initial app structure. Install the latest version of CYB package npm install -g cyb Select the path where you want to store your app. For example /Desktop/My_apps cd username/Desktop/My_apps Enter the name and generate the structure of the app. A folder with all necessary components will be created cyb init appname Go to the app folder and link your app with your Cyb cyb link Open the Appstore in CYB. You will see your app on \u00abYour app\u00bb page You can also use our Help App (cyb://.help/create) for more info.","title":"App development"},{"location":"cyb/app-guidelines/#app-deployment","text":"When you are ready with development of your app you can easely publish it to the DApp Store so every user of Cyb will see it. Open your app, click on \"deploy\" button and enter IPFS hash of your app. Sign the transaction - your app will be registred in Chaingear. And that is it!","title":"App deployment"},{"location":"cyb/app-guidelines/#app-design","text":"App design process follows Web3 design principles.","title":"App design"},{"location":"cyb/app-guidelines/#state-rules","text":"We use simple colored states for states of transactions or operations: mempool/failed - red confirmed/reverted - yellow finalized/succesful - green","title":"State rules"},{"location":"cyb/app-guidelines/#data-visualization","text":"Data should be visualised in a simple and attractive way. No overloaded plots an diagrams, we use animation instead.","title":"Data visualization"},{"location":"cyb/app-guidelines/#blockchain-objects-presenting","text":"Blockchain data is too complicated and sometimes not obvious for people. Thus we use adaptive tricks to make work process more convenient: Logical grouping for objects. Every app page has common groups of data (general, blockchain specific) for inheritance of views and better navigation or data observing. Classical accounting terms used for balance and cashflow operations. Blockahains use econimic principles for interaction between subjects thus we can describe such processes in established terms. Robohash logo for contracts entities. Contracts can act by themself, have and algorithms, so it's more natural to perceive them like robots instead of pieces of code.","title":"Blockchain objects presenting"},{"location":"cyb/cyb/","text":"Cyb: web3 browser \u00b6 @xhipster @asadovka Concept. Definitions are work in progress. Current implementation is not in comply with spec yet Abstract \u00b6 Cyb is a friendly software robot who helps you explore the universes. Put it simply it just a web3 browser. At the very beginning cyb is focusing on developers and advanced blockchain users who are able to work with private keys and transactions. But we see how Cyb becomes friendly for everyone who wants to interact with consensus computers in a web of the third generation. This new web is designed to free developers from outdated things such as html and v8. So developers can use any markup, execution and rendering engine they wish. That is why we don't focus on implementation of mentioned things. Instead this paper discuss implementation agnostic concepts of browser that are simple enough to be adopted by web3 developers. Initially we design Cyb for conventional desktop browsing. But suggested concepts can be easily used for mobile, voice, vr and robotics implementations. Introduction \u00b6 Current state of web3 experience is non satisfactory. Still we did not meet any piece of software that is able to deliver deep, emotional web3 experience. So we decide to bring to the table one contender that strictly follows web3 principles defined by ourselves ;-) In a rush for this passion we define the following web3 apps which we believe together implement the full web3 vision in the context of a browsing for web3 agents and app developers: .main : main page for every joe .path : navigation bar and its backend .connect : connection manager and state widget .keys : keystore interface and id widget .cyber : cyberd node manager and app for link chains .pins : favourites backend and application bar .sign : phishing resistant signer for messages and transactions with scheduler .crr : cyb root registry .cyb : origianl web3 appstore .access : permission manager that respects agents' resources .feed : notification backend and feed app .ipfs : ipfs node manager and agent experience .eth : ethereum node manager + ens resolver .wallet : universal wallet ux .help : cyb educational library and feedback mechanism .dev : web3 development tool with support of contracts .cg : all the things chaingearable .settings : cyb settings All this apps are considered as core apps and are included in every Cyb distribution. Let us describe in details every app as a pure concept. .main \u00b6 Purpose of the main app is to make agent happier in a moment it returns for surfing and between experiences. Main page of the browser consists of three main elements: search bar: provides all search functions relevance bar: the most relevant cyberlinks for a particular agent footer: cyberlinks to ecosystem resources which are important for education and contribution .path \u00b6 Navigation bar in Cyb is based on the following elements: back button - returns user to the previous state of web3 agent search bar - provides direct access to certain state star button - allow users to pin cyberlinks forward button - brings user to the future state based on Cyb prediction Search bar is used to browse web3. With the help of DURA with knowledge of application involved ( . ) it can get content across different content addressing protocols such as IPFS, DAT, SWARM, and inside blockchains, tangles and DAGs thus forming heterogeneous environment of web3. In web3 vision doc we describe in details a concept of web3 browsing based on DURA specs. That is, in web3 appending \"dot\" works very different in comparison with web2. Dot is literally a search query to a particular app that also has a content address in heterogeneous network. All symbols after \"dot\" make a map with content address of an app in root registry, and all data before \"dot\" is a query parameter to an app. <illustration> For example: .help query will open Cyb help app. chaingear.help will open chaingear info page in the help app. Query without \"dot\" will be automatically redirected to search in cyberd (Note: queries without dot is synonym to <your-query>.cyber ). Empty query always leads to the main page. . query returns a root registry that is being used by default in Cyb. <api-definition> All cyberlinks that was requested by agent can be accessed using path app that is integral part of Cyb experience. .connect \u00b6 In web3 all data has the state, so it become easier to navigate through it and make agent experience better. To be sure that you are working with actual state Cyb needs to manage connection to web3 providers. Our purpose is to build web3 browser that is agnostic from addressing, identity and consensus protocols. But currently we use ipfs, parity-light and cyberd nodes to show off possible experience at early stage of web3 development without necessity to connect to web3 provider at all (be your own web3 provider) for basic needs such as popular static content surfing and simple transfers of tokens. <illustration> Cyb is hiding all complexities of web3 connections under one colorful indicator that range from green to red. Ideally it works like indicator of internet connection we all used to see in smartphones. Connection indicator cyberlinked to a connect app that is integral part of id bar. It gives an ability for an agent to understand status of connections and chose web3 providers. <api-definition> Ultimate purpose of connect is to remove necessity of agents to manually switch between networks. Agent do not need to think about switching across networks. It is a goal of app developers and browser vendors to define an approach that allow seamless interaction during web3 experience with all network magic happens underneath. Cyb is developing in a way that allow async interactions with several peer-to-peer networks in an app context. .keys \u00b6 Purpose of id bar is to enable the concept of identity. Using identity an agent is able to authenticate messages and sign transactions in web3. Cyb assumes that an agent interacting with web3 is using active identity, but offers ability to change id of a signed transaction during signing. Agent understand which id is active using identicon. Cyb computes unique and deterministic identicons for every id, but offer agent to set any identicon for local pleasure. Clicking on id bar allow agent to choose active identity from a keys app. <illustration> Keys app is inherent component of id bar and embedded in Cyb. This app allows to store cryptographic secrets. Think of it as lastpass you don't need to trust that is able to compute different addresses, one time passwords and signatures in the context of an app. The following convention is used for keys : id: String, chainId: Number keystore: Promise <String> mnemonic: String derivationPath?: Promise <String> otherAddresses: Array <String> privateKey: String publicKey: Promise <String> type: String subtype: String The following API is being used to programmatically interact with id bar: setDefaultId(addressIndex: Number): Promise <Boolean> sign(transactionObject: Object): Promise <String> signMessage(messageObject: Object): Promise <String> verifyMessage(verificationObject: Object): Promise <Boolean> .cyber \u00b6 It happens then agent knows some content address but have no idea in which network it can be retrieved as well as what app can deal with it. That is why Cyb has default integration with cyber [CYBER] protocol. Cyb append .cyber app for all request without a dot. .cyber is an app that has simple interface to cyberd, which returns prediction of related cyberlinks thus agent can get required resource directly through peer-to-peer network. Cyb has a setting of default search engine, thus an agent can plug a search she wants. <api-definition> .pins \u00b6 App bar is a place where user can quickly get access to most used web3 objects. User can pin such objects by clicking on button \"favourite\" on navigation bar and then it will appear in app bar. Cyberlink manager is an attached app that allow agents to group and tag pins. .sign \u00b6 .sign allows users to sign messages and transactions in a way that brings web3 experience to the whole new level. Browser use embedded app for signing transactions so user can be always sure that transaction details are valid. In a web2 there is no inherent mechanism to be sure that overlay of an app is produced by a browser and not an app itself. Cyb solves this problem deterministically generating background and sound of overlay window in a way that an underlying app cannot know the seed for generating desired sound and visual pattern. The user need to remember its unique pattern once to safely interacting with different apps including not so trusted. Another problem we are approach to solve with .sign is deferred transactions. Cyb has its own address for which an agent can delegate some rights. Using this API app developer can create a logic that allow create and execute complex sequences of transactions client side. Since inception of Ethereum we sign thousands of transactions and miss even more. That is why we believe this feature is critical for awesome web3 experience. .crr \u00b6 According to 3 rules of root registry every developer can deliver best possible experience for their agents. That is why we want to mix the best from every word in our worlds in our implementation of root registry. To bring better user experience about 3k of records will be cybersquatted to align interests of existing app developers and agents who look for a beautiful, simple and trustful experience. .crr is a potpourri of the most well known concepts consolidated under one namespace! Let me introduce what is included in the shake: programming languages : up to 200 names common programs : up to 100 names tokens : Up to 1k names top-level domains : up to 300 names top english words : up to 1k names utf symbols : up to 300 names Initially cyber\u2022Congress will own all this cybersquatted records. In order to improve probability of adoption of .crr cyber\u2022Congres will distribute this names to original app developers based on proof of dns mechanism. That is, names for programming languages, common programs, tokens and top-level domains will be distributed based on a proof of dns according to verified registry produced by cyber\u2022Congress. Top english words and utf symbols will be distributed using competitions, grants and awards produced by cyber\u2022Congress. Initially unregistered names in .crr will be distributed under flat fee for cyb root registry owner in Chaingear. We are going to start from 1 ETH for every name and will see will it be enough to protect from abusive squatting or not. It is possible that we will switch to auction form of distribution in a future. .cyb \u00b6 Extension over .crr . Added fields: logo, tagline, manifest, meta, code, crr. As result it become suitable to be an app store for browser. Appstore treats pinned apps as installed if at leas one permission is granted. .access \u00b6 Permission management is of paramount importance in the process of safe application distribution. We want to improve upon 3 critical aspects of permission management in web: app authentication resource management dynamic permissions App authentication is hard in web2. You need somehow know the origin and this is practically hard in a face of government level adversaries, than you must compute hash of received file and compare it with a file hash received from origin. Due to practical complexity nobody do that. In web3 if you know that address is correct authentication is done automagically. That is why browser can easily verify that permission is granted for expected app and not malicious. Resource management was not in place. In web2 all permission systems was primarily build around a concept of granting access to a particular data which browser has access to. While this approach find itself useful it just not enough to run any application from untrusted developers. Computing resources has fundamental value now, thus must be carefully managed and metered. In web3 its weird that any untrusted app can eat all resources of a machine in no time. Moreover, if an application is executed in a sandbox all we need to feel ourselves safe (in addition to authenticated permissions) is ensure that app do not eat more resources than expected. Libraries that help app developers to mine some proof-of-work algorithms using visitor machine become ubiquitous. Practically that means that in addition to shity ads web2 users will experience even more worse web experience: greedy, slow and battery consuming apps are coming. The answer to this upcoming problem in a browser permission system which is able to produce bounds on apps consumption of fundamental resources such as cpu, gpu, ram, storage and broadband. We believe that resource management must be in the core of web3 application engine. We are currently doing research on how that can be implemented: containerisation seems to be low hanging fruit that can be embedded right into web experience. Permission affordances . Current permission systems are static in a sense that browser provide limited set of predefined apis. Cookies, location, camera, microphone, sound and notifications: that is very limited set of things browsers can afford. Permissions of a third party developers are not native for a browsers either. Browser just don't care about what data with which apps agent want to share. We ask ourselves what if a browser can ask apps what kind of permissions they can provide thus exposing this permission system to any other apps? We believe this approach will allow web3 developers provide experience inaccessible for previous architectures. .feed \u00b6 Notification panel displaying all pending transactions and web3 events corresponding to certain account. Settings button leads to settings page where user can manage connection to IPFS, Ethereum and Cyber nodes (local or remote ways). .ipfs \u00b6 This app is a third party app developed by IPFS Shipyard. This is very basic app for interacting with ipfs. .eth \u00b6 Simple app which ger DURI requests and route requests to ethereum node (contracts, transactions and blocks). Else resolve ENS. .wallet \u00b6 We believe that transfer of tokens is very basic experience in a web3 thus want to provide embedded in browser wallet app as soon as possible. Thus we consider either to develop our own bicycle or partner with some 3d party wallet developer. .dev \u00b6 Developers experience is critical for the whole web3 adoption. This app helps to develop and publish web3 apps. .cg \u00b6 Chaingear is an app that help developers create ethereum based CRUD databases. We believe it will help developers to adopt web3 easier the same they MySQL helped to site developers in the very beginning of web. .help \u00b6 Help is a two way help application. Using this app cyb helps agents to use itself. Using .help agents help Cyb evolve. As an open source project we are welcome for contributions. Gitcoin is an excellent instrument that we use for delegating tasks for community a processing payments for completed ones. We have our vision of how to develop browser and what kind of features develop first. But we give an opportunity for community to decide and vote with tokens what kind of browser we need to see in near future. Our product Chaingear is also made for this. We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page. .settings \u00b6 Cyb settings Saga on privacy and anonymity \u00b6 ... On censorship resistance \u00b6 ...","title":"Whitepaper"},{"location":"cyb/cyb/#cyb-web3-browser","text":"@xhipster @asadovka Concept. Definitions are work in progress. Current implementation is not in comply with spec yet","title":"Cyb: web3 browser"},{"location":"cyb/cyb/#abstract","text":"Cyb is a friendly software robot who helps you explore the universes. Put it simply it just a web3 browser. At the very beginning cyb is focusing on developers and advanced blockchain users who are able to work with private keys and transactions. But we see how Cyb becomes friendly for everyone who wants to interact with consensus computers in a web of the third generation. This new web is designed to free developers from outdated things such as html and v8. So developers can use any markup, execution and rendering engine they wish. That is why we don't focus on implementation of mentioned things. Instead this paper discuss implementation agnostic concepts of browser that are simple enough to be adopted by web3 developers. Initially we design Cyb for conventional desktop browsing. But suggested concepts can be easily used for mobile, voice, vr and robotics implementations.","title":"Abstract"},{"location":"cyb/cyb/#introduction","text":"Current state of web3 experience is non satisfactory. Still we did not meet any piece of software that is able to deliver deep, emotional web3 experience. So we decide to bring to the table one contender that strictly follows web3 principles defined by ourselves ;-) In a rush for this passion we define the following web3 apps which we believe together implement the full web3 vision in the context of a browsing for web3 agents and app developers: .main : main page for every joe .path : navigation bar and its backend .connect : connection manager and state widget .keys : keystore interface and id widget .cyber : cyberd node manager and app for link chains .pins : favourites backend and application bar .sign : phishing resistant signer for messages and transactions with scheduler .crr : cyb root registry .cyb : origianl web3 appstore .access : permission manager that respects agents' resources .feed : notification backend and feed app .ipfs : ipfs node manager and agent experience .eth : ethereum node manager + ens resolver .wallet : universal wallet ux .help : cyb educational library and feedback mechanism .dev : web3 development tool with support of contracts .cg : all the things chaingearable .settings : cyb settings All this apps are considered as core apps and are included in every Cyb distribution. Let us describe in details every app as a pure concept.","title":"Introduction"},{"location":"cyb/cyb/#main","text":"Purpose of the main app is to make agent happier in a moment it returns for surfing and between experiences. Main page of the browser consists of three main elements: search bar: provides all search functions relevance bar: the most relevant cyberlinks for a particular agent footer: cyberlinks to ecosystem resources which are important for education and contribution","title":".main"},{"location":"cyb/cyb/#path","text":"Navigation bar in Cyb is based on the following elements: back button - returns user to the previous state of web3 agent search bar - provides direct access to certain state star button - allow users to pin cyberlinks forward button - brings user to the future state based on Cyb prediction Search bar is used to browse web3. With the help of DURA with knowledge of application involved ( . ) it can get content across different content addressing protocols such as IPFS, DAT, SWARM, and inside blockchains, tangles and DAGs thus forming heterogeneous environment of web3. In web3 vision doc we describe in details a concept of web3 browsing based on DURA specs. That is, in web3 appending \"dot\" works very different in comparison with web2. Dot is literally a search query to a particular app that also has a content address in heterogeneous network. All symbols after \"dot\" make a map with content address of an app in root registry, and all data before \"dot\" is a query parameter to an app. <illustration> For example: .help query will open Cyb help app. chaingear.help will open chaingear info page in the help app. Query without \"dot\" will be automatically redirected to search in cyberd (Note: queries without dot is synonym to <your-query>.cyber ). Empty query always leads to the main page. . query returns a root registry that is being used by default in Cyb. <api-definition> All cyberlinks that was requested by agent can be accessed using path app that is integral part of Cyb experience.","title":".path"},{"location":"cyb/cyb/#connect","text":"In web3 all data has the state, so it become easier to navigate through it and make agent experience better. To be sure that you are working with actual state Cyb needs to manage connection to web3 providers. Our purpose is to build web3 browser that is agnostic from addressing, identity and consensus protocols. But currently we use ipfs, parity-light and cyberd nodes to show off possible experience at early stage of web3 development without necessity to connect to web3 provider at all (be your own web3 provider) for basic needs such as popular static content surfing and simple transfers of tokens. <illustration> Cyb is hiding all complexities of web3 connections under one colorful indicator that range from green to red. Ideally it works like indicator of internet connection we all used to see in smartphones. Connection indicator cyberlinked to a connect app that is integral part of id bar. It gives an ability for an agent to understand status of connections and chose web3 providers. <api-definition> Ultimate purpose of connect is to remove necessity of agents to manually switch between networks. Agent do not need to think about switching across networks. It is a goal of app developers and browser vendors to define an approach that allow seamless interaction during web3 experience with all network magic happens underneath. Cyb is developing in a way that allow async interactions with several peer-to-peer networks in an app context.","title":".connect"},{"location":"cyb/cyb/#keys","text":"Purpose of id bar is to enable the concept of identity. Using identity an agent is able to authenticate messages and sign transactions in web3. Cyb assumes that an agent interacting with web3 is using active identity, but offers ability to change id of a signed transaction during signing. Agent understand which id is active using identicon. Cyb computes unique and deterministic identicons for every id, but offer agent to set any identicon for local pleasure. Clicking on id bar allow agent to choose active identity from a keys app. <illustration> Keys app is inherent component of id bar and embedded in Cyb. This app allows to store cryptographic secrets. Think of it as lastpass you don't need to trust that is able to compute different addresses, one time passwords and signatures in the context of an app. The following convention is used for keys : id: String, chainId: Number keystore: Promise <String> mnemonic: String derivationPath?: Promise <String> otherAddresses: Array <String> privateKey: String publicKey: Promise <String> type: String subtype: String The following API is being used to programmatically interact with id bar: setDefaultId(addressIndex: Number): Promise <Boolean> sign(transactionObject: Object): Promise <String> signMessage(messageObject: Object): Promise <String> verifyMessage(verificationObject: Object): Promise <Boolean>","title":".keys"},{"location":"cyb/cyb/#cyber","text":"It happens then agent knows some content address but have no idea in which network it can be retrieved as well as what app can deal with it. That is why Cyb has default integration with cyber [CYBER] protocol. Cyb append .cyber app for all request without a dot. .cyber is an app that has simple interface to cyberd, which returns prediction of related cyberlinks thus agent can get required resource directly through peer-to-peer network. Cyb has a setting of default search engine, thus an agent can plug a search she wants. <api-definition>","title":".cyber"},{"location":"cyb/cyb/#pins","text":"App bar is a place where user can quickly get access to most used web3 objects. User can pin such objects by clicking on button \"favourite\" on navigation bar and then it will appear in app bar. Cyberlink manager is an attached app that allow agents to group and tag pins.","title":".pins"},{"location":"cyb/cyb/#sign","text":".sign allows users to sign messages and transactions in a way that brings web3 experience to the whole new level. Browser use embedded app for signing transactions so user can be always sure that transaction details are valid. In a web2 there is no inherent mechanism to be sure that overlay of an app is produced by a browser and not an app itself. Cyb solves this problem deterministically generating background and sound of overlay window in a way that an underlying app cannot know the seed for generating desired sound and visual pattern. The user need to remember its unique pattern once to safely interacting with different apps including not so trusted. Another problem we are approach to solve with .sign is deferred transactions. Cyb has its own address for which an agent can delegate some rights. Using this API app developer can create a logic that allow create and execute complex sequences of transactions client side. Since inception of Ethereum we sign thousands of transactions and miss even more. That is why we believe this feature is critical for awesome web3 experience.","title":".sign"},{"location":"cyb/cyb/#crr","text":"According to 3 rules of root registry every developer can deliver best possible experience for their agents. That is why we want to mix the best from every word in our worlds in our implementation of root registry. To bring better user experience about 3k of records will be cybersquatted to align interests of existing app developers and agents who look for a beautiful, simple and trustful experience. .crr is a potpourri of the most well known concepts consolidated under one namespace! Let me introduce what is included in the shake: programming languages : up to 200 names common programs : up to 100 names tokens : Up to 1k names top-level domains : up to 300 names top english words : up to 1k names utf symbols : up to 300 names Initially cyber\u2022Congress will own all this cybersquatted records. In order to improve probability of adoption of .crr cyber\u2022Congres will distribute this names to original app developers based on proof of dns mechanism. That is, names for programming languages, common programs, tokens and top-level domains will be distributed based on a proof of dns according to verified registry produced by cyber\u2022Congress. Top english words and utf symbols will be distributed using competitions, grants and awards produced by cyber\u2022Congress. Initially unregistered names in .crr will be distributed under flat fee for cyb root registry owner in Chaingear. We are going to start from 1 ETH for every name and will see will it be enough to protect from abusive squatting or not. It is possible that we will switch to auction form of distribution in a future.","title":".crr"},{"location":"cyb/cyb/#cyb","text":"Extension over .crr . Added fields: logo, tagline, manifest, meta, code, crr. As result it become suitable to be an app store for browser. Appstore treats pinned apps as installed if at leas one permission is granted.","title":".cyb"},{"location":"cyb/cyb/#access","text":"Permission management is of paramount importance in the process of safe application distribution. We want to improve upon 3 critical aspects of permission management in web: app authentication resource management dynamic permissions App authentication is hard in web2. You need somehow know the origin and this is practically hard in a face of government level adversaries, than you must compute hash of received file and compare it with a file hash received from origin. Due to practical complexity nobody do that. In web3 if you know that address is correct authentication is done automagically. That is why browser can easily verify that permission is granted for expected app and not malicious. Resource management was not in place. In web2 all permission systems was primarily build around a concept of granting access to a particular data which browser has access to. While this approach find itself useful it just not enough to run any application from untrusted developers. Computing resources has fundamental value now, thus must be carefully managed and metered. In web3 its weird that any untrusted app can eat all resources of a machine in no time. Moreover, if an application is executed in a sandbox all we need to feel ourselves safe (in addition to authenticated permissions) is ensure that app do not eat more resources than expected. Libraries that help app developers to mine some proof-of-work algorithms using visitor machine become ubiquitous. Practically that means that in addition to shity ads web2 users will experience even more worse web experience: greedy, slow and battery consuming apps are coming. The answer to this upcoming problem in a browser permission system which is able to produce bounds on apps consumption of fundamental resources such as cpu, gpu, ram, storage and broadband. We believe that resource management must be in the core of web3 application engine. We are currently doing research on how that can be implemented: containerisation seems to be low hanging fruit that can be embedded right into web experience. Permission affordances . Current permission systems are static in a sense that browser provide limited set of predefined apis. Cookies, location, camera, microphone, sound and notifications: that is very limited set of things browsers can afford. Permissions of a third party developers are not native for a browsers either. Browser just don't care about what data with which apps agent want to share. We ask ourselves what if a browser can ask apps what kind of permissions they can provide thus exposing this permission system to any other apps? We believe this approach will allow web3 developers provide experience inaccessible for previous architectures.","title":".access"},{"location":"cyb/cyb/#feed","text":"Notification panel displaying all pending transactions and web3 events corresponding to certain account. Settings button leads to settings page where user can manage connection to IPFS, Ethereum and Cyber nodes (local or remote ways).","title":".feed"},{"location":"cyb/cyb/#ipfs","text":"This app is a third party app developed by IPFS Shipyard. This is very basic app for interacting with ipfs.","title":".ipfs"},{"location":"cyb/cyb/#eth","text":"Simple app which ger DURI requests and route requests to ethereum node (contracts, transactions and blocks). Else resolve ENS.","title":".eth"},{"location":"cyb/cyb/#wallet","text":"We believe that transfer of tokens is very basic experience in a web3 thus want to provide embedded in browser wallet app as soon as possible. Thus we consider either to develop our own bicycle or partner with some 3d party wallet developer.","title":".wallet"},{"location":"cyb/cyb/#dev","text":"Developers experience is critical for the whole web3 adoption. This app helps to develop and publish web3 apps.","title":".dev"},{"location":"cyb/cyb/#cg","text":"Chaingear is an app that help developers create ethereum based CRUD databases. We believe it will help developers to adopt web3 easier the same they MySQL helped to site developers in the very beginning of web.","title":".cg"},{"location":"cyb/cyb/#help","text":"Help is a two way help application. Using this app cyb helps agents to use itself. Using .help agents help Cyb evolve. As an open source project we are welcome for contributions. Gitcoin is an excellent instrument that we use for delegating tasks for community a processing payments for completed ones. We have our vision of how to develop browser and what kind of features develop first. But we give an opportunity for community to decide and vote with tokens what kind of browser we need to see in near future. Our product Chaingear is also made for this. We use user's feedback to make products better. So we provide options for bug reporting and feedback leaving on every page.","title":".help"},{"location":"cyb/cyb/#settings","text":"Cyb settings","title":".settings"},{"location":"cyb/cyb/#saga-on-privacy-and-anonymity","text":"...","title":"Saga on privacy and anonymity"},{"location":"cyb/cyb/#on-censorship-resistance","text":"...","title":"On censorship resistance"},{"location":"cyb/web3-vision/","text":"DURA: a missing piece for web3 \u00b6 @xhipster cyberCongress Early draft for web3 summit. Looking for a feedback . Abstract \u00b6 Originally an idea of web3 was inspired by Gavin Wood in 2014. A vision of Gavin was around 4 implementable concepts: content addressing, cryptographic identities, consensus computing and browsers. In parallel an idea of Interplanetary File System has been developed by Juan Benet. IPFS creates a foundation for web3: a system of content addressing and cryptographic identities. Since 2014 consensus computing has suffered insanely rapid development so one more missing piece is also in place. Still missing piece is a web3 browsing. Some projects such as Metamask has demonstrated a taste of web3. But one critical component in terms of browsing is just not there. URL scheme is outdated in terms of desired web3 properties and needs a drop in replacement. In this paper firstly we discuss necessary properties that we expect from web3. Based on this analysis we propose DURA scheme aka Distributed Unified Resource Address as drop in replacement for URLs that is being implemented in web3 browser Cyb . We believe DURA is a dump enough scheme (your captain) which can bring up basic consensus across web3 browser vendors due to simplicity, openness and protocol agnostic approach. Introduction \u00b6 Conventional protocols of the Internet such as TCP/IP, DNS, HTTPS and URL brought a web into the point there it is now. Along with all benefits they has created this protocols brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network growth and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets. This fact is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols: into very core of web3. Speed \u00b6 Usability researches state that interactions that do not make sense in 100 milliseconds are considered as slow by an agent. Achieving such instant speeds is nearly impossible in the current stack of protocols. The following generation of the web must enable instant responses user requests. Necessity for lookup a location of resource using remote machine is an obvious bottleneck for reaching desired properties. Globality \u00b6 Current internet is starting to split into regions mutually inaccessible for each other. China is de facto such a region. Some countries are very close to joining the club. It is of paramount importance that web3 would remain global even in the face of government level adversaries. Security \u00b6 Current web applications are still in its infancy in term of permission abilities. Security of web apps is a very complicated topic. But one thing is obvious: it is hard to setup a secure system with third party apps without built in authentication system of the code being run on client machine. Current system of mutable resource location based on certificate authorities can not be safe by design. Permanent \u00b6 We are all experience broken links. IPFS has immunity to this issue. As long as you keep a file anybody can access it using globally defined immutable in time address of this file computed from the file itself. Mesh ready \u00b6 Current internet paradigm is based on 1 internet provider paradigm. That is basically a bad shit, because in general even if you have 2 or more internet connection like wifi and lte your device and/or operation system don't allow you to get the full possibilities of connectivity enforcing you to use only one connection at a time. Another major bad shit in current internet architecture is that your device is treated as leech by default. Every device keeps data necessary to being useful for surround devices. Huge portion of our network traffic goes not from origin server but from ISP cache. That means that changing a paradigm we can get to very different topology where our neighbors are our web3 providers. Verifiability \u00b6 ... Privacy and Anonymity \u00b6 ... Offline \u00b6 ... Realtime \u00b6 ... Roles \u00b6 In a web of the third generation roles are not like in a web2. There is no clear split on users, internet providers and sites. Key difference is such that interactions can happen truly peer to peer the one can be sovereign enough to be their own internet. Apps. Any content hash can be an app if it is known how to parse it. Agents. An app can become an agent if she can prove that she exist by digital signature. Providers. Any agent that is able to serve content can be web3 provider. Content addresses \u00b6 To understand why they so important we need to understand a difference in foundational concept of web2 and web3. Web2: Where => What-How : You must know resource location on a particular server to retrieve it. Web3: What => How : Instead of location based paradigm web3 is based on the content addressing paradigm. Key point is that we do not need to have knowledge of resource location in order to link to an object. In a web3 the answer to the How is either local or blockchain based registry with simple map between input address and address of an app. Cryptographic identities \u00b6 ... Consensus computers \u00b6 There are some very exhaustive articles around the topic of web3 which are really about consensus computing part of it. Worth to note that saying that any particular blockchain or even all blockchains altogether is web3 is like saying that databases is world wide web. Yes databases technology contributed to a development of www, but without several protocols this databases would not become interconnected through billions of web sites. Remember that in order to implement a vision of full web3 potential we need to find drop in replacement for every piece of current protocol stack: IP, TCP, HTTPS, DNS and URL. None of currently deployed blockchain technologies don't have necessary properties to directly replace dinosaurs. I would say that distributed ledger technology is a better database stack for the upcoming web. DURA scheme \u00b6 Distributed Unified Resource Address or put simply DURA is a more simpler and trustful scheme that has been used in a conventional web. It doesn't requires central authorities such as ICANN or others: [local-handler]://[content-address].[root-registry-app]/[app-navigation] We believe that a term cyberlink can be used for DURA links in order to differentiate with hyperlinks of previous internet architecture. Local handler \u00b6 dura:// It is a local handler that every os can handle. Being fully optional it can be very important in the very beginning of web3. Content address \u00b6 dura://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa If a browser is able to understand in which network she can resolve this particular content hash it can resolve it without much ado. Though we expect that IPFS will be not the only system and it become practically hard to say with 100% certainty whether given hash is ipfs hash or swarm or torrent or some other address type. That is there the concept of a root registry came into the game. In some sense it serves as an alternative to a self describing scheme used in CIDs. In some sense its not as it offer visually more sound links for agents. We believe that the root registry and self description concepts are complementary to each other. Root registry \u00b6 Current state of DNS root management is outdated. The most bad thing is that we still must to trust the most important things in our lives to strange organizations such as ICANN, IANA. We can ask ourselves why after 40 years of ubiquitous computer movement we still don't have simple common knowledge about what file extensions must be used with what software? Our proposal is a concept of a root register. Structure of root register is a simple map between short name and ipfs hash of a program that is being triggered: com:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa io:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa exe:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa pdf:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa eth:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa A name must be letter, number or hypen. Purpose of a root register is to reach some very basic os and network agnostic agreement about what extensions with what programs must be used. Of course the problem with such registry is that it must be somehow and somewhere maintained. Three rules of a root registry \u00b6 Software vendors must compete for a better root registry. Software vendors must add setting with a change of a root registry. Agents of browsers and operation systems must have ability to overwrite maps for local pleasure. One of the implementation is a cyb root registry Root registry app \u00b6 cyb://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa.ipfs This is an example of full DURA link that is able get hash of ipfs app in a root registry, than throw predicate into this app and get a desired resource from a peer to peer network. Note: need to add real case of ipfs DURA app being used in Cyb. Note: need to add real case on how web2 link can be resolved using DURA approach. App navigation \u00b6 Traditionally its up to developers how to structure navigation within an app. But we believe that some scheme will be invented to differentiate statefull and stateless links as it is very important for DURA extensions \u00b6 A lot of cool stuff can be implemented using extensions. Extension is any symbol that adds predictable an logical behavior for parsing and rendering of requests. Examples: - local paths - connection links - URL parameters Key principle is that semantics is programmable and can be delivered from a distributed network using symbols that has been registered in a root registry. Where extension \u00b6 Keyword: - / before content address is a local path Examples: /users/xhipster/cybernode.ai /ethereum/cybersearch.eth Linkchain extension \u00b6 Keyword: - @ between DURA statements Examples: xhipster.eth@cybercongress.ai Need to find cyberlinks between two cyberlinks. Reference \u00b6 ...","title":"Web3 vision"},{"location":"cyb/web3-vision/#dura-a-missing-piece-for-web3","text":"@xhipster cyberCongress Early draft for web3 summit. Looking for a feedback .","title":"DURA: a missing piece for web3"},{"location":"cyb/web3-vision/#abstract","text":"Originally an idea of web3 was inspired by Gavin Wood in 2014. A vision of Gavin was around 4 implementable concepts: content addressing, cryptographic identities, consensus computing and browsers. In parallel an idea of Interplanetary File System has been developed by Juan Benet. IPFS creates a foundation for web3: a system of content addressing and cryptographic identities. Since 2014 consensus computing has suffered insanely rapid development so one more missing piece is also in place. Still missing piece is a web3 browsing. Some projects such as Metamask has demonstrated a taste of web3. But one critical component in terms of browsing is just not there. URL scheme is outdated in terms of desired web3 properties and needs a drop in replacement. In this paper firstly we discuss necessary properties that we expect from web3. Based on this analysis we propose DURA scheme aka Distributed Unified Resource Address as drop in replacement for URLs that is being implemented in web3 browser Cyb . We believe DURA is a dump enough scheme (your captain) which can bring up basic consensus across web3 browser vendors due to simplicity, openness and protocol agnostic approach.","title":"Abstract"},{"location":"cyb/web3-vision/#introduction","text":"Conventional protocols of the Internet such as TCP/IP, DNS, HTTPS and URL brought a web into the point there it is now. Along with all benefits they has created this protocols brought more problem into the table. Globality being a key property of the the web since inception is under real threat. Speed of connections degrade with network growth and from ubiquitous government interventions into privacy and security of web users. One property, not obvious in the beginning, become really important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time have pass. Reliance on one at a time internet service provider architecture allow governments censor packets. This fact is the last straw in conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have ability to work with the state it has and after acquiring connection being able to sync with global state and continue verify state's validity in realtime while having connection. Now this properties offered on app level while such properties must be integrated into lower level protocols: into very core of web3.","title":"Introduction"},{"location":"cyb/web3-vision/#speed","text":"Usability researches state that interactions that do not make sense in 100 milliseconds are considered as slow by an agent. Achieving such instant speeds is nearly impossible in the current stack of protocols. The following generation of the web must enable instant responses user requests. Necessity for lookup a location of resource using remote machine is an obvious bottleneck for reaching desired properties.","title":"Speed"},{"location":"cyb/web3-vision/#globality","text":"Current internet is starting to split into regions mutually inaccessible for each other. China is de facto such a region. Some countries are very close to joining the club. It is of paramount importance that web3 would remain global even in the face of government level adversaries.","title":"Globality"},{"location":"cyb/web3-vision/#security","text":"Current web applications are still in its infancy in term of permission abilities. Security of web apps is a very complicated topic. But one thing is obvious: it is hard to setup a secure system with third party apps without built in authentication system of the code being run on client machine. Current system of mutable resource location based on certificate authorities can not be safe by design.","title":"Security"},{"location":"cyb/web3-vision/#permanent","text":"We are all experience broken links. IPFS has immunity to this issue. As long as you keep a file anybody can access it using globally defined immutable in time address of this file computed from the file itself.","title":"Permanent"},{"location":"cyb/web3-vision/#mesh-ready","text":"Current internet paradigm is based on 1 internet provider paradigm. That is basically a bad shit, because in general even if you have 2 or more internet connection like wifi and lte your device and/or operation system don't allow you to get the full possibilities of connectivity enforcing you to use only one connection at a time. Another major bad shit in current internet architecture is that your device is treated as leech by default. Every device keeps data necessary to being useful for surround devices. Huge portion of our network traffic goes not from origin server but from ISP cache. That means that changing a paradigm we can get to very different topology where our neighbors are our web3 providers.","title":"Mesh ready"},{"location":"cyb/web3-vision/#verifiability","text":"...","title":"Verifiability"},{"location":"cyb/web3-vision/#privacy-and-anonymity","text":"...","title":"Privacy and Anonymity"},{"location":"cyb/web3-vision/#offline","text":"...","title":"Offline"},{"location":"cyb/web3-vision/#realtime","text":"...","title":"Realtime"},{"location":"cyb/web3-vision/#roles","text":"In a web of the third generation roles are not like in a web2. There is no clear split on users, internet providers and sites. Key difference is such that interactions can happen truly peer to peer the one can be sovereign enough to be their own internet. Apps. Any content hash can be an app if it is known how to parse it. Agents. An app can become an agent if she can prove that she exist by digital signature. Providers. Any agent that is able to serve content can be web3 provider.","title":"Roles"},{"location":"cyb/web3-vision/#content-addresses","text":"To understand why they so important we need to understand a difference in foundational concept of web2 and web3. Web2: Where => What-How : You must know resource location on a particular server to retrieve it. Web3: What => How : Instead of location based paradigm web3 is based on the content addressing paradigm. Key point is that we do not need to have knowledge of resource location in order to link to an object. In a web3 the answer to the How is either local or blockchain based registry with simple map between input address and address of an app.","title":"Content addresses"},{"location":"cyb/web3-vision/#cryptographic-identities","text":"...","title":"Cryptographic identities"},{"location":"cyb/web3-vision/#consensus-computers","text":"There are some very exhaustive articles around the topic of web3 which are really about consensus computing part of it. Worth to note that saying that any particular blockchain or even all blockchains altogether is web3 is like saying that databases is world wide web. Yes databases technology contributed to a development of www, but without several protocols this databases would not become interconnected through billions of web sites. Remember that in order to implement a vision of full web3 potential we need to find drop in replacement for every piece of current protocol stack: IP, TCP, HTTPS, DNS and URL. None of currently deployed blockchain technologies don't have necessary properties to directly replace dinosaurs. I would say that distributed ledger technology is a better database stack for the upcoming web.","title":"Consensus computers"},{"location":"cyb/web3-vision/#dura-scheme","text":"Distributed Unified Resource Address or put simply DURA is a more simpler and trustful scheme that has been used in a conventional web. It doesn't requires central authorities such as ICANN or others: [local-handler]://[content-address].[root-registry-app]/[app-navigation] We believe that a term cyberlink can be used for DURA links in order to differentiate with hyperlinks of previous internet architecture.","title":"DURA scheme"},{"location":"cyb/web3-vision/#local-handler","text":"dura:// It is a local handler that every os can handle. Being fully optional it can be very important in the very beginning of web3.","title":"Local handler"},{"location":"cyb/web3-vision/#content-address","text":"dura://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa If a browser is able to understand in which network she can resolve this particular content hash it can resolve it without much ado. Though we expect that IPFS will be not the only system and it become practically hard to say with 100% certainty whether given hash is ipfs hash or swarm or torrent or some other address type. That is there the concept of a root registry came into the game. In some sense it serves as an alternative to a self describing scheme used in CIDs. In some sense its not as it offer visually more sound links for agents. We believe that the root registry and self description concepts are complementary to each other.","title":"Content address"},{"location":"cyb/web3-vision/#root-registry","text":"Current state of DNS root management is outdated. The most bad thing is that we still must to trust the most important things in our lives to strange organizations such as ICANN, IANA. We can ask ourselves why after 40 years of ubiquitous computer movement we still don't have simple common knowledge about what file extensions must be used with what software? Our proposal is a concept of a root register. Structure of root register is a simple map between short name and ipfs hash of a program that is being triggered: com:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa io:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa exe:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa pdf:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa eth:QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa A name must be letter, number or hypen. Purpose of a root register is to reach some very basic os and network agnostic agreement about what extensions with what programs must be used. Of course the problem with such registry is that it must be somehow and somewhere maintained.","title":"Root registry"},{"location":"cyb/web3-vision/#three-rules-of-a-root-registry","text":"Software vendors must compete for a better root registry. Software vendors must add setting with a change of a root registry. Agents of browsers and operation systems must have ability to overwrite maps for local pleasure. One of the implementation is a cyb root registry","title":"Three rules of a root registry"},{"location":"cyb/web3-vision/#root-registry-app","text":"cyb://QmQLXHs7K98JNQdWrBB2cQLJahPhmupbDjRuH1b9ibmwVa.ipfs This is an example of full DURA link that is able get hash of ipfs app in a root registry, than throw predicate into this app and get a desired resource from a peer to peer network. Note: need to add real case of ipfs DURA app being used in Cyb. Note: need to add real case on how web2 link can be resolved using DURA approach.","title":"Root registry app"},{"location":"cyb/web3-vision/#app-navigation","text":"Traditionally its up to developers how to structure navigation within an app. But we believe that some scheme will be invented to differentiate statefull and stateless links as it is very important for","title":"App navigation"},{"location":"cyb/web3-vision/#dura-extensions","text":"A lot of cool stuff can be implemented using extensions. Extension is any symbol that adds predictable an logical behavior for parsing and rendering of requests. Examples: - local paths - connection links - URL parameters Key principle is that semantics is programmable and can be delivered from a distributed network using symbols that has been registered in a root registry.","title":"DURA extensions"},{"location":"cyb/web3-vision/#where-extension","text":"Keyword: - / before content address is a local path Examples: /users/xhipster/cybernode.ai /ethereum/cybersearch.eth","title":"Where extension"},{"location":"cyb/web3-vision/#linkchain-extension","text":"Keyword: - @ between DURA statements Examples: xhipster.eth@cybercongress.ai Need to find cyberlinks between two cyberlinks.","title":"Linkchain extension"},{"location":"cyb/web3-vision/#reference","text":"...","title":"Reference"},{"location":"cyb-js/cyb-js/","text":"Cyb-js will come soon \u00b6","title":"Overview"},{"location":"cyb-js/cyb-js/#cyb-js-will-come-soon","text":"","title":"Cyb-js will come soon"},{"location":"cyber-markets/cyber-markets/","text":"About Markets \u00b6","title":"Overview"},{"location":"cyber-markets/cyber-markets/#about-markets","text":"","title":"About Markets"},{"location":"cyber-markets/api/Readme/","text":"Build and Run docs locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Readme"},{"location":"cyber-markets/api/Readme/#build-and-run-docs-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run docs locally"},{"location":"cyber-markets/components/overview/","text":"Markets Components \u00b6 Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Exchanges Connector \u00b6 Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka. Storer \u00b6 Writes data from kafka topics or directly from exchanges-connector to cassandra cluster Tickers real-time \u00b6 Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped. Tickers historical \u00b6 Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Overview"},{"location":"cyber-markets/components/overview/#markets-components","text":"Component Scale Status Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back Exchanges Connector 1 - N Connect to CEXs/DEXs for raw data 8080:/actuator/metrics Tickers real-time 1 - N Calculate Tickers From Raw real-time data Tickers historical 1 Calculate Tickers From Raw historical data Storer 1 Write raw data to database Markets REST API 1 - N Rest Api To Markets Entities y Markets Stream API 1 - N not implemented Web Socket Api To Markets Entities y Markets Api Docs 1 - N Markets Api Docs Based On Swagger y","title":"Markets Components"},{"location":"cyber-markets/components/overview/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-markets/components/overview/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-markets/components/overview/#exchanges-connector","text":"Collect raw data from centralized and decentralized exchanges such as trades, order books and put it to kafka.","title":"Exchanges Connector"},{"location":"cyber-markets/components/overview/#storer","text":"Writes data from kafka topics or directly from exchanges-connector to cassandra cluster","title":"Storer"},{"location":"cyber-markets/components/overview/#tickers-real-time","text":"Calculate tickers from trades which collecting exchanges connectors module. By default, the recalculation occurs every 3 seconds. Trades that do not hit the gap in 3 seconds are skipped.","title":"Tickers real-time"},{"location":"cyber-markets/components/overview/#tickers-historical","text":"Calculate tickers, prices, etc and aggregate raw data which collecting exchanges connectors module. Tickers are calculated from data which stored in the Elassandra.","title":"Tickers historical"},{"location":"cyber-markets/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-markets docker rm fast-data-dev-markets Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-markets docker rm elassandra-markets Chains \u00b6 Run parity node(cheat sheet) \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Cheat-sheet"},{"location":"cyber-markets/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-markets docker rm fast-data-dev-markets","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-markets/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-markets docker rm elassandra-markets","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-markets/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-markets/contributing/cheat-sheet/#run-parity-nodecheat-sheet","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node(cheat sheet)"},{"location":"cyber-markets/contributing/contributing/","text":"Contributing to Cyber Markets \u00b6 Thank you for considering a contribution to Cyber Markets! This guide explains how to: * Get started * Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. * Detekt code analyze tool should not report any issues * JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing"},{"location":"cyber-markets/contributing/contributing/#contributing-to-cyber-markets","text":"Thank you for considering a contribution to Cyber Markets! This guide explains how to: * Get started * Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Markets"},{"location":"cyber-markets/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-markets/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-markets/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-markets/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-markets/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. * Detekt code analyze tool should not report any issues * JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-markets/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-markets/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-markets/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-markets/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-markets/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d Bootstrap Elassandra with keyspaces(required) \u00b6 docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Exchanges Connector, Tickers, or API from intellij Idea \u00b6 Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Dev-environment"},{"location":"cyber-markets/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-markets/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-markets/contributing/dev-environment/#prestart","text":"Instal Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-markets/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-markets/contributing/dev-environment/#start-containersrequired","text":"For mac: docker-compose -f dev-environment/env-mac.yml up -d For linux family: docker-compose -f dev-environment/env.yml up -d","title":"Start containers(required)"},{"location":"cyber-markets/contributing/dev-environment/#bootstrap-elassandra-with-keyspacesrequired","text":"docker cp dev-environment/elassandra-bootstrap.cql elassandra-markets:/elassandra-bootstrap.cql docker exec -it elassandra-markets bash cqlsh -f elassandra-bootstrap.cql","title":"Bootstrap Elassandra with keyspaces(required)"},{"location":"cyber-markets/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-markets/contributing/dev-environment/#run-exchanges-connector-tickers-or-api-from-intellij-idea","text":"Go to ExchangesConnectorApplication.kt and press green triangle on left to the code (on example line 16): If, you use parity endpoint different rather that localhost:8545, than Etherdelta connector will fail due to lack of environment property PARITY_URL. Let's define it: Select \"Edit Configurations\" Add next properties: Now, run exchanges connector one more time then etherdelta connector should start. You can add environment variables in the same way for Tickers, APIs and etc.","title":"Run Exchanges Connector, Tickers, or API from intellij Idea"},{"location":"cyber-search/cyber-search/","text":"About Search \u00b6 \ud83d\ude80 Toolchain for transactions parsing and processing!!!!","title":"About Search"},{"location":"cyber-search/cyber-search/#about-search","text":"\ud83d\ude80 Toolchain for transactions parsing and processing!!!!","title":"About Search"},{"location":"cyber-search/api/","text":"Build and Run locally \u00b6 docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Readme"},{"location":"cyber-search/api/#build-and-run-locally","text":"docker build -t build/raw-api -f ./docs/api/Dockerfile ./ && docker run -p 8080 :8080 build/raw-api","title":"Build and Run locally"},{"location":"cyber-search/components/bitcoin-components/","text":"Bitcoin Components \u00b6 Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Bitcoin Pump \u00b6 Pumps Bitcoin raw data(block,tx,uncles) into Kafka. Bitcoin Cassandra Dump \u00b6 Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Bitcoin Contract Summary \u00b6 Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-components","text":"Component Scale Description Cluster Address Metrics External Bitcoin Pump 1 Bitcoin Chain Data Kafka Pump 8080:/actuator/metrics Bitcoin Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Bitcoin Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Bitcoin Components"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-pump","text":"Pumps Bitcoin raw data(block,tx,uncles) into Kafka.","title":"Bitcoin Pump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-cassandra-dump","text":"Consumes Bitcoin raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Bitcoin Cassandra Dump"},{"location":"cyber-search/components/bitcoin-components/#bitcoin-contract-summary","text":"Collects summary for each contract stored in Bitcoin chain. Consumes Kafka topics and update data in Elassandra.","title":"Bitcoin Contract Summary"},{"location":"cyber-search/components/cassandra-service/","text":"Cassandra Service \u00b6 Usage of cassandra-service Module \u00b6 \u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable Specifying CHAIN_FAMILY \u00b6 When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules. Without CHAIN_FAMILY \u00b6 Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Cassandra"},{"location":"cyber-search/components/cassandra-service/#cassandra-service","text":"","title":"Cassandra Service"},{"location":"cyber-search/components/cassandra-service/#usage-of-cassandra-service-module","text":"\u0421assandra service module could be used in two ways: With specifying CHAIN_FAMILY environment variable Without specifying CHAIN_FAMILY environment variable","title":"Usage of cassandra-service Module"},{"location":"cyber-search/components/cassandra-service/#specifying-chain_family","text":"When you're specifying CHAIN_FAMILY environment variable cassandra-service module will work in the context of one keyspace based on this family. For example if CHAIN_FAMILY=BITCOIN it will create bitcoin keyspace if it doesn't exists and do all migrations for this keyspace if they're not applied. In spring context only repositories beans related to this keyspace will be available All of the cassandra work handled by CassandraRepositoriesConfiguration and it specific inheritor. This kind of interaction with cassandra-service used in dumps and contract summaries modules.","title":"Specifying CHAIN_FAMILY"},{"location":"cyber-search/components/cassandra-service/#without-chain_family","text":"Without specifying CHAIN_FAMILY environment variable cassandra-service module will act like this: It will run no migrations and creations of keyspaces It will look on available keyspaces in current cassandra database based on SearchRepositoriesConfiguration inheritors and if they're exists it will initialize all repositories beans for this keyspaces and put them into spring context. This is useful when you want to interact with all of available keyspaces and indicies. For example in search-api module we need all of repositories beans to build requests.","title":"Without CHAIN_FAMILY"},{"location":"cyber-search/components/custom-chain-name/","text":"Custom Chain Name \u00b6 You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom chain name"},{"location":"cyber-search/components/custom-chain-name/#custom-chain-name","text":"You have an ability to run our services with custom chain name based on supported chain families. It may be needed if you have your own custom chain and you want API endpoints, Kafka topic names and Cassandra keyspaces to use your chain name. Currently we're supporting BITCOIN and ETHEREUM chain families. To make it work you need to define CHAIN_NAME environment variable along with CHAIN_FAMILY . Also you may need to define your chain node url in PUMP by setting CHAIN_NODE_URL environment variable. For example if you want to index ETHEREUM based chain called MY_PRECIOUS . You have to use following config in all services ( CHAIN_NODE_URL is necessary only for PUMP service): CHAIN_FAMILY = ETHEREUM CHAIN_NAME = MY_PRECIOUS CHAIN_NODE_URL = https: //my_precious_node_url:port Then all Kafka topics will be named as MY_PRECIOUS_TX_PUMP , MY_PRECIOUS_BLOCK_PUMP , MY_PRECIOUS_UNCLE_PUMP . Cassandra keyspace will be named my_precious . And all API url will be build like this http://localhost:8080/my_precious/block/42 and etc.","title":"Custom Chain Name"},{"location":"cyber-search/components/ethereum-components/","text":"Ethereum Components \u00b6 Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics Ethereum Pump \u00b6 Pumps Ethereum raw data(block,tx,uncles) into Kafka. Ethereum Cassandra Dump \u00b6 Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra. Ethereum Contract Summary \u00b6 Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum"},{"location":"cyber-search/components/ethereum-components/#ethereum-components","text":"Component Scale Description Cluster Address Metrics External Ethereum Pump 1 Ethereum Chain Data Kafka Pump 8080:/actuator/metrics Ethereum Cassandra Dump 1 Dump Kafka Topics Into Cassandra 8080:/actuator/metrics Ethereum Contract Summary 1 - N Calculates Contract Summaries (balances and etc) 8080:/actuator/metrics","title":"Ethereum Components"},{"location":"cyber-search/components/ethereum-components/#ethereum-pump","text":"Pumps Ethereum raw data(block,tx,uncles) into Kafka.","title":"Ethereum Pump"},{"location":"cyber-search/components/ethereum-components/#ethereum-cassandra-dump","text":"Consumes Ethereum raw data from Kafka topics, translate it into cassandra entities and store them into Elassandra.","title":"Ethereum Cassandra Dump"},{"location":"cyber-search/components/ethereum-components/#ethereum-contract-summary","text":"Collects summary for each contract stored in Ethereum chain. Consumes Kafka topics and update data in Elassandra.","title":"Ethereum Contract Summary"},{"location":"cyber-search/components/search-common-components/","text":"Search Common Components \u00b6 Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y Chains Components \u00b6 Ethereum Bitcoin Kafka \u00b6 Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality. Elassandra (Elastic + Cassandra) \u00b6 Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data. Search Api \u00b6 Main search access endpoint. See api calls documentations .","title":"Search-common"},{"location":"cyber-search/components/search-common-components/#search-common-components","text":"Component Scale Description Cluster Address Metrics External Kafka 1 Data Entry Kafka Manager 1 Kafka Explorer Elassandra 1 - N Data Back And Search elassandra.search.svc:9042 Search Api 1 - N Api To Search Chain Entities search-api.search.svc:80 8080:/actuator/metrics y Search Api Docs 1 - N Search Api Docs Based On Swagger search-api-docs.search.svc:80 y","title":"Search Common Components"},{"location":"cyber-search/components/search-common-components/#chains-components","text":"Ethereum Bitcoin","title":"Chains Components"},{"location":"cyber-search/components/search-common-components/#kafka","text":"Kafka is message queue used as main data entry. All raw data firstly goes into kafka, than various services consume it to provide new functionality.","title":"Kafka"},{"location":"cyber-search/components/search-common-components/#elassandra-elastic-cassandra","text":"Elassandra is used as main backend storage with linear growth fulltext-search index on source cassandra data.","title":"Elassandra (Elastic + Cassandra)"},{"location":"cyber-search/components/search-common-components/#search-api","text":"Main search access endpoint. See api calls documentations .","title":"Search Api"},{"location":"cyber-search/contributing/cheat-sheet/","text":"Kafka \u00b6 Stop kafka and delete kafka data(cheat sheet) \u00b6 docker stop fast-data-dev-search docker rm fast-data-dev-search Elassandra \u00b6 Stop elassandra and delete elassandra data(cheat sheet) \u00b6 docker stop elassandra-search docker rm elassandra-search Get indices info \u00b6 curl -XGET 'localhost:9200/_cat/indices?v&pretty' Chains \u00b6 Run parity node \u00b6 sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Run bitcoind node \u00b6 docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Cheat-sheet"},{"location":"cyber-search/contributing/cheat-sheet/#kafka","text":"","title":"Kafka"},{"location":"cyber-search/contributing/cheat-sheet/#stop-kafka-and-delete-kafka-datacheat-sheet","text":"docker stop fast-data-dev-search docker rm fast-data-dev-search","title":"Stop kafka and delete kafka data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#elassandra","text":"","title":"Elassandra"},{"location":"cyber-search/contributing/cheat-sheet/#stop-elassandra-and-delete-elassandra-datacheat-sheet","text":"docker stop elassandra-search docker rm elassandra-search","title":"Stop elassandra and delete elassandra data(cheat sheet)"},{"location":"cyber-search/contributing/cheat-sheet/#get-indices-info","text":"curl -XGET 'localhost:9200/_cat/indices?v&pretty'","title":"Get indices info"},{"location":"cyber-search/contributing/cheat-sheet/#chains","text":"","title":"Chains"},{"location":"cyber-search/contributing/cheat-sheet/#run-parity-node","text":"sudo docker run -d -p 8545 :8545 --name parity_eth \\ -v ${ REPLACE_IT_BY_HOST_FOLDER } :/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4","title":"Run parity node"},{"location":"cyber-search/contributing/cheat-sheet/#run-bitcoind-node","text":"docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0","title":"Run bitcoind node"},{"location":"cyber-search/contributing/contributing/","text":"Contributing to Cyber Search \u00b6 Thank you for considering a contribution to Cyber Search! This guide explains how to: * Get started * Development workflow * Get help if you encounter trouble Get in touch \u00b6 Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time) Development Workflow \u00b6 Development Setup \u00b6 Please, use development environment setup guide . Make Changes \u00b6 Use this Architecture Overview as a start point for making changes. Local Check \u00b6 Several checks should passed to succeed build. * Detekt code analyze tool should not report any issues * JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows Creating Commits And Writing Commit Messages \u00b6 The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section Submitting Your Change \u00b6 After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy. Getting Help \u00b6 If you run into any trouble, please reach out to us on the issue you are working on. Our Thanks \u00b6 We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Contributing"},{"location":"cyber-search/contributing/contributing/#contributing-to-cyber-search","text":"Thank you for considering a contribution to Cyber Search! This guide explains how to: * Get started * Development workflow * Get help if you encounter trouble","title":"Contributing to Cyber Search"},{"location":"cyber-search/contributing/contributing/#get-in-touch","text":"Before starting to work on a feature or a fix, please open an issue to discuss the use case or bug with us. This can save both you and us a lot of time. For any non-trivial change, we'll ask you to create a short design document explaining: Why is this change done? What's the use case? What will the API look like? (For new features) What test cases should it have? What could go wrong? How will it roughly be implemented? (We'll happily provide code pointers to save you time)","title":"Get in touch"},{"location":"cyber-search/contributing/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"cyber-search/contributing/contributing/#development-setup","text":"Please, use development environment setup guide .","title":"Development Setup"},{"location":"cyber-search/contributing/contributing/#make-changes","text":"Use this Architecture Overview as a start point for making changes.","title":"Make Changes"},{"location":"cyber-search/contributing/contributing/#local-check","text":"Several checks should passed to succeed build. * Detekt code analyze tool should not report any issues * JUnit tests should pass Before committing you changes, please, run local project check by: ./gradlew build //linux, mac gradlew.bat build //windows","title":"Local Check"},{"location":"cyber-search/contributing/contributing/#creating-commits-and-writing-commit-messages","text":"The commit messages that accompany your code changes are an important piece of documentation, please follow these guidelines when writing commit messages: solidity/CONTRIBUTING.md Keep commits discrete: avoid including multiple unrelated changes in a single commit Keep commits self-contained: avoid spreading a single change across multiple commits. A single commit should make sense in isolation Add GitHub issue to CHANGELOG.md Include GitHub issue in the commit message on a first line at the beginning. Example: #123 Refactor CONTRIBUTING.md --Add Creating Commits And Writing Commit Messages Section","title":"Creating Commits And Writing Commit Messages"},{"location":"cyber-search/contributing/contributing/#submitting-your-change","text":"After you submit your pull request, a core developer will review it. It is normal that this takes several iterations, so don't get discouraged by change requests. They ensure the high quality that we all enjoy.","title":"Submitting Your Change"},{"location":"cyber-search/contributing/contributing/#getting-help","text":"If you run into any trouble, please reach out to us on the issue you are working on.","title":"Getting Help"},{"location":"cyber-search/contributing/contributing/#our-thanks","text":"We deeply appreciate your effort toward improving Search. For any contribution, large or small, you will be immortalized in the release notes for the version you've contributed to.","title":"Our Thanks"},{"location":"cyber-search/contributing/dev-environment/","text":"Development environment \u00b6 Useful Links \u00b6 cheat sheet Prestart \u00b6 Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea Run Kafka, Elassandra, Prometheus and Grafana \u00b6 Start containers(required) \u00b6 For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d Run chain node (only for pumps) \u00b6 In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 . Import project to Intellij Idea \u00b6 Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation Run Ethereum Pump from intellij Idea \u00b6 Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Dev-environment"},{"location":"cyber-search/contributing/dev-environment/#development-environment","text":"","title":"Development environment"},{"location":"cyber-search/contributing/dev-environment/#useful-links","text":"cheat sheet","title":"Useful Links"},{"location":"cyber-search/contributing/dev-environment/#prestart","text":"Install Java 8 JDK Install Docker and Docker Compose Install Intellij Idea","title":"Prestart"},{"location":"cyber-search/contributing/dev-environment/#run-kafka-elassandra-prometheus-and-grafana","text":"","title":"Run Kafka, Elassandra, Prometheus and Grafana"},{"location":"cyber-search/contributing/dev-environment/#start-containersrequired","text":"For mac: cd dev-environment docker-compose -f env-mac.yml up -d For linux family: cd dev-environment docker-compose -f env.yml up -d","title":"Start containers(required)"},{"location":"cyber-search/contributing/dev-environment/#run-chain-node-only-for-pumps","text":"In order to fetch data from chains pumps need chain node to interact with. To run chain node locally using docker use following commands: Parity for Ethereum sudo docker run -d -p 8545:8545 --name parity_eth \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/cyberdata parity/parity:stable \\ --db-path /cyberdata --jsonrpc-hosts all --jsonrpc-interface all --jsonrpc-threads 4 Bitcoind for Bitcoin docker run -d -p 8332:8332 --name bitcoind --restart always \\ -v ${REPLACE_IT_BY_HOST_FOLDER}:/home/bitcoin/.bitcoin ruimarinho/bitcoin-core:0.15.1 \\ -server -rest -txindex -rpcpassword=cyber -rpcuser=cyber -rpcallowip=0.0.0.0/0 Or you could use any public available node (with appropriate settings) by passing CHAIN_NODE_URL environment variable to pump. For example CHAIN_NODE_URL=http://127.0.0.1:8545 .","title":"Run chain node (only for pumps)"},{"location":"cyber-search/contributing/dev-environment/#import-project-to-intellij-idea","text":"Open Project in idea by selecting: Import Project -> selecting build.gradle file from the repository root Wait for dependency downloading and indexation","title":"Import project to Intellij Idea"},{"location":"cyber-search/contributing/dev-environment/#run-ethereum-pump-from-intellij-idea","text":"Go to EthereumPumpApplication.kt and press green triangle on left to the code (on example line 14): Pump will fail due to lack of CHAIN_FAMILY environment property, let's define it: Select \"Edit Configuration\" Add properties: Now, run pump one more time, it should start.","title":"Run Ethereum Pump from intellij Idea"},{"location":"cyber-search/contributing/pump-development/","text":"Pump Development Guide \u00b6 To develop your own chain pump you have two deal with two modules: * common * pumps Some explanations \u00b6 To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily Adding models to common module \u00b6 First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own. Developing pump \u00b6 Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot ( https://projects.spring.io/spring-boot/ ). So you also should be familiar with spring beans, spring dependency injection and spring configuration. Creating gradle module \u00b6 To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class). Creating spring boot main class \u00b6 The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } } Default spring beans in context \u00b6 After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: * CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . * CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily . Implementing pump \u00b6 To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities. Memory Pool Pump \u00b6 You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items. Create Dockerfile \u00b6 Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar Update CI \u00b6 Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest * Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Pump-development"},{"location":"cyber-search/contributing/pump-development/#pump-development-guide","text":"To develop your own chain pump you have two deal with two modules: * common * pumps","title":"Pump Development Guide"},{"location":"cyber-search/contributing/pump-development/#some-explanations","text":"To describe single chain in our system we're using following scheme. Every chain has ChainFamily . For example BITCOIN, ETHEREUM families. ChainFamily has it's own set of entities like TX, BLOCK, UNCLE and default url of chain node. Next, chain has a name. Chain name used everywhere in modules. Kafka topics, Cassandra keyspaces, API endpoints will be based on chain name. By default chain name is equal to ChainFamily . For example if you have ChainFamily == BITCOIN and you not specifying a chain name it will be set to BITCOIN . Main goal of chain name is to separate forks of one chain that have the same structure. For example BITCOIN and BITCOIN_CASH or ETHEREUM and ETHEREUM_CLASSIC . So as far as you can understand single chain pump is chain pump for specific ChainFamily","title":"Some explanations"},{"location":"cyber-search/contributing/pump-development/#adding-models-to-common-module","text":"First of all you have to put models representing your blockchain entities in common module. They should be placed in fund.cyber.search.model.{your_chain_family_name} package. This models will be used to transfer your blockchain data among our microservices (using Kafka). After doing so put description of your chain family to ChainFamily enum. You could find it in fund.cyber.search.model.chains.ChainInfo.kt file. Description includes following info: defaultNodeUrl - default URL of blockchain node to connect for data. entityTypes - map of blockchain entities types (from fund.cyber.search.model.chains.ChainEntity enum) to their class representation. If you can't find needed entity type in fund.cyber.search.model.chains.ChainEntity enum you have to add it by your own.","title":"Adding models to common module"},{"location":"cyber-search/contributing/pump-development/#developing-pump","text":"Now you're ready to write pump for your chain family. Note that this pump is suitable only for block based chains. We're using Spring Boot ( https://projects.spring.io/spring-boot/ ). So you also should be familiar with spring beans, spring dependency injection and spring configuration.","title":"Developing pump"},{"location":"cyber-search/contributing/pump-development/#creating-gradle-module","text":"To start with, create new Gradle submodule with name of your chain in pumps module. Include your module in pumps/build.gradle and in settings.gradle . Example of pumps/build.gradle file with your module: project ( \":pumps:common\" ) { apply plugin: \"io.spring.dependency-management\" jar . archiveName = \"common-pumps\" dependencies { ... } } ... project ( \":pumps:{your_module_name}\" ) { apply plugin: \"org.springframework.boot\" dependencies { compile project ( \":pumps:common\" ) //your module dependencies } } ... Example of settings.gradle file: include \"common\" include \"common-kafka\" include \"cassandra-service\" ... include \"pumps:common\" include \"pumps:bitcoin\" include \"pumps:ethereum\" include \"pumps:{your_chain_name}\" ... All classes should be placed under fund.cyber.pump.{your_chain_name} package (except main class).","title":"Creating gradle module"},{"location":"cyber-search/contributing/pump-development/#creating-spring-boot-main-class","text":"The next step is creating spring boot main class. It should be placed in your module under fund.cyber package. Example of main class: @SpringBootApplication ( exclude = [ KafkaAutoConfiguration :: class ]) class BitcoinPumpApplication { companion object { @JvmStatic fun main ( args : Array < String >) { val application = SpringApplication ( BitcoinPumpApplication :: class . java ) application . runPump ( args ) } } }","title":"Creating spring boot main class"},{"location":"cyber-search/contributing/pump-development/#default-spring-beans-in-context","text":"After spring context start you'll have few already configured beans in context that you could inject in your module classes: org.springframework.retry.support.RetryTemplate - Spring Retry template for retrying failed operations. io.micrometer.core.instrument.MeterRegistry - bean for monitoring. fund.cyber.search.model.chains.ChainInfo - bean with all needed properties of running chain. class ChainInfo ( val family : ChainFamily , val name : String = \"\" , val nodeUrl : String = family . defaultNodeUrl ) { val fullName get () = family . name + if ( name . isEmpty ()) \"\" else \"_$name\" val entityTypes get () = family . entityTypes . keys fun entityClassByType ( type : ChainEntityType ) = family . entityTypes [ type ] } ChainInfo bean constructed from environment properties at start: * CHAIN_FAMILY - chain family name (matches ChainFamily enum). For example CHAIN_FAMILY=BITCOIN . Required option . * CHAIN_NAME - name of your specific chain. Not necessary . By default equals to CHAIN_FAMILY . All kafka topic will be named accordingly. For example CHAIN_NAME=BITCOIN_CASH then a topic name will be BITCOIN_CASH_TX_PUMP . * CHAIN_NODE_URL - URL of node running your chain. Not necessary . By default will be equal to one that described in your ChainFamily .","title":"Default spring beans in context"},{"location":"cyber-search/contributing/pump-development/#implementing-pump","text":"To integrate your pump with our system you simply have to implement two interfaces: * fund.cyber.pump.common.node.BlockBundle /** * Blockchain block with all dependent entities. Should collect all entities in scope of one block. * For example: transactions, uncles, etc.. */ interface BlockBundle { /** * Hash of the block */ val hash : String /** * Hash of the parent block */ val parentHash : String /** * Number of the block in blockchain */ val number : Long /** * Size of the block in bytes */ val blockSize : Int /** * Get dependent entity values list by entity type. * * @param chainEntityType type of entity (for example: [ChainEntityType.TX]) * @return list of entity values (for example: transactions) */ fun entitiesByType ( chainEntityType : ChainEntityType ): List < ChainEntity > } fund.cyber.pump.common.node.BlockchainInterface /** * Interface representing blockchain * * @param T block bundle of this blockchain */ interface BlockchainInterface < out T : BlockBundle > { /** * Get last number of the block in blockchain network. * * @return block number */ fun lastNetworkBlock (): Long /** * Get [BlockBundle] by block number. * * @param number block number * @return block bundle */ fun blockBundleByNumber ( number : Long ): T } Also you should define spring bean of fund.cyber.pump.common.node.BlockchainInterface implementation either by annotate it with @Component or defining @Bean in spring configuration. For example: @Component class BitcoinBlockchainInterface ( ... ) : BlockchainInterface < BitcoinBlockBundle > { private val downloadSpeedMonitor = monitoring . timer ( \"pump_bundle_download\" ) override fun lastNetworkBlock (): Long = bitcoinJsonRpcClient . getLastBlockNumber () override fun blockBundleByNumber ( number : Long ): BitcoinBlockBundle { return downloadSpeedMonitor . recordCallable { val block = bitcoinJsonRpcClient . getBlockByNumber ( number ) !! return @recordCallable rpcToBundleEntitiesConverter . convertToBundle ( block ) } } } So, as you can see, you're not care of what happening with data next and how to store it. All you need is just to tell BlockBundle how to map it's fields on entities and we'll take care of everything else. Note that you should use toSearchHashFormat() extension function placed in fund.cyber.api.common.Func.kt on all fields in hex format when building your chain entities.","title":"Implementing pump"},{"location":"cyber-search/contributing/pump-development/#memory-pool-pump","text":"You also could add memory pool pumping logic by simply implement PoolInterface interface interface PoolInterface < T : PoolItem > { fun subscribePool (): Flowable < T > } It contains only one method that should return io.reactivex.Flowable of pool items.","title":"Memory Pool Pump"},{"location":"cyber-search/contributing/pump-development/#create-dockerfile","text":"Put Docker file in root folder of your module. Here is template of Dockerfile: # Build Stage # Container with application FROM openjdk:8-jre-slim COPY /build/libs /cyberapp/bin ENTRYPOINT exec java $JAVA_OPTS -jar /cyberapp/bin/${your_chain_name}.jar","title":"Create Dockerfile"},{"location":"cyber-search/contributing/pump-development/#update-ci","text":"Finally, you should update .circleci/config.yml file with steps for building and pushing docker image. * Add deploy job to jobs section. Template: deploy_chain_pumps_${your_chain_name}_image: <<: *defaults steps: - attach_workspace: at: ~/build - setup_remote_docker: version: 17.11.0-ce - run: name: Build ${your_chain_name} Pump Image command: | docker build -t build/pump-${your_chain_name} -f ./pumps/${your_chain_name}/Dockerfile ./pumps/${your_chain_name} docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker push cybernode/chain-pump-${your_chain_name}:$CIRCLE_TAG docker tag build/pump-${your_chain_name} cybernode/chain-pump-${your_chain_name}:latest docker push cybernode/chain-pump-${your_chain_name}:latest * Add deploy job to workflows.search_build.jobs section. Template: - deploy_chain_pumps_${your_chain_name}_image: <<: *release_filter requires: - build_project","title":"Update CI"},{"location":"cyberd/Changelog/","text":"Change Log \u00b6 Unreleased \u00b6 Full Changelog Fixed bugs: Cannot interact with node #203 Update cyberd.ai #185 CI: Cyberd docker image building. Include genesis file #160 Closed issues: Launch 0.1.1 testnet #205 Cyberd release scheme. #180 Ranking research #24 Merged pull requests: UPD run validator md #232 ( SaveTheAles ) Fix docs #228 ( arturalbov ) upd run-validator.md #225 ( SaveTheAles ) v0.1.1 (2019-01-31) \u00b6 Full Changelog Fixed bugs: Error on replay with wrong app hash calculation block \\#914381 block \\#914381 #197 Closed issues: Mass testing of cyberd-testnet 0.1.1 #219 Export 0.1.1 genesis from 0.1.0 testnet. #215 Setup 0.1.1 chain params for staking, distr, slashing, mint modules. #201 euler-1 (2019-01-31) \u00b6 Full Changelog Merged pull requests: Euler 0.1.1 #223 ( arturalbov ) Fix import reader #222 ( arturalbov ) Landing update #220 ( asadovka ) DISCUSS: Chain params for 0.1.1 network #202 ( hleb-albau ) euler-1.1-dev1 (2019-01-30) \u00b6 Full Changelog Implemented enhancements: Docker image with possibility to select compute unit #211 Fixed bugs: Link msg bandwidth cost doesn't take into account links count #216 Merged pull requests: #215 Fix pou distribution uses share not percentage #221 ( hleb-albau ) #215 Fix gentx commands. #218 ( hleb-albau ) euler-1.1-dev0 (2019-01-29) \u00b6 Full Changelog Implemented enhancements: Links Exporter #184 Fixed bugs: Fix validator manual #189 Change mint module to default cosmos module #181 Current Bandwidth do not respect price #168 negative bandwidnt #164 Wrong CYB allocation in Genesis #156 Update docker with 0.1.0 version #154 Move import_private command to keys subcomand #152 Handle import of ethereum privkeys with 0x #150 Closed issues: Fix typos #207 Update landing for 0.4 whitepaper #204 Update to tendermint v29.0 #187 Change Bandwidth Price to Average for 24h Sliding Window #179 High bandwidth cost for create validator msg #178 Bandwidth Specification Change #177 Add rpc endpoint to submit signed link and send messages #173 Tx size reduction: switch to more compact links tx encoding. #171 index entities count endpoint #165 Add ipfs hashes to release #161 Change token name from CBD to CYB in the testnetwork #157 Add more issues templates. #153 Add trust-node by default in cli #151 Numerous small fixes in whitepaper #89 Cyberd landing make up #36 Whitepaper 0.4 #25 Merged pull requests: CI: testnet ipfs files hashes #217 ( arturalbov ) WIP: CI Cyberd docker image building. Include genesis file #214 ( arturalbov ) #197 Error on replay with wrong app hash calculation #213 ( hleb-albau ) Links exporter #210 ( arturalbov ) R4R #181 Constant Block Reward #209 ( hleb-albau ) 157 change token protocol name #208 ( hleb-albau ) Fix typos get_CYB.md #206 ( serejandmyself ) R4R: #156 Add Missing euler tokens cmd #200 ( hleb-albau ) Update to cosmos-sdk v0.30.0 #198 ( hleb-albau ) Tx size reduction: switch to more compact links tx encoding #192 ( arturalbov ) Fix rank calculation context data #191 ( hleb-albau ) Important fixes in validators manual #190 ( xhipster ) Change Bandwidth Price to Average for 24h Sliding Window #188 ( arturalbov ) R4R Update cosmos to latest develop #186 ( hleb-albau ) #153 Add more issues templates. #183 ( hleb-albau ) #178 #168 Bandwidth price + Msg cost for non link txes #182 ( hleb-albau ) #173 Add rpc endpoint to submit signed link and send messages #174 ( hleb-albau ) Technical notes on euler release #169 ( xhipster ) Handle import of ethereum privkeys with 0x. Trust node by default #167 ( arturalbov ) #164 fix rpc negative bw #165 add index entities count endpoint #166 ( hleb-albau ) Move import_private command to keys subcomand #159 ( arturalbov ) v0.1.0 (2019-01-06) \u00b6 Full Changelog Implemented enhancements: CID rank merkle proofs #133 Simple go cyberd client #122 Add remainig bandwidth endpoint #109 Get rid of RPC proxy #94 Fixed bugs: Cannot check balance using cli #149 Index out of range for cids with not calculated rank yet. #139 Account inmem balances should be updated by wrapping accountKeeper #99 Closed issues: Cli: Add possibility to restore acc from priv keys #146 Go mod Ci dependency error #128 Update to cosmos 29 #126 Write down bandwidth specification #114 Make docker container based on nvidia-gpu image. #104 Index transactions by addresses. #103 Calculate rank in ||, post results each 600 blocks. #101 Linkchains support #91 Include addresses into knowledge graph #90 Graphics for whitepaper #88 Add bandwidth by stake #77 Launch testnet Euler #73 Test Ethereum Network Statistical Significance #52 Cyberd landing design #51 Simulation #31 Link Chain PoC | Zeronet #26 Write benchmark and test SpringRank with different amount of objects/edges #22 Parse Ethereum network and calculate SpringRank #21 Research basic chains fundamentals #15 Perfomance testing of Solana #10 Research on perfomance of consensus computers #9 Perfomance testing scenario for cyberd #8 Merged pull requests: [euler] Launch euler testnet #148 ( hleb-albau ) Cli: Add possibility to restore acc from priv keys #147 ( arturalbov ) Rank merkle proofs #144 ( arturalbov ) Setup bw params #143 ( hleb-albau ) Merkle tree implementation #141 ( arturalbov ) #139 Index out of range for cids with not calculated rank yet. #140 ( hleb-albau ) Fix http client and rank logs #137 ( hleb-albau ) [euler-dev3] New testnet #136 ( hleb-albau ) #101 Copy state before index creation #135 ( hleb-albau ) #92 Populate state with random addresses #134 ( hleb-albau ) Small fixes #131 ( arturalbov ) Parallel rank calculation #130 ( arturalbov ) Update stake index every block #129 ( hleb-albau ) #126 Update to cosmos 29 #127 ( hleb-albau ) Refactoring #125 ( hleb-albau ) Various Bug fixes #124 ( hleb-albau ) In-memory storages refactoring #123 ( arturalbov ) #114 Write bw specification #121 ( hleb-albau ) WIP #114 Add cbdbank module #120 ( hleb-albau ) Update readme #119 ( hleb-albau ) Clean up folders #118 ( arturalbov ) #103 Index transactions by signers. #117 ( hleb-albau ) v0.0.8 (2018-12-11) \u00b6 Full Changelog Implemented enhancements: Cid validation #93 Update to cosmos-sdk version 0.26.1 #79 Build node releases with cleveldb #59 Remove 'cosmosaccaddr' prefix from cyberd address #39 Closed issues: Make up cyberd landing #87 Add possibility to join for new validators. #75 Calculate rank using GPU #74 Create basic wiki cyberd indexer #71 Create cyberd PoC based on Cosmos SDK #37 Draw logo for cyberd #16 Build basic economic model #1 Merged pull requests: #76 Define Basic RPC specification #111 ( hleb-albau ) #104 Make docker container based on nvidia-gpu image #110 ( hleb-albau ) [DON'T MERGE] Add bandwidth by stake. Part 2. #108 ( arturalbov ) Add bandwidth by stake Part 1 #107 ( arturalbov ) #93 Cid validation #106 ( hleb-albau ) #78 Add guide How to join network as validator #105 ( hleb-albau ) #1 Build basic economic model #102 ( hleb-albau ) Update cosmos to 0.27.0 #100 ( hleb-albau ) Small fixes for validators joining #98 ( arturalbov ) Remove poc folder #97 ( hleb-albau ) Add possibility to join for new validators #96 ( arturalbov ) Calculate eth network significance #85 ( hleb-albau ) 74 gpu rank calculation #83 ( hleb-albau ) Remove 'cosmosaccaddr' prefix from cyberd address #82 ( arturalbov ) UPD docs_upd job #81 ( SaveTheAles ) Update cosmos-sdk version to 0.26.1 #80 ( arturalbov ) v0.0.7 (2018-10-25) \u00b6 Full Changelog Implemented enhancements: Proxy service. Add search pagination #69 Add send tokens endpoint to proxy #62 Fixed bugs: Proxy service. Search request with \"spaces\" fails #67 Non-deterministic rank calculation #66 Closed issues: Claim service: increment tx sequence manually #64 Merged pull requests: #71 Create basic wiki cyberd indexer #72 ( hleb-albau ) Proxy service. Add search pagination #70 ( arturalbov ) Proxy service. Search request with spaces fails #68 ( arturalbov ) Claim service: increment tx sequence manually #65 ( arturalbov ) v0.0.6 (2018-10-24) \u00b6 Full Changelog v0.0.5 (2018-10-23) \u00b6 Full Changelog Implemented enhancements: [RPC] Fix small finding #57 v0.0.4 (2018-10-23) \u00b6 Full Changelog Closed issues: Service to claim cyberd zeronet tokens #61 Update cosmos-sdk to latest dev branch version #56 Perfomance Degradation: Heavy Disk Usage #50 Merged pull requests: Add send tokens endpoint to proxy #63 ( arturalbov ) Claim service #60 ( arturalbov ) #57 [RPC] Fix small finding #58 ( hleb-albau ) Update cosmos-sdk to latest dev branch version #53 ( hleb-albau ) v0.0.3 (2018-10-19) \u00b6 Full Changelog Implemented enhancements: CLI: Add \"wait_for_confirmation\" Flag #47 Fixed bugs: RPC Client: /search on non existing cid return first added cid #48 Closed issues: Write validation logic of IPFS hash for Losion Zeronet #18 Genesis Zeronet #17 Performance testing of Zeronet #4 Merged pull requests: Add cyberdproxy process to docker container. Add status endpoint #55 ( arturalbov ) Proxy rpc #54 ( arturalbov ) v0.0.2 (2018-10-05) \u00b6 Full Changelog Implemented enhancements: Cosmos PoC: Integrate Rank Calculation #43 Cosmos PoC: Extenend Standart Tendermint RPC API #42 Closed issues: Implement persistent storage for links. #40 run extra node for cyberd #20 Merged pull requests: Fix search on non existing cid #49 ( arturalbov ) Add Circle CI build job #46 ( arturalbov ) Extenend Standart Tendermint RPC API #45 ( arturalbov ) #43 Simplest Rank #44 ( hleb-albau ) #40 introduce in-memory store #41 ( hleb-albau ) #37 redesign db, app refactor #38 ( hleb-albau ) Cosmos POC: Clean up CLI #35 ( arturalbov ) Update cyberd/cosmos README #34 ( arturalbov ) v0.0.1 (2018-09-25) \u00b6 Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax ) * This Change Log was automatically generated by github_changelog_generator","title":"Changelog"},{"location":"cyberd/Changelog/#change-log","text":"","title":"Change Log"},{"location":"cyberd/Changelog/#unreleased","text":"Full Changelog Fixed bugs: Cannot interact with node #203 Update cyberd.ai #185 CI: Cyberd docker image building. Include genesis file #160 Closed issues: Launch 0.1.1 testnet #205 Cyberd release scheme. #180 Ranking research #24 Merged pull requests: UPD run validator md #232 ( SaveTheAles ) Fix docs #228 ( arturalbov ) upd run-validator.md #225 ( SaveTheAles )","title":"Unreleased"},{"location":"cyberd/Changelog/#v011-2019-01-31","text":"Full Changelog Fixed bugs: Error on replay with wrong app hash calculation block \\#914381 block \\#914381 #197 Closed issues: Mass testing of cyberd-testnet 0.1.1 #219 Export 0.1.1 genesis from 0.1.0 testnet. #215 Setup 0.1.1 chain params for staking, distr, slashing, mint modules. #201","title":"v0.1.1 (2019-01-31)"},{"location":"cyberd/Changelog/#euler-1-2019-01-31","text":"Full Changelog Merged pull requests: Euler 0.1.1 #223 ( arturalbov ) Fix import reader #222 ( arturalbov ) Landing update #220 ( asadovka ) DISCUSS: Chain params for 0.1.1 network #202 ( hleb-albau )","title":"euler-1 (2019-01-31)"},{"location":"cyberd/Changelog/#euler-11-dev1-2019-01-30","text":"Full Changelog Implemented enhancements: Docker image with possibility to select compute unit #211 Fixed bugs: Link msg bandwidth cost doesn't take into account links count #216 Merged pull requests: #215 Fix pou distribution uses share not percentage #221 ( hleb-albau ) #215 Fix gentx commands. #218 ( hleb-albau )","title":"euler-1.1-dev1 (2019-01-30)"},{"location":"cyberd/Changelog/#euler-11-dev0-2019-01-29","text":"Full Changelog Implemented enhancements: Links Exporter #184 Fixed bugs: Fix validator manual #189 Change mint module to default cosmos module #181 Current Bandwidth do not respect price #168 negative bandwidnt #164 Wrong CYB allocation in Genesis #156 Update docker with 0.1.0 version #154 Move import_private command to keys subcomand #152 Handle import of ethereum privkeys with 0x #150 Closed issues: Fix typos #207 Update landing for 0.4 whitepaper #204 Update to tendermint v29.0 #187 Change Bandwidth Price to Average for 24h Sliding Window #179 High bandwidth cost for create validator msg #178 Bandwidth Specification Change #177 Add rpc endpoint to submit signed link and send messages #173 Tx size reduction: switch to more compact links tx encoding. #171 index entities count endpoint #165 Add ipfs hashes to release #161 Change token name from CBD to CYB in the testnetwork #157 Add more issues templates. #153 Add trust-node by default in cli #151 Numerous small fixes in whitepaper #89 Cyberd landing make up #36 Whitepaper 0.4 #25 Merged pull requests: CI: testnet ipfs files hashes #217 ( arturalbov ) WIP: CI Cyberd docker image building. Include genesis file #214 ( arturalbov ) #197 Error on replay with wrong app hash calculation #213 ( hleb-albau ) Links exporter #210 ( arturalbov ) R4R #181 Constant Block Reward #209 ( hleb-albau ) 157 change token protocol name #208 ( hleb-albau ) Fix typos get_CYB.md #206 ( serejandmyself ) R4R: #156 Add Missing euler tokens cmd #200 ( hleb-albau ) Update to cosmos-sdk v0.30.0 #198 ( hleb-albau ) Tx size reduction: switch to more compact links tx encoding #192 ( arturalbov ) Fix rank calculation context data #191 ( hleb-albau ) Important fixes in validators manual #190 ( xhipster ) Change Bandwidth Price to Average for 24h Sliding Window #188 ( arturalbov ) R4R Update cosmos to latest develop #186 ( hleb-albau ) #153 Add more issues templates. #183 ( hleb-albau ) #178 #168 Bandwidth price + Msg cost for non link txes #182 ( hleb-albau ) #173 Add rpc endpoint to submit signed link and send messages #174 ( hleb-albau ) Technical notes on euler release #169 ( xhipster ) Handle import of ethereum privkeys with 0x. Trust node by default #167 ( arturalbov ) #164 fix rpc negative bw #165 add index entities count endpoint #166 ( hleb-albau ) Move import_private command to keys subcomand #159 ( arturalbov )","title":"euler-1.1-dev0 (2019-01-29)"},{"location":"cyberd/Changelog/#v010-2019-01-06","text":"Full Changelog Implemented enhancements: CID rank merkle proofs #133 Simple go cyberd client #122 Add remainig bandwidth endpoint #109 Get rid of RPC proxy #94 Fixed bugs: Cannot check balance using cli #149 Index out of range for cids with not calculated rank yet. #139 Account inmem balances should be updated by wrapping accountKeeper #99 Closed issues: Cli: Add possibility to restore acc from priv keys #146 Go mod Ci dependency error #128 Update to cosmos 29 #126 Write down bandwidth specification #114 Make docker container based on nvidia-gpu image. #104 Index transactions by addresses. #103 Calculate rank in ||, post results each 600 blocks. #101 Linkchains support #91 Include addresses into knowledge graph #90 Graphics for whitepaper #88 Add bandwidth by stake #77 Launch testnet Euler #73 Test Ethereum Network Statistical Significance #52 Cyberd landing design #51 Simulation #31 Link Chain PoC | Zeronet #26 Write benchmark and test SpringRank with different amount of objects/edges #22 Parse Ethereum network and calculate SpringRank #21 Research basic chains fundamentals #15 Perfomance testing of Solana #10 Research on perfomance of consensus computers #9 Perfomance testing scenario for cyberd #8 Merged pull requests: [euler] Launch euler testnet #148 ( hleb-albau ) Cli: Add possibility to restore acc from priv keys #147 ( arturalbov ) Rank merkle proofs #144 ( arturalbov ) Setup bw params #143 ( hleb-albau ) Merkle tree implementation #141 ( arturalbov ) #139 Index out of range for cids with not calculated rank yet. #140 ( hleb-albau ) Fix http client and rank logs #137 ( hleb-albau ) [euler-dev3] New testnet #136 ( hleb-albau ) #101 Copy state before index creation #135 ( hleb-albau ) #92 Populate state with random addresses #134 ( hleb-albau ) Small fixes #131 ( arturalbov ) Parallel rank calculation #130 ( arturalbov ) Update stake index every block #129 ( hleb-albau ) #126 Update to cosmos 29 #127 ( hleb-albau ) Refactoring #125 ( hleb-albau ) Various Bug fixes #124 ( hleb-albau ) In-memory storages refactoring #123 ( arturalbov ) #114 Write bw specification #121 ( hleb-albau ) WIP #114 Add cbdbank module #120 ( hleb-albau ) Update readme #119 ( hleb-albau ) Clean up folders #118 ( arturalbov ) #103 Index transactions by signers. #117 ( hleb-albau )","title":"v0.1.0 (2019-01-06)"},{"location":"cyberd/Changelog/#v008-2018-12-11","text":"Full Changelog Implemented enhancements: Cid validation #93 Update to cosmos-sdk version 0.26.1 #79 Build node releases with cleveldb #59 Remove 'cosmosaccaddr' prefix from cyberd address #39 Closed issues: Make up cyberd landing #87 Add possibility to join for new validators. #75 Calculate rank using GPU #74 Create basic wiki cyberd indexer #71 Create cyberd PoC based on Cosmos SDK #37 Draw logo for cyberd #16 Build basic economic model #1 Merged pull requests: #76 Define Basic RPC specification #111 ( hleb-albau ) #104 Make docker container based on nvidia-gpu image #110 ( hleb-albau ) [DON'T MERGE] Add bandwidth by stake. Part 2. #108 ( arturalbov ) Add bandwidth by stake Part 1 #107 ( arturalbov ) #93 Cid validation #106 ( hleb-albau ) #78 Add guide How to join network as validator #105 ( hleb-albau ) #1 Build basic economic model #102 ( hleb-albau ) Update cosmos to 0.27.0 #100 ( hleb-albau ) Small fixes for validators joining #98 ( arturalbov ) Remove poc folder #97 ( hleb-albau ) Add possibility to join for new validators #96 ( arturalbov ) Calculate eth network significance #85 ( hleb-albau ) 74 gpu rank calculation #83 ( hleb-albau ) Remove 'cosmosaccaddr' prefix from cyberd address #82 ( arturalbov ) UPD docs_upd job #81 ( SaveTheAles ) Update cosmos-sdk version to 0.26.1 #80 ( arturalbov )","title":"v0.0.8 (2018-12-11)"},{"location":"cyberd/Changelog/#v007-2018-10-25","text":"Full Changelog Implemented enhancements: Proxy service. Add search pagination #69 Add send tokens endpoint to proxy #62 Fixed bugs: Proxy service. Search request with \"spaces\" fails #67 Non-deterministic rank calculation #66 Closed issues: Claim service: increment tx sequence manually #64 Merged pull requests: #71 Create basic wiki cyberd indexer #72 ( hleb-albau ) Proxy service. Add search pagination #70 ( arturalbov ) Proxy service. Search request with spaces fails #68 ( arturalbov ) Claim service: increment tx sequence manually #65 ( arturalbov )","title":"v0.0.7 (2018-10-25)"},{"location":"cyberd/Changelog/#v006-2018-10-24","text":"Full Changelog","title":"v0.0.6 (2018-10-24)"},{"location":"cyberd/Changelog/#v005-2018-10-23","text":"Full Changelog Implemented enhancements: [RPC] Fix small finding #57","title":"v0.0.5 (2018-10-23)"},{"location":"cyberd/Changelog/#v004-2018-10-23","text":"Full Changelog Closed issues: Service to claim cyberd zeronet tokens #61 Update cosmos-sdk to latest dev branch version #56 Perfomance Degradation: Heavy Disk Usage #50 Merged pull requests: Add send tokens endpoint to proxy #63 ( arturalbov ) Claim service #60 ( arturalbov ) #57 [RPC] Fix small finding #58 ( hleb-albau ) Update cosmos-sdk to latest dev branch version #53 ( hleb-albau )","title":"v0.0.4 (2018-10-23)"},{"location":"cyberd/Changelog/#v003-2018-10-19","text":"Full Changelog Implemented enhancements: CLI: Add \"wait_for_confirmation\" Flag #47 Fixed bugs: RPC Client: /search on non existing cid return first added cid #48 Closed issues: Write validation logic of IPFS hash for Losion Zeronet #18 Genesis Zeronet #17 Performance testing of Zeronet #4 Merged pull requests: Add cyberdproxy process to docker container. Add status endpoint #55 ( arturalbov ) Proxy rpc #54 ( arturalbov )","title":"v0.0.3 (2018-10-19)"},{"location":"cyberd/Changelog/#v002-2018-10-05","text":"Full Changelog Implemented enhancements: Cosmos PoC: Integrate Rank Calculation #43 Cosmos PoC: Extenend Standart Tendermint RPC API #42 Closed issues: Implement persistent storage for links. #40 run extra node for cyberd #20 Merged pull requests: Fix search on non existing cid #49 ( arturalbov ) Add Circle CI build job #46 ( arturalbov ) Extenend Standart Tendermint RPC API #45 ( arturalbov ) #43 Simplest Rank #44 ( hleb-albau ) #40 introduce in-memory store #41 ( hleb-albau ) #37 redesign db, app refactor #38 ( hleb-albau ) Cosmos POC: Clean up CLI #35 ( arturalbov ) Update cyberd/cosmos README #34 ( arturalbov )","title":"v0.0.2 (2018-10-05)"},{"location":"cyberd/Changelog/#v001-2018-09-25","text":"Closed issues: Make cyberd docs to be included into common wiki. #32 Write LT/NLT logic to Losion Zeronet #19 Research basic technologies #14 Research basic papers #13 Perfomance testing of Ethermint #12 Perfomance testing of Plasma #11 Perfomance testing of PoA networks #7 Cleanup paper #6 Perfomance testing of EOS #3 Merged pull requests: Genesis zeronet: Cosmos SDK #33 ( arturalbov ) [WIP] 21 calculate spring rank for ethereum #23 ( hleb-albau ) Fixing typo #2 ( trummax ) * This Change Log was automatically generated by github_changelog_generator","title":"v0.0.1 (2018-09-25)"},{"location":"cyberd/Contributing/","text":"","title":"Contributing"},{"location":"cyberd/bandwidth/","text":"Minimal Bandwidth Spec \u00b6 bounded stake . Stake, that deducted from your acc coins and put as deposit to take part in consensus. Due to passive inflation model and slashing, deposit not match 1-to-1 to final reward. So, for example, stakeholders may wish to set up a script, that will periodically withdraw and rebound rewards to increase their share( bounded stake ). active stake . Currently available for direct transfer, not-bounded stake. bandwidth stake . Sum of active stake and bounded stake for given account. Model \u00b6 Cyberd use a very simple bandwidth model. Main goal of that model is to reduce daily network growth to given constant, say 3gb per day. Thus, here we introduce resource credits (RS). Each message type have assigned RS cost. There is constant DesirableNetworkBandwidthForRecoveryPeriod determining desirable for RecoveryPeriod spent RS value. RecoveryPeriod is defining how fast user can recover their bandwidth from 0 to user max bandwidth. User has maximum RS proportional to his stake by formula user_max_rc = bandwidth_stake% * DesirableNetworkBandwidthForRecoveryPeriod . There is period AdjustPricePeriod summing how much RS was spent for that period( AdjustPricePeriodTotalSpent ). Also, there is constant AdjustPricePeriodDesiredSpent , used to calculate network loading. AdjustPricePeriodTotalSpent/AdjustPricePeriodDesiredSpent ratio defined so called current price multiplier . If network usage is low, price multiplier adjust message cost(by simply multiplying) to allow user with lower stake to do more transactions. If resource demand increase, price multiplier goes >1 thus increase messages cost and limiting final tx count for some long-term period(RC recovery will be < then RC spending). Bandwidth stake change \u00b6 There are only few ways to change acc bandwidth stake : Direct coins transfer. When distribution payouts occurs. For example, when validator change his commission rates, all delegations will be automatically unbounded. Another example, delegator itself unbound some part or full share. Implementation details. All this cases will change user active stake , so we define bank hook CoinsTransferHook . Due to inconsistent stateupon hook invoking (coins can be substructed, but deposit not added yet), we should update user max RC only after deliver tx occur.","title":"Bandwidth Specification"},{"location":"cyberd/bandwidth/#minimal-bandwidth-spec","text":"bounded stake . Stake, that deducted from your acc coins and put as deposit to take part in consensus. Due to passive inflation model and slashing, deposit not match 1-to-1 to final reward. So, for example, stakeholders may wish to set up a script, that will periodically withdraw and rebound rewards to increase their share( bounded stake ). active stake . Currently available for direct transfer, not-bounded stake. bandwidth stake . Sum of active stake and bounded stake for given account.","title":"Minimal Bandwidth Spec"},{"location":"cyberd/bandwidth/#model","text":"Cyberd use a very simple bandwidth model. Main goal of that model is to reduce daily network growth to given constant, say 3gb per day. Thus, here we introduce resource credits (RS). Each message type have assigned RS cost. There is constant DesirableNetworkBandwidthForRecoveryPeriod determining desirable for RecoveryPeriod spent RS value. RecoveryPeriod is defining how fast user can recover their bandwidth from 0 to user max bandwidth. User has maximum RS proportional to his stake by formula user_max_rc = bandwidth_stake% * DesirableNetworkBandwidthForRecoveryPeriod . There is period AdjustPricePeriod summing how much RS was spent for that period( AdjustPricePeriodTotalSpent ). Also, there is constant AdjustPricePeriodDesiredSpent , used to calculate network loading. AdjustPricePeriodTotalSpent/AdjustPricePeriodDesiredSpent ratio defined so called current price multiplier . If network usage is low, price multiplier adjust message cost(by simply multiplying) to allow user with lower stake to do more transactions. If resource demand increase, price multiplier goes >1 thus increase messages cost and limiting final tx count for some long-term period(RC recovery will be < then RC spending).","title":"Model"},{"location":"cyberd/bandwidth/#bandwidth-stake-change","text":"There are only few ways to change acc bandwidth stake : Direct coins transfer. When distribution payouts occurs. For example, when validator change his commission rates, all delegations will be automatically unbounded. Another example, delegator itself unbound some part or full share. Implementation details. All this cases will change user active stake , so we define bank hook CoinsTransferHook . Due to inconsistent stateupon hook invoking (coins can be substructed, but deposit not added yet), we should update user max RC only after deliver tx occur.","title":"Bandwidth stake change"},{"location":"cyberd/cyberd/","text":"cyberd: Computing the knowledge from web3 \u00b6 Notes on euler release of cyber:// protocol reference implementation using Go. cyber\u2022Congress : @xhipster, @litvintech, @hleb-albau, @arturalbov, @belya cyb: - nick. a friendly software robot who helps you explore universes cyber: - noun. a superintelligent network computer for answers - verb. to do something intelligent, to be very smart cyber:// - web3 protocol for computing answers and knowledge exchange CYB: - ticker. transferable token expressing a will to become smarter CYBER: - ticker. non-transferable token measuring intelligence CBD: - ticker. ERC-20 proto token representing substance from which CYB emerge cyberlink: - link type. expressing connection from one link to another as link-x.link-y Content \u00b6 cyberd: Computing the knowledge from web3 Content Abstract Introduction to web3 On adversarial examples problem Cyber protocol at euler Knowledge graph Cyberlinks Notion of consensus computer Relevance machine cyber\u2022Rank Proof of relevance Speed and scalability Implementation in a browser From Inception to Genesis Validators incentive Satoshi Lottery Inception Possible applications Economic protection is smith Ability to evolve is darwin turing is about computing more In a search for equilibria is nash On faster evolution at weiner Genesis is secure as merkle Conclusion References Abstract \u00b6 A consensus computer allows computing of provably relevant answers without opinionated blackbox intermediaries such as Google, Youtube, Amazon or Facebook. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution, but there are at least three problems associated with implementation. Of course, the first problem is the subjective nature of relevance. The second problem is that it is hard to scale consensus computer for a huge knowledge graph. The third problem is that the quality of such a knowledge graph will suffer from different attack surfaces such as sybil, selfish behaviour of interacting agents. In this paper, we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on Tendermint consensus of cyber\u2022rank computed on GPU, (2) discuss implementation details and (3) design distribution and incentive scheme based on our experience. We believe the minimalistic architecture of the protocol is critical for the formation of a network of domain-specific knowledge consensus computers. As a result of our work some applications never existed before emerge. We expand the work including our vision on features we expect to work up to Genesis. Introduction to web3 \u00b6 Original protocols of the Internet such as TCP/IP, DNS, URL, and HTTPS brought a web into the point where it is now. Along with all the benefits they have created they brought more problem to the table. Globality being a vital property of the web since inception is under real threat. The speed of connections degrades with network grow and from ubiquitous government interventions into privacy and security of web users. One property, not evident in the beginning, become important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time has passed . Reliance on \"one at a time ISP\" architecture allows governments effectively censor packets. It is the last straw in a conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have the ability to work with the state it has and after acquiring connection being able to sync with global state and continue to verify state's validity in realtime while having a connection. Now, these properties offered on the app level while such properties must be integrated into lower level protocols. The emergence of a web3 stack creates an opportunity for a new kind of Internet. We call it web3. It has a promise to remove problems of a conventional protocol stack and add to the web better speed and more accessible connection. However, as usual in a story with a new stack, new problems emerge. One of such problem is general-purpose search. Existing general-purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on TCP/IP, DNS, URL and HTTPS protocols. Web3 creates a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures. On adversarial examples problem \u00b6 Conventional architecture of search engines where one entity process and rank all the shit suffers from one hard but the particular problem that still has not been solved even by brilliant Google scientists: adversarial examples problem . The problem Google acknowledge is that it is rather hard to algorithmically reason either this particular sample is adversarial or not independently on how cool the learning technology is. Obviously, a cryptoeconomic approach can change beneficiaries in this game effectively removing possible sybil attack vectors and removing the necessity to make a decision on example crawling and meaning extraction from one entity to the whole world. Learning sybil-resistant model will probably lead to orders of magnitude more predictive results. Cyber protocol at euler \u00b6 compute euler inception of cyber protocol based on Satoshi lottery and CBD balances def knowledge graph state take cyberlinks check the validity of signatures check bandwidth limit check the validity of CIDv0 if signatures, bandwidth limit, and CIDv0 are ok than cyberlink is valid for every valid cyberlink emit prediction as an array of CIDv0 every round calculate cyber\u2022rank deltas for the knowledge graph every round distribute CYB based on defined rules apply more secure consensus state based on CBD balances 6 times up to merkle Knowledge graph \u00b6 We represent a knowledge graph as a weighted graph of directed links between content addresses or content identifications or CIDs. In this paper, we will use them as synonyms. Content addresses are essentially a web3 links. Instead of using nonobvious and mutable thing: https://github.com/cosmos/cosmos/blob/master/WHITEPAPER.md we can use pretty much exact thing: Qme4z71Zea9xaXScUi6pbsuTKCCNFp5TAv8W5tjdfH7yuHhttps Using content addresses for building a knowledge graph we get so much needed superpowers of ipfs - like p2p protocols for a search engine: mesh-network future proof interplanetary tolerant accessible technology agnostic Web3 agents generate our knowledge graph. Web3 agents include itself to the knowledge graph by transacting only once. Thereby they prove the existence of private keys for content addresses of revealed public keys. Our euler implementation is based on cosmos-sdk identities and cidv0 content addresses. Web 3 agents generate knowledge graph by applying cyberlinks. Cyberlinks \u00b6 To understand cyberlinks, we need to understand the difference between URL link and IPFS link. URL link points to the location of content, but IPFS link point to the content itself. The difference in web architecture based on location links and content links is drastical, hence require new approaches. Cyberlink is an approach to link two content addresses semantically. QmdvsvrVqdkzx8HnowpXGLi88tXZDsoNrGhGvPvHBQB6sH.QmdSQ1AGTizWjSRaVLJ8Bw9j1xi6CGLptNUcUodBwCkKNS This cyberlink means that cyberd presentation on cyberc0n is referencing Tezos whitepaper. A concept of cyberlink is a convention around simple semantics of communication format in any peer to peer network: <content-address x>.<content-address y> You can see that cyberlink represents a link between two links. Easy peasy! Cyberlink is a simple yet powerful semantic construction for building a predictive model of the universe. Cyberlinks can form link chains if exist a series of two cyberlinks from one agent in which the second link in the first cyberlink is equal to the first link in the second cyberlink: <content-address x>.<content-address y> <content-address y>.<content-address z> Using this simple principle, all interacting agents can reach consensus around interpreting clauses. So link chains are helpful for interpreting rich communications around relevance. Also using the following link: QmNedUe2wktW65xXxWqcR8EWWssHVMXm3Ly4GKiRRSEBkn the one can signal the start and stop of execution in the knowledge graph. If web3 agents expand native IPFS links with something semantically richer as DURA links than web3 agents can easier to reach consensus on the rules for program execution. Indeed, DURA protocol is a proper implementation of a cyberlinks concept. euler implementation of cyberlinks based on DURA specification is available in .cyber app of browser cyb . Based on cyberlinks we can compute the relevance of subjects and objects in a knowledge graph. That is why we need a consensus computer. Notion of consensus computer \u00b6 Consensus computer is an abstract computing machine that emerges from agents interactions. A consensus computer has a capacity in terms of fundamental computing resources such as memory and computing. To interact with agents, a computer needs a bandwidth. Ideal consensus computer is a computer in which: the sum of all *individual agents* computations and memory is equal to the sum of all verified by agents computations and memory of a *consensus computer* We know that: verifications of computations < computations + verifications of computations Hence we will not be able to achieve an ideal consensus computer ever. CAP theorem and scalability trilemma also prove this statement. However, this theory can work as a performance indicator of a consensus computer. The euler implementation is a 64-bit consensus computer of the relevance for 64-byte string space that is as far from ideal at least as 1/146. We must bind computational, storage and bandwidth supply of relevance machine with maximized demand of queries. Computation and storage in case of basic relevance machine can be easily predicted based on bandwidth, but bandwidth requires a limiting mechanism. Bandwidth limiting mechanism is work in progress. Current notes on implementation are in the docs . So agents must have CYB tokens in accordance to their will of learning the knowledge graph. However, proposed mechanics of CYB tokens work not only as spam protection but as the economic regulation mechanism to align the ability of validators to process knowledge graph and market demand for processing. Relevance machine \u00b6 Relevance machine is a machine that transition knowledge graph state based on some reputation score of agents. This machine enables simple construction for search question querying and answers delivering. The reputation score is projected on every agent's cyberlink. A simple rule prevents agents abuse: one content address can be voted by a token only once. So it does not matter for ranking from how much accounts you voted. The only sum of their balances matters. A useful property of a relevance machine is that it must have inductive reasoning property or follows the blackbox principle. She must be able to interfere predictions without any knowledge about objects except who linked, when linked and what was linked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirement for a computer for memory and computations. That is, deduction of meaning inside consensus computer is expensive thus our design depends on the blindness assumption. Instead of deducting a meaning inside consensus computer we design a system in which meaning extraction is incentivized because agents need CYB to compute relevance. Also, thanks to content addressing the relevance machine following the blackbox principle do not need to store the data but can effectively operate on it. Human intelligence organized in a way to prune none-relevant and none-important memories with time has passed. The same way can do relevance machine. Also, one useful property of relevance machine is that it needs to store neither past state nor full current state to remain useful, or more precisely: relevant . So relevance machine can implement aggressive pruning strategies such as pruning all history of knowledge graph formation or forgetting links that become non-relevant. The pruning group of features can be implemented in nash . euler implementation of relevance machine is based on the most straightforward mechanism which is called cyber\u2022Rank. cyber\u2022Rank \u00b6 Ranking using consensus computer is hard because consensus computers bring serious resource bounds. e.g. Nebulas still fail to deliver something useful on-chain. First, we must ask ourselves why do we need to compute and store the rank on-chain, and not go Colony or Truebit way? If rank computed inside consensus computer, you have an easy content distribution of the rank as well as an easy way to build provable applications on top of the rank. Hence we decided to follow more cosmic architecture. In the next section, we describe the proof of relevance mechanism which allows the network to scale with the help of domain-specific relevance machines that works in parallel. Eventually, relevance machine needs to find (1) deterministic algorithm that allows computing a rank for a continuously appended network to scale the consensus computer to orders of magnitude that of Google. Perfect algorithm (2) must have linear memory and computation complexity. The most importantly it must have (3) highest provable prediction capabilities for the existence of relevant links. After some research, we found that we can not find silver bullet here. We find an algorithm that probably satisfies our criteria: SpringRank . An original idea of the algorithm came to Caterina from physics. Links represented as a system of springs with some energy, and the task of computing the ranks is the task of finding a relaxed state of springs. However, we got at least 3 problems with SpringRank: 1. We were not able to implement it on-chain fast using Go in euler . 2. We were not able to prove it for knowledge graph because we did not have provable knowledge graph yet. 3. Also, we were not able to prove it by applying it for the Ethereum blockchain during computing the genesis file for euler . It could work, but for the time being it is better to call this kind of distribution a lottery. So we decided to find some more basic bulletproof way to bootstrap the network: a rank from which Lary and Sergey have bootstrapped a previous network. The problem with original PageRank is that it is not resistant to sybil attacks. Token weighted PageRank limited by token-weighted bandwidth do not have inherent problems of naive PageRank and is resistant to sybil attacks. For the time being, we will call it cyber\u2022Rank until something better emerge. In the centre of spam protection system is an assumption that write operations can be executed only by those who have a vested interest in the evolutionary success of a relevance machine. Every 1% of stake in consensus computer gives the ability to use 1% of possible network bandwidth and computing capabilities. As nobody uses all possessed bandwidth, we can safely use 10x fractional reserves with 2-minute recalculation target. We would love to discuss the problem of vote buying mainly. Vote buying by itself is not such bad. The problem with vote buying appears in the systems where voting affects the allocation of inflation in the system like Steem or any state-based system. So vote buying can become easily profitable for adversary employing a zero-sum game without a necessity to add value. Our original idea of a decentralized search was based on this approach, but we reject this idea completely removing incentive on consensus level for knowledge graph formation completely. In our setting in which every participant must bring some value to the system to affect predictive model vote buying become NP-hard problem hence is useful for the system. To switch from one algorithm to another, we are going to make simulations and experiment with economic a/b testing based on winning chains through hard spoons. Consensus computer based on relevance machine for cyber\u2022Rank can answer and deliver relevant results for any given search request in the 64 byte CID space. However, to build a network of domain-specific relevance machines, it is not enough. Consensus computers must have the ability to prove relevance for each other. Proof of relevance \u00b6 We design a system under the assumption that regarding search such thing as bad behaviour does not exist as anything bad can be in the intention of finding answers. Also, this approach significantly reduces attack surfaces. Ranks are computed on the only fact that something has been searched, thus linked and as a result, affected the predictive model. A good analogy is observing in quantum mechanics. That is why we do not need such things as negative voting. Doing this we remove subjectivity out of the protocol and can define proof of relevance. Rank state = rank values stored in a one-dimensional array and merkle tree of those values Each new CID gets a unique number. The number starts from zero and incrementing by one for each new CID. So that we can store rank in a one-dimensional array where indices are CID numbers. Merkle Tree calculated based on RFC-6962 standard . Since rank stored in a one-dimensional array where indices are CID numbers (we could say that it ordered by CID numbers) leaves in merkle tree from left to right are SHA-256 hashes of rank value. Index of the leaf is CID number. It helps to easily find proofs for specified CID ( log n iterations where n is a number of leaves). To store merkle tree is necessary to split the tree into subtrees with a number of leaves multiply of the power of 2. The smallest one is obviously subtree with only one leaf (and therefore height == 0 ). Leaf addition looks as follows. Each new leaf is added as subtree with height == 0 . Then sequentially merge subtrees with the same height from right to left. Example: \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u2502 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u2502 (5-leaf) (6-leaf) (7-leaf) To get merkle root hash - join subtree roots from right to left. Rank merkle tree can be stored differently: Full tree - all subtrees with all leaves and intermediary nodes Short tree - contains only subtrees roots The trick is that full tree is only necessary for providing merkle proofs. For consensus purposes and updating tree, it's enough to have a short tree . To store merkle tree in database use only a short tree . Marshaling of a short tree with n subtrees (each subtree takes 40 bytes): <subtree_1_root_hash_bytes><subtree_1_height_bytes> .... <subtree_n_root_hash_bytes><subtree_n_height_bytes> For 1,099,511,627,775 leaves short tree would contain only 40 subtrees roots and take only 1600 bytes. Let us denote rank state calculation: p - rank calculation period lbn - last confirmed block number cbn - current block number lr - length of rank values array For rank storing and calculation we have two separate in-memory contexts: Current rank context. It includes the last calculated rank state (values and merkle tree) plus all links and user stakes submitted to the moment of this rank submission. New rank context. It's currently calculating (or already calculated and waiting for submission) rank state. Consists of new calculated rank state (values and merkle tree) plus new incoming links and updated user stakes. Calculation of new rank state happens once per p blocks and going in parallel. The iteration starts from block number that \u2261 0 (mod p) and goes till next block number that \u2261 0 (mod p) . For block number cbn \u2261 0 (mod p) (including block number 1 cause in cosmos blocks starts from 1): Check if the rank calculation is finished. If yes then go to (2.) if not - wait till calculation finished (actually this situation should not happen because it means that rank calculation period is too short). Submit rank, links and user stakes from new rank context to current rank context. Store last calculated rank merkle tree root hash. Start new rank calculation in parallel (on links and stakes from current rank context). For each block: All links go to a new rank context. New coming CIDs gets rank equals to zero. We could do it by checking last CIDs number and lr (it equals the number of CIDs that already have rank). Then add CIDs with number >lr to the end of this array with the value equal to zero. Update current context merkle tree with CIDs from the previous step Store latest merkle tree from current context (let us call it last block merkle tree). Check if new rank calculation finished. If yes go to (4.) if not go to next block. Push calculated rank state to new rank context. Store merkle tree of newly calculated rank. To sum up. In current rank context , we have rank state from last calculated iteration (plus, every block, it updates with new CIDs). Moreover, we have links and user stakes that are participating in current rank calculation iteration (whether it finished or not). The new rank context contains links and stakes that will go to next rank calculation and newly calculated rank state (if a calculation is finished) that waiting for submitting. If we need to restart node firstly, we need to restore both contexts (current and new). Load links and user stakes from a database using different versions: 1. Links and stakes from last calculated rank version v = lbn - (lbn mod n) go to current rank context. 2. Links and stakes between versions v and lbn go to new rank context. Also to restart node correctly, we have to store following entities in database: Last calculated rank hash (merkle tree root) A newly calculated rank short merkle tree Last block short merkle tree With last calculated rank hash and newly calculated rank merkle tree we could check if the rank calculation was finished before node restart. If they are equal, then rank wasn't calculated, and we should run the rank calculation. If not we could skip rank calculation and use newly calculated rank merkle tree to participate in consensus when it comes to block number cbn \u2261 0 (mod p) (rank values will not be available until rank calculation happens in next iteration. Still validator can participate in consensus so nothing bad). Last block merkle tree necessary to participate in consensus till the start of next rank calculation iteration. So, after the restart we could end up with two states: 1. Restored current rank context and new rank context without rank values (links, user stakes, and merkle tree). 2. Restored current rank context without rank values. Restored new rank context only with links and user stakes. A node can participate in consensus but cannot provide rank values (and merkle proofs) till two rank calculation iterations finished (current and next). Search index should be run in parallel and do not influence the work of the consensus machine. The validator should be able to turn off index support. Maybe even make it a separate daemon. Base idea. Always submit new links to index and take rank values from current context (insert in sorted array operation). When a new rank state is submitted trigger index to update rank values and do sortings (in most cases new arrays will be almost sorted). Need to solve the problem of adjusting arrays capacity (not to copy arrays each time newly linked cid added). A possible solution is to adjust capacity with reserve before resorting array. Todo: Therefore for building index, we need to find a sorting algorithm that will be fast on almost sorted arrays. Also, we should implement it for GPU so it should better be parallelizable: Mergesort(Timsort), Heapsort, Smoothsort ... Now we have proof of rank of any given content address. While the relevance is still subjective by nature, we have a collective proof that something was relevant for some community at some point in time. For any given CID it is possible to prove the relevance Using this type of proof any two IBC compatible consensus computers can proof the relevance to each other so that domain-specific relevance machines can flourish. Thanks to inter-blockchain communication protocol you basically can launch your own domain-specific search engine either private or public by forking cyberd which is focused on the common public knowledge . So in our search architecture, domain-specific relevance machine can learn from common knowledge. We are going to work on IBC during smith implementation. In our relevance for commons euler implementation proof of relevance root hash is computed on Cuda GPUs every round. Speed and scalability \u00b6 We need speedy confirmation times to feels like the usual web app. It is a strong architecture requirement that shape an economic topology and scalability of the cyber protocol. Proposed blockchain design is based on Tendermint consensus algorithm with 146 validators and has very fast 1 second finality time. Average confirmation timeframe at half the second with asynchronous interaction make complex blockchain search almost invisible for agents. Let us say that our node implementation based on cosmos-sdk can process 10k transactions per second. Thus every day at least 8.64 million agents can submit 100 cyberlinks each and impact results simultaneously. That is enough to verify all assumptions in the wild. As blockchain technology evolves we want to check that every hypothesis work before scale it further. Moreover, proposed design needs demand for full bandwidth in order the relevance become valuable. That is why we strongly focus on accessible, but provable distribution to millions from inception. Implementation in a browser \u00b6 We wanted to imagine how that could work in a web3 browser. To our disappointment we was not able to find the web3 browser that can showcase the coolness of the proposed approach in action. That is why we decide to develop the web3 browser cyb that has sample application .cyber for interacting with cyber:// protocol. From Inception to Genesis \u00b6 It is trivial to develop euler like proof-of-concept implementation, but it is hard to achieve stable protocol merkle a lot of CYB value on which can exist. euler is Inception that already happened, merkle is Genesis that is far away. That is why we decide to innovate a bit on the going main net process. We do not have CYB balances and rank guaranties before merkle but we can have exponentially growing semantic core which can be improved based on measurements and observations during development and gradual transfer of value since euler . So think that Genesis or merkle is very stable and can store semantic core and value, but all releases before can store the whole semantic core and only part of the value you would love to store due to weak security guarantees. The percents of CYB value to be distributed based on CBD balances: euler = 1 smith = 4 darwin = 8 turing = 15 nash = 21 weiner = 25 merkle = 27 To secure the value of CYB before Genesis 100 CBD ERC-20 tokens are issued by cyberFoundation . So snapshot balances are computed 7 times based on CBD. Essentially CBD substance is distributed by cyberFoundation in the following proportion: Proof-of-use: 70% is allocated to web3 agents according to some probabilistic algorithm. E.g., first euler proof-of-use distribution we call Satoshi Lottery is allocated to key owned Ethereum addresses based on ongoing research. First allocation is based on SpringRank. Proof-of-code: 15% is allocated for direct contribution to the code base. E.g., as assigned by cyberFoundation to cyberCongress contribution including team is 11.2% and the other 3.8% allocated to developers community projects such as Gitcoin community and cyberColony based experimental organization. Proof-of-value: 15% is allocated for a direct contribution of funds. 8% of this value either has been already contributed nor has some reservation for ongoing contributions by close friends and 7% is going to be distributed during Eos-like auction not defined precisely yet. All contribution from the auction will go to Aragon based cyberFoundation and will be managed by CBD token holders. Details of code and value distribution can be produced by cyberFoundation. Except for 7 CBD based distributions, CYB tokens can be created only by validators based on staking and slashing parameters. The basic consensus is that newly created CYB tokens are distributed to validators as they do the essential work to make relevance machine run both regarding energy consumed for computation and cost for storage capacity. So validators decide where the tokens can flow further. Validators incentive \u00b6 Validators are the essential building block of the proposed architecture. Hence we want to bring them a better incentive to participate before the main net. In our case validators will compute and process requests for billions edge knowledge graph hence it would be naive to expect that it is possible to expect to prepare such a network for production for free. In the beginning, inflation must be high enough to compensate risks of early investments into the ecosystem. This is approximation of year inflation expressed in percents defined for testnets: euler = 200 smith = 134 darwin = 90 turing = 60 nash = 40 weiner = 27 merkle = 18 The scheme motivates developers to release earlier to be less diluted from holding CBD and honour validators if development is going slower than expected. After Genesis starting inflation rate will become fixed at 1 000 000 000 CYB per block. Join . Once we have validators, we can think about first million web3 agents. Satoshi Lottery \u00b6 Satoshi Lottery is the inception version of the proof-of-use distribution that already happens in the tenth birthday of Bitcoin Genesis at 3 Jan 2019. It is a highly experimental way of provable distribution. The basic idea is that a comprehensive set of agents receive CYB tokens because they behave well. The basic algorithm is of 5 steps: - Compute SpringRank for Ethereum addresses - Sort by SpringRank - Filter top 1M addresses by SpringRank - Compute CYB balances based on CBD - Create genesis for cyber protocol Translation todo: Tolik's article have to be translated here. Next test net we will improve the logic of the lottery based on received data and repeat this every test net until Genesis. Soon you will be able to verify either you were lucky enough to receive CYB or not just searching your ethereum address. If you were, you will be able to claim CYB even without compromising your Ethereum keys. Inception \u00b6 The genesis file for euler containing lottery results and CBD based distribution has the following cid: Qma5U4joYWEf41ku16g9cQr6fADsxCPsiWeYZBxpnpu1D4 132307 accounts with 8 274 000 000 000 000 CYB tokens has been created in Inception of the network. Amount of created tokens is consist of the following sources: - 1% of CYB value allocated to euler testnet based on proof-of-use distribution as planned - 0.7% of CYB value allocated to euler testnet based on proof-of-value and proof-of-code distribution except 11.8 CBD due to bug. Appropriate corrections will be done during scheduled hardfork. Possible applications \u00b6 A lot of cool applications can be built on top of proposed architecture: Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist, Brave, and Metamask. All of them suffer from trying to embed web2 in web3. Our approach is a bit different. We consider web2 as the unsafe subset of web3. That is why we decide to develop a web3 browser that can showcase the cyber approach to answer questions better. Programmable semantic cores . Currently, the most popular keywords in a gigantic semantic core of Google are keywords of apps such as youtube, facebook, github, etc. However, developers have very limited possibility to explain Google how to better structure results. The cyber approach brings this power back to developers. On any given user input string in any application relevant answer can be computed either globally, in the context of an app, a user, a geo or in all of them combined. Search actions . Proposed design enable native support for blockchain asset related activity. It is trivial to design applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Offline search . IPFS make possible easy retrieval of documents from surroundings without a global internet connection. cyberd can itself can be distributed using IPFS. That creates a possibility for ubiquitous offline search. Command tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install go-filecoin: 0.001 BTC per month per GB - apt install siad: 0.0001 BTC per month per GB - apt install storjd: 0.00008 BTC per month per GB According to the best prediction, I made a decision try `mine go-filecoin` Git clone ... Building go-filecoin Starting go-filecoin Creating a wallet using @xhipster seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables the creation of devices which can earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is a simple yet powerful state reality tool with the ability to find particular things. cyberd offers minimalistic but continuously self-improving data source that provides necessary tools for programming economically rational robots. According to top-10000 english words the most popular word in English is defined article the that means a pointer to a particular thing. That fact can be explained as the following: particular things are the most important for us. So the nature of our current semantic computing is to find unique things. Hence the understanding of unique things become essential for robots too. Language convergence . A programmer should not care about what language do the user use. We don't need to know about what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what means what. However, we have the tools to make that dream come true. It is not hard to predict that the shorter a word, the more its cyber\u2022rank will be. Global publicly available list of symbols, words, and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of genuinely global language everybody can accept. Recent scientific advances in machine translation are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers precisely this. This is sure not the exhaustive list of possible applications but very exciting, though. Economic protection is smith \u00b6 About private knowledge on relevance. Explain the difference between private cyberlinks and private relevance machines. The plan for learning the beast. How cyberlink ipfs, wiki, bitcoin and ethereum? Ability to evolve is darwin \u00b6 About the importance of alternative implementation. turing is about computing more \u00b6 Ability to programmatically extend state based on proven knowledge graph is of paramount importance. Thus we consider that WASM programs will be available for execution in cyber consensus computer on top of knowledge graph. Our approach to the economics of consensus computer is that users buy an amount of RAM, CPU, and GPU as they want to execute programs. OpenCypher or GraphQL like language can be provided to explore semantics of the knowledge graph. The following list is simple programs we can envision that can be built on top of simple relevance machine. Self prediction . A consensus computer can continuously build a knowledge graph by itself predicting the existence of cyberlinks and applying these predictions to a state of itself. Hence a consensus computer can participate in the economic consensus of the cyber protocol. Universal oracle. A consensus computer can store the most relevant data in the key-value store, where the key is cid and value is bytes of actual content. She is doing it by making a decision every round about which cid value she want to prune and which she wants to apply based on the utility measure of content addresses in the knowledge graph. To compute utility measure validators check availability and size of content for the top-ranked content address in the knowledge graph, then weight on the size of cids and its ranks. The emergent key-value store will be available to write for consensus computer only and not agents, but values can be used in programs. Proof of location . It is possible to construct cyberlinks with proof-of-location based on some existing protocol such as Foam . So location-based search also can become provable if web3 agents will mine triangulations and attaching proof of location for every link chain. Proof of web3 agent . Agents are a subset of content addresses with one fundamental property: consensus computer can prove the existence of private keys for content addresses for the subset of knowledge graph even if those addresses has never transacted in its own chain. Hence it is possible to compute much provable stuff on top of that knowledge. E.g., some inflation can be distributed to addresses that have never transacted in the cyber network but have the provable link. Motivation for read requests . It would be great to create cybernomics not only for write requests to consensus computer but from read requests also. So read requests can be two order of magnitude cheaper, but guaranteed. Read requests to a search engine can be provided by the second tier of nodes which earn CYB tokens in state channels. We consider implementing state channels based on HTLC and proof verification which unlocks amount earned for already served requests. Prediction markets on link relevance . We can move the idea further by the ranking of knowledge graph based on prediction market on links relevance. An app that allow betting on link relevance can become a unique source of truth for the direction of terms as well as motivate agents to submit more links. Private cyberlinks . Privacy is foundational. While we are committed to privacy achieving implementation of private cyberlinks is unfeasible for our team up to Genesis. Hence it is up to the community to work on wasm programs that can be executed on top of the protocol. The problem is to compute cyberRank based on cyberlink submitted by a web3 agent without revealing neither previous request nor public keys of a web3 agent. Zero-knowledge proofs, in general, are very expensive. We believe that privacy of search should be must by design, but not sure that we know how to implement it. Coda like recursive snarks and mimblewimble constructions, in theory, can solve part of the privacy issue, but they are new, untested and anyway will be more expensive regarding computations than a transparent alternative. In a search for equilibria is nash \u00b6 We need to find answers for a lot of hard questions regarding consensus variables and its default values. So we decide to stick to a community generated feedback on the road to Genesis and continuously adjust them to keep going better. On scalability trilemma ... Decentralization comes with costs and slowness. We want to find a good balance between speed, reliance, and ability to scale, as we believe all three are sensitive for widespread web3 adoption. That is the area of research for us now. We need real economic measurements to apply a scientific method for this class of challenges. On faster evolution at weiner \u00b6 The primary purpose of wiener stage is to be able to update the consensus of a network from a consensus computer state using some on-chain upgrade mechanism. Evolvability and governance are connected tightly. Ability to reflect input from the world and output changes of itself is an essential evolutionary feature. Hence, thanks to cosmos-sdk euler implementation have basic but compelling features such as on-chain voting with vetos and abstain that drastically simplified open discussions for a change. So we are going to use this feature from the inception of the network. However, we can go in a different direction than cosmos-sdk offers. Following ideas from Tezos in weiner we can define the current state of a protocol as the immutable content address that included in round merkle root. Also instead of formal governance procedure, we would love to check the hypothesis that changing state of a protocol is possible indeed using relevance machine itself. Starting protocol can be as simple as follows: The closer some content address to QmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh the more probability that it becomes the winning during an upgrade. The closest protocol to cyber-protocol-current is the protocol which is the most relevant to users. Hence it is up to nodes to signal cyber-protocol-current by sending cyberlinks with semantics like <cQmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh> <cid-of-protocol> . Genesis is secure as merkle \u00b6 Before unleashing our creature, we need to have strong assurance that implementations are secure. Merkle is our final genesis release after security audits and more formalism. After this release, the network of relevance machines become fully functional and evolvable. Conclusion \u00b6 We define and implement a protocol for provable communications of consensus computers on relevance. The protocol is based on a simple idea of content defined knowledge graphs which are generated by web3 agents using cyberlinks. Cyberlinks are processed by a consensus computer using a concept we call relevance machine. euler consensus computer is based on CIDv0 and uses go-ipfs and cosmos-sdk as a foundation. IPFS provide significant benefits regarding resources consumption. CIDv0 as primary objects are robust in its simplicity. For every CIDv0 cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is CYB weighted PageRank with economic protection from sybil attacks and selfish voting. Every round merkle root of the rank tree is published so every computer can prove to any computer a relevance value for a given CID. Sybil resistance is based on bandwidth limiting. Embedded ability to execute programs offer inspiring apps. Starting primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorrent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed semantics of linking offers a robust mechanism for predicting meaningful relations between objects by a consensus computer itself. The source code of a relevance machine is open source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to process it. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today and serve it to millions of web3 agents. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm with standard governance module. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and, e.g. makes it possible to design economically rational self-owned robots that can autonomously understand objects around them. References \u00b6 cyberd Scholarly context adrift Web3 stack Search engines information retrieval in practice Motivating game for adversarial example research An idea of decentralized search IPFS DAT cosmos-sdk CIDv0 Bandwidth in cyber network Thermodynamics of predictions DURA Nebulas Colony Truebit SpringRank PageRank RFC-6962 IBC protocol Tendermint Comparison of web3 browsers Cyb CBD cyberFoundation in Aragon How to become validator in cyber protocol Tolik's article on Satoshi Lottery Top 10000 english words Multilingual neural machine translation Foam Coda Mimblewimble Tezos","title":"Whitepaper"},{"location":"cyberd/cyberd/#cyberd-computing-the-knowledge-from-web3","text":"Notes on euler release of cyber:// protocol reference implementation using Go. cyber\u2022Congress : @xhipster, @litvintech, @hleb-albau, @arturalbov, @belya cyb: - nick. a friendly software robot who helps you explore universes cyber: - noun. a superintelligent network computer for answers - verb. to do something intelligent, to be very smart cyber:// - web3 protocol for computing answers and knowledge exchange CYB: - ticker. transferable token expressing a will to become smarter CYBER: - ticker. non-transferable token measuring intelligence CBD: - ticker. ERC-20 proto token representing substance from which CYB emerge cyberlink: - link type. expressing connection from one link to another as link-x.link-y","title":"cyberd: Computing the knowledge from web3"},{"location":"cyberd/cyberd/#content","text":"cyberd: Computing the knowledge from web3 Content Abstract Introduction to web3 On adversarial examples problem Cyber protocol at euler Knowledge graph Cyberlinks Notion of consensus computer Relevance machine cyber\u2022Rank Proof of relevance Speed and scalability Implementation in a browser From Inception to Genesis Validators incentive Satoshi Lottery Inception Possible applications Economic protection is smith Ability to evolve is darwin turing is about computing more In a search for equilibria is nash On faster evolution at weiner Genesis is secure as merkle Conclusion References","title":"Content"},{"location":"cyberd/cyberd/#abstract","text":"A consensus computer allows computing of provably relevant answers without opinionated blackbox intermediaries such as Google, Youtube, Amazon or Facebook. Stateless content-addressable peer-to-peer communication networks such as IPFS and stateful consensus computers such as Ethereum provide part of the solution, but there are at least three problems associated with implementation. Of course, the first problem is the subjective nature of relevance. The second problem is that it is hard to scale consensus computer for a huge knowledge graph. The third problem is that the quality of such a knowledge graph will suffer from different attack surfaces such as sybil, selfish behaviour of interacting agents. In this paper, we (1) define a protocol for provable consensus computing of relevance between IPFS objects based on Tendermint consensus of cyber\u2022rank computed on GPU, (2) discuss implementation details and (3) design distribution and incentive scheme based on our experience. We believe the minimalistic architecture of the protocol is critical for the formation of a network of domain-specific knowledge consensus computers. As a result of our work some applications never existed before emerge. We expand the work including our vision on features we expect to work up to Genesis.","title":"Abstract"},{"location":"cyberd/cyberd/#introduction-to-web3","text":"Original protocols of the Internet such as TCP/IP, DNS, URL, and HTTPS brought a web into the point where it is now. Along with all the benefits they have created they brought more problem to the table. Globality being a vital property of the web since inception is under real threat. The speed of connections degrades with network grow and from ubiquitous government interventions into privacy and security of web users. One property, not evident in the beginning, become important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they would not break after time has passed . Reliance on \"one at a time ISP\" architecture allows governments effectively censor packets. It is the last straw in a conventional web stack for every engineer who is concerned about the future of our children. Other properties while being not so critical are very desirable: offline and real-time. Average internet user being offline must have the ability to work with the state it has and after acquiring connection being able to sync with global state and continue to verify state's validity in realtime while having a connection. Now, these properties offered on the app level while such properties must be integrated into lower level protocols. The emergence of a web3 stack creates an opportunity for a new kind of Internet. We call it web3. It has a promise to remove problems of a conventional protocol stack and add to the web better speed and more accessible connection. However, as usual in a story with a new stack, new problems emerge. One of such problem is general-purpose search. Existing general-purpose search engines are restrictive centralized databases everybody forced to trust. These search engines were designed primarily for client-server architecture based on TCP/IP, DNS, URL and HTTPS protocols. Web3 creates a challenge and opportunity for a search engine based on developing technologies and specifically designed for them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine in a way inaccessible for previous architectures.","title":"Introduction to web3"},{"location":"cyberd/cyberd/#on-adversarial-examples-problem","text":"Conventional architecture of search engines where one entity process and rank all the shit suffers from one hard but the particular problem that still has not been solved even by brilliant Google scientists: adversarial examples problem . The problem Google acknowledge is that it is rather hard to algorithmically reason either this particular sample is adversarial or not independently on how cool the learning technology is. Obviously, a cryptoeconomic approach can change beneficiaries in this game effectively removing possible sybil attack vectors and removing the necessity to make a decision on example crawling and meaning extraction from one entity to the whole world. Learning sybil-resistant model will probably lead to orders of magnitude more predictive results.","title":"On adversarial examples problem"},{"location":"cyberd/cyberd/#cyber-protocol-at-euler","text":"compute euler inception of cyber protocol based on Satoshi lottery and CBD balances def knowledge graph state take cyberlinks check the validity of signatures check bandwidth limit check the validity of CIDv0 if signatures, bandwidth limit, and CIDv0 are ok than cyberlink is valid for every valid cyberlink emit prediction as an array of CIDv0 every round calculate cyber\u2022rank deltas for the knowledge graph every round distribute CYB based on defined rules apply more secure consensus state based on CBD balances 6 times up to merkle","title":"Cyber protocol at euler"},{"location":"cyberd/cyberd/#knowledge-graph","text":"We represent a knowledge graph as a weighted graph of directed links between content addresses or content identifications or CIDs. In this paper, we will use them as synonyms. Content addresses are essentially a web3 links. Instead of using nonobvious and mutable thing: https://github.com/cosmos/cosmos/blob/master/WHITEPAPER.md we can use pretty much exact thing: Qme4z71Zea9xaXScUi6pbsuTKCCNFp5TAv8W5tjdfH7yuHhttps Using content addresses for building a knowledge graph we get so much needed superpowers of ipfs - like p2p protocols for a search engine: mesh-network future proof interplanetary tolerant accessible technology agnostic Web3 agents generate our knowledge graph. Web3 agents include itself to the knowledge graph by transacting only once. Thereby they prove the existence of private keys for content addresses of revealed public keys. Our euler implementation is based on cosmos-sdk identities and cidv0 content addresses. Web 3 agents generate knowledge graph by applying cyberlinks.","title":"Knowledge graph"},{"location":"cyberd/cyberd/#cyberlinks","text":"To understand cyberlinks, we need to understand the difference between URL link and IPFS link. URL link points to the location of content, but IPFS link point to the content itself. The difference in web architecture based on location links and content links is drastical, hence require new approaches. Cyberlink is an approach to link two content addresses semantically. QmdvsvrVqdkzx8HnowpXGLi88tXZDsoNrGhGvPvHBQB6sH.QmdSQ1AGTizWjSRaVLJ8Bw9j1xi6CGLptNUcUodBwCkKNS This cyberlink means that cyberd presentation on cyberc0n is referencing Tezos whitepaper. A concept of cyberlink is a convention around simple semantics of communication format in any peer to peer network: <content-address x>.<content-address y> You can see that cyberlink represents a link between two links. Easy peasy! Cyberlink is a simple yet powerful semantic construction for building a predictive model of the universe. Cyberlinks can form link chains if exist a series of two cyberlinks from one agent in which the second link in the first cyberlink is equal to the first link in the second cyberlink: <content-address x>.<content-address y> <content-address y>.<content-address z> Using this simple principle, all interacting agents can reach consensus around interpreting clauses. So link chains are helpful for interpreting rich communications around relevance. Also using the following link: QmNedUe2wktW65xXxWqcR8EWWssHVMXm3Ly4GKiRRSEBkn the one can signal the start and stop of execution in the knowledge graph. If web3 agents expand native IPFS links with something semantically richer as DURA links than web3 agents can easier to reach consensus on the rules for program execution. Indeed, DURA protocol is a proper implementation of a cyberlinks concept. euler implementation of cyberlinks based on DURA specification is available in .cyber app of browser cyb . Based on cyberlinks we can compute the relevance of subjects and objects in a knowledge graph. That is why we need a consensus computer.","title":"Cyberlinks"},{"location":"cyberd/cyberd/#notion-of-consensus-computer","text":"Consensus computer is an abstract computing machine that emerges from agents interactions. A consensus computer has a capacity in terms of fundamental computing resources such as memory and computing. To interact with agents, a computer needs a bandwidth. Ideal consensus computer is a computer in which: the sum of all *individual agents* computations and memory is equal to the sum of all verified by agents computations and memory of a *consensus computer* We know that: verifications of computations < computations + verifications of computations Hence we will not be able to achieve an ideal consensus computer ever. CAP theorem and scalability trilemma also prove this statement. However, this theory can work as a performance indicator of a consensus computer. The euler implementation is a 64-bit consensus computer of the relevance for 64-byte string space that is as far from ideal at least as 1/146. We must bind computational, storage and bandwidth supply of relevance machine with maximized demand of queries. Computation and storage in case of basic relevance machine can be easily predicted based on bandwidth, but bandwidth requires a limiting mechanism. Bandwidth limiting mechanism is work in progress. Current notes on implementation are in the docs . So agents must have CYB tokens in accordance to their will of learning the knowledge graph. However, proposed mechanics of CYB tokens work not only as spam protection but as the economic regulation mechanism to align the ability of validators to process knowledge graph and market demand for processing.","title":"Notion of consensus computer"},{"location":"cyberd/cyberd/#relevance-machine","text":"Relevance machine is a machine that transition knowledge graph state based on some reputation score of agents. This machine enables simple construction for search question querying and answers delivering. The reputation score is projected on every agent's cyberlink. A simple rule prevents agents abuse: one content address can be voted by a token only once. So it does not matter for ranking from how much accounts you voted. The only sum of their balances matters. A useful property of a relevance machine is that it must have inductive reasoning property or follows the blackbox principle. She must be able to interfere predictions without any knowledge about objects except who linked, when linked and what was linked. If we assume that a consensus computer must have some information about linked objects the complexity of such model growth unpredictably, hence a requirement for a computer for memory and computations. That is, deduction of meaning inside consensus computer is expensive thus our design depends on the blindness assumption. Instead of deducting a meaning inside consensus computer we design a system in which meaning extraction is incentivized because agents need CYB to compute relevance. Also, thanks to content addressing the relevance machine following the blackbox principle do not need to store the data but can effectively operate on it. Human intelligence organized in a way to prune none-relevant and none-important memories with time has passed. The same way can do relevance machine. Also, one useful property of relevance machine is that it needs to store neither past state nor full current state to remain useful, or more precisely: relevant . So relevance machine can implement aggressive pruning strategies such as pruning all history of knowledge graph formation or forgetting links that become non-relevant. The pruning group of features can be implemented in nash . euler implementation of relevance machine is based on the most straightforward mechanism which is called cyber\u2022Rank.","title":"Relevance machine"},{"location":"cyberd/cyberd/#cyberrank","text":"Ranking using consensus computer is hard because consensus computers bring serious resource bounds. e.g. Nebulas still fail to deliver something useful on-chain. First, we must ask ourselves why do we need to compute and store the rank on-chain, and not go Colony or Truebit way? If rank computed inside consensus computer, you have an easy content distribution of the rank as well as an easy way to build provable applications on top of the rank. Hence we decided to follow more cosmic architecture. In the next section, we describe the proof of relevance mechanism which allows the network to scale with the help of domain-specific relevance machines that works in parallel. Eventually, relevance machine needs to find (1) deterministic algorithm that allows computing a rank for a continuously appended network to scale the consensus computer to orders of magnitude that of Google. Perfect algorithm (2) must have linear memory and computation complexity. The most importantly it must have (3) highest provable prediction capabilities for the existence of relevant links. After some research, we found that we can not find silver bullet here. We find an algorithm that probably satisfies our criteria: SpringRank . An original idea of the algorithm came to Caterina from physics. Links represented as a system of springs with some energy, and the task of computing the ranks is the task of finding a relaxed state of springs. However, we got at least 3 problems with SpringRank: 1. We were not able to implement it on-chain fast using Go in euler . 2. We were not able to prove it for knowledge graph because we did not have provable knowledge graph yet. 3. Also, we were not able to prove it by applying it for the Ethereum blockchain during computing the genesis file for euler . It could work, but for the time being it is better to call this kind of distribution a lottery. So we decided to find some more basic bulletproof way to bootstrap the network: a rank from which Lary and Sergey have bootstrapped a previous network. The problem with original PageRank is that it is not resistant to sybil attacks. Token weighted PageRank limited by token-weighted bandwidth do not have inherent problems of naive PageRank and is resistant to sybil attacks. For the time being, we will call it cyber\u2022Rank until something better emerge. In the centre of spam protection system is an assumption that write operations can be executed only by those who have a vested interest in the evolutionary success of a relevance machine. Every 1% of stake in consensus computer gives the ability to use 1% of possible network bandwidth and computing capabilities. As nobody uses all possessed bandwidth, we can safely use 10x fractional reserves with 2-minute recalculation target. We would love to discuss the problem of vote buying mainly. Vote buying by itself is not such bad. The problem with vote buying appears in the systems where voting affects the allocation of inflation in the system like Steem or any state-based system. So vote buying can become easily profitable for adversary employing a zero-sum game without a necessity to add value. Our original idea of a decentralized search was based on this approach, but we reject this idea completely removing incentive on consensus level for knowledge graph formation completely. In our setting in which every participant must bring some value to the system to affect predictive model vote buying become NP-hard problem hence is useful for the system. To switch from one algorithm to another, we are going to make simulations and experiment with economic a/b testing based on winning chains through hard spoons. Consensus computer based on relevance machine for cyber\u2022Rank can answer and deliver relevant results for any given search request in the 64 byte CID space. However, to build a network of domain-specific relevance machines, it is not enough. Consensus computers must have the ability to prove relevance for each other.","title":"cyber\u2022Rank"},{"location":"cyberd/cyberd/#proof-of-relevance","text":"We design a system under the assumption that regarding search such thing as bad behaviour does not exist as anything bad can be in the intention of finding answers. Also, this approach significantly reduces attack surfaces. Ranks are computed on the only fact that something has been searched, thus linked and as a result, affected the predictive model. A good analogy is observing in quantum mechanics. That is why we do not need such things as negative voting. Doing this we remove subjectivity out of the protocol and can define proof of relevance. Rank state = rank values stored in a one-dimensional array and merkle tree of those values Each new CID gets a unique number. The number starts from zero and incrementing by one for each new CID. So that we can store rank in a one-dimensional array where indices are CID numbers. Merkle Tree calculated based on RFC-6962 standard . Since rank stored in a one-dimensional array where indices are CID numbers (we could say that it ordered by CID numbers) leaves in merkle tree from left to right are SHA-256 hashes of rank value. Index of the leaf is CID number. It helps to easily find proofs for specified CID ( log n iterations where n is a number of leaves). To store merkle tree is necessary to split the tree into subtrees with a number of leaves multiply of the power of 2. The smallest one is obviously subtree with only one leaf (and therefore height == 0 ). Leaf addition looks as follows. Each new leaf is added as subtree with height == 0 . Then sequentially merge subtrees with the same height from right to left. Example: \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2534\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u2502 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u250c\u2500\u2534\u2500\u2510 \u2502 (5-leaf) (6-leaf) (7-leaf) To get merkle root hash - join subtree roots from right to left. Rank merkle tree can be stored differently: Full tree - all subtrees with all leaves and intermediary nodes Short tree - contains only subtrees roots The trick is that full tree is only necessary for providing merkle proofs. For consensus purposes and updating tree, it's enough to have a short tree . To store merkle tree in database use only a short tree . Marshaling of a short tree with n subtrees (each subtree takes 40 bytes): <subtree_1_root_hash_bytes><subtree_1_height_bytes> .... <subtree_n_root_hash_bytes><subtree_n_height_bytes> For 1,099,511,627,775 leaves short tree would contain only 40 subtrees roots and take only 1600 bytes. Let us denote rank state calculation: p - rank calculation period lbn - last confirmed block number cbn - current block number lr - length of rank values array For rank storing and calculation we have two separate in-memory contexts: Current rank context. It includes the last calculated rank state (values and merkle tree) plus all links and user stakes submitted to the moment of this rank submission. New rank context. It's currently calculating (or already calculated and waiting for submission) rank state. Consists of new calculated rank state (values and merkle tree) plus new incoming links and updated user stakes. Calculation of new rank state happens once per p blocks and going in parallel. The iteration starts from block number that \u2261 0 (mod p) and goes till next block number that \u2261 0 (mod p) . For block number cbn \u2261 0 (mod p) (including block number 1 cause in cosmos blocks starts from 1): Check if the rank calculation is finished. If yes then go to (2.) if not - wait till calculation finished (actually this situation should not happen because it means that rank calculation period is too short). Submit rank, links and user stakes from new rank context to current rank context. Store last calculated rank merkle tree root hash. Start new rank calculation in parallel (on links and stakes from current rank context). For each block: All links go to a new rank context. New coming CIDs gets rank equals to zero. We could do it by checking last CIDs number and lr (it equals the number of CIDs that already have rank). Then add CIDs with number >lr to the end of this array with the value equal to zero. Update current context merkle tree with CIDs from the previous step Store latest merkle tree from current context (let us call it last block merkle tree). Check if new rank calculation finished. If yes go to (4.) if not go to next block. Push calculated rank state to new rank context. Store merkle tree of newly calculated rank. To sum up. In current rank context , we have rank state from last calculated iteration (plus, every block, it updates with new CIDs). Moreover, we have links and user stakes that are participating in current rank calculation iteration (whether it finished or not). The new rank context contains links and stakes that will go to next rank calculation and newly calculated rank state (if a calculation is finished) that waiting for submitting. If we need to restart node firstly, we need to restore both contexts (current and new). Load links and user stakes from a database using different versions: 1. Links and stakes from last calculated rank version v = lbn - (lbn mod n) go to current rank context. 2. Links and stakes between versions v and lbn go to new rank context. Also to restart node correctly, we have to store following entities in database: Last calculated rank hash (merkle tree root) A newly calculated rank short merkle tree Last block short merkle tree With last calculated rank hash and newly calculated rank merkle tree we could check if the rank calculation was finished before node restart. If they are equal, then rank wasn't calculated, and we should run the rank calculation. If not we could skip rank calculation and use newly calculated rank merkle tree to participate in consensus when it comes to block number cbn \u2261 0 (mod p) (rank values will not be available until rank calculation happens in next iteration. Still validator can participate in consensus so nothing bad). Last block merkle tree necessary to participate in consensus till the start of next rank calculation iteration. So, after the restart we could end up with two states: 1. Restored current rank context and new rank context without rank values (links, user stakes, and merkle tree). 2. Restored current rank context without rank values. Restored new rank context only with links and user stakes. A node can participate in consensus but cannot provide rank values (and merkle proofs) till two rank calculation iterations finished (current and next). Search index should be run in parallel and do not influence the work of the consensus machine. The validator should be able to turn off index support. Maybe even make it a separate daemon. Base idea. Always submit new links to index and take rank values from current context (insert in sorted array operation). When a new rank state is submitted trigger index to update rank values and do sortings (in most cases new arrays will be almost sorted). Need to solve the problem of adjusting arrays capacity (not to copy arrays each time newly linked cid added). A possible solution is to adjust capacity with reserve before resorting array. Todo: Therefore for building index, we need to find a sorting algorithm that will be fast on almost sorted arrays. Also, we should implement it for GPU so it should better be parallelizable: Mergesort(Timsort), Heapsort, Smoothsort ... Now we have proof of rank of any given content address. While the relevance is still subjective by nature, we have a collective proof that something was relevant for some community at some point in time. For any given CID it is possible to prove the relevance Using this type of proof any two IBC compatible consensus computers can proof the relevance to each other so that domain-specific relevance machines can flourish. Thanks to inter-blockchain communication protocol you basically can launch your own domain-specific search engine either private or public by forking cyberd which is focused on the common public knowledge . So in our search architecture, domain-specific relevance machine can learn from common knowledge. We are going to work on IBC during smith implementation. In our relevance for commons euler implementation proof of relevance root hash is computed on Cuda GPUs every round.","title":"Proof of relevance"},{"location":"cyberd/cyberd/#speed-and-scalability","text":"We need speedy confirmation times to feels like the usual web app. It is a strong architecture requirement that shape an economic topology and scalability of the cyber protocol. Proposed blockchain design is based on Tendermint consensus algorithm with 146 validators and has very fast 1 second finality time. Average confirmation timeframe at half the second with asynchronous interaction make complex blockchain search almost invisible for agents. Let us say that our node implementation based on cosmos-sdk can process 10k transactions per second. Thus every day at least 8.64 million agents can submit 100 cyberlinks each and impact results simultaneously. That is enough to verify all assumptions in the wild. As blockchain technology evolves we want to check that every hypothesis work before scale it further. Moreover, proposed design needs demand for full bandwidth in order the relevance become valuable. That is why we strongly focus on accessible, but provable distribution to millions from inception.","title":"Speed and scalability"},{"location":"cyberd/cyberd/#implementation-in-a-browser","text":"We wanted to imagine how that could work in a web3 browser. To our disappointment we was not able to find the web3 browser that can showcase the coolness of the proposed approach in action. That is why we decide to develop the web3 browser cyb that has sample application .cyber for interacting with cyber:// protocol.","title":"Implementation in a browser"},{"location":"cyberd/cyberd/#from-inception-to-genesis","text":"It is trivial to develop euler like proof-of-concept implementation, but it is hard to achieve stable protocol merkle a lot of CYB value on which can exist. euler is Inception that already happened, merkle is Genesis that is far away. That is why we decide to innovate a bit on the going main net process. We do not have CYB balances and rank guaranties before merkle but we can have exponentially growing semantic core which can be improved based on measurements and observations during development and gradual transfer of value since euler . So think that Genesis or merkle is very stable and can store semantic core and value, but all releases before can store the whole semantic core and only part of the value you would love to store due to weak security guarantees. The percents of CYB value to be distributed based on CBD balances: euler = 1 smith = 4 darwin = 8 turing = 15 nash = 21 weiner = 25 merkle = 27 To secure the value of CYB before Genesis 100 CBD ERC-20 tokens are issued by cyberFoundation . So snapshot balances are computed 7 times based on CBD. Essentially CBD substance is distributed by cyberFoundation in the following proportion: Proof-of-use: 70% is allocated to web3 agents according to some probabilistic algorithm. E.g., first euler proof-of-use distribution we call Satoshi Lottery is allocated to key owned Ethereum addresses based on ongoing research. First allocation is based on SpringRank. Proof-of-code: 15% is allocated for direct contribution to the code base. E.g., as assigned by cyberFoundation to cyberCongress contribution including team is 11.2% and the other 3.8% allocated to developers community projects such as Gitcoin community and cyberColony based experimental organization. Proof-of-value: 15% is allocated for a direct contribution of funds. 8% of this value either has been already contributed nor has some reservation for ongoing contributions by close friends and 7% is going to be distributed during Eos-like auction not defined precisely yet. All contribution from the auction will go to Aragon based cyberFoundation and will be managed by CBD token holders. Details of code and value distribution can be produced by cyberFoundation. Except for 7 CBD based distributions, CYB tokens can be created only by validators based on staking and slashing parameters. The basic consensus is that newly created CYB tokens are distributed to validators as they do the essential work to make relevance machine run both regarding energy consumed for computation and cost for storage capacity. So validators decide where the tokens can flow further.","title":"From Inception to Genesis"},{"location":"cyberd/cyberd/#validators-incentive","text":"Validators are the essential building block of the proposed architecture. Hence we want to bring them a better incentive to participate before the main net. In our case validators will compute and process requests for billions edge knowledge graph hence it would be naive to expect that it is possible to expect to prepare such a network for production for free. In the beginning, inflation must be high enough to compensate risks of early investments into the ecosystem. This is approximation of year inflation expressed in percents defined for testnets: euler = 200 smith = 134 darwin = 90 turing = 60 nash = 40 weiner = 27 merkle = 18 The scheme motivates developers to release earlier to be less diluted from holding CBD and honour validators if development is going slower than expected. After Genesis starting inflation rate will become fixed at 1 000 000 000 CYB per block. Join . Once we have validators, we can think about first million web3 agents.","title":"Validators incentive"},{"location":"cyberd/cyberd/#satoshi-lottery","text":"Satoshi Lottery is the inception version of the proof-of-use distribution that already happens in the tenth birthday of Bitcoin Genesis at 3 Jan 2019. It is a highly experimental way of provable distribution. The basic idea is that a comprehensive set of agents receive CYB tokens because they behave well. The basic algorithm is of 5 steps: - Compute SpringRank for Ethereum addresses - Sort by SpringRank - Filter top 1M addresses by SpringRank - Compute CYB balances based on CBD - Create genesis for cyber protocol Translation todo: Tolik's article have to be translated here. Next test net we will improve the logic of the lottery based on received data and repeat this every test net until Genesis. Soon you will be able to verify either you were lucky enough to receive CYB or not just searching your ethereum address. If you were, you will be able to claim CYB even without compromising your Ethereum keys.","title":"Satoshi Lottery"},{"location":"cyberd/cyberd/#inception","text":"The genesis file for euler containing lottery results and CBD based distribution has the following cid: Qma5U4joYWEf41ku16g9cQr6fADsxCPsiWeYZBxpnpu1D4 132307 accounts with 8 274 000 000 000 000 CYB tokens has been created in Inception of the network. Amount of created tokens is consist of the following sources: - 1% of CYB value allocated to euler testnet based on proof-of-use distribution as planned - 0.7% of CYB value allocated to euler testnet based on proof-of-value and proof-of-code distribution except 11.8 CBD due to bug. Appropriate corrections will be done during scheduled hardfork.","title":"Inception"},{"location":"cyberd/cyberd/#possible-applications","text":"A lot of cool applications can be built on top of proposed architecture: Web3 browsers . It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist, Brave, and Metamask. All of them suffer from trying to embed web2 in web3. Our approach is a bit different. We consider web2 as the unsafe subset of web3. That is why we decide to develop a web3 browser that can showcase the cyber approach to answer questions better. Programmable semantic cores . Currently, the most popular keywords in a gigantic semantic core of Google are keywords of apps such as youtube, facebook, github, etc. However, developers have very limited possibility to explain Google how to better structure results. The cyber approach brings this power back to developers. On any given user input string in any application relevant answer can be computed either globally, in the context of an app, a user, a geo or in all of them combined. Search actions . Proposed design enable native support for blockchain asset related activity. It is trivial to design applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody. Offline search . IPFS make possible easy retrieval of documents from surroundings without a global internet connection. cyberd can itself can be distributed using IPFS. That creates a possibility for ubiquitous offline search. Command tools . Command line tools can rely on relevant and structured answers from a search engine. That practically means that the following CLI tool is possible to implement > cyberd earn using 100 gb hdd Enjoy the following predictions: - apt install go-filecoin: 0.001 BTC per month per GB - apt install siad: 0.0001 BTC per month per GB - apt install storjd: 0.00008 BTC per month per GB According to the best prediction, I made a decision try `mine go-filecoin` Git clone ... Building go-filecoin Starting go-filecoin Creating a wallet using @xhipster seed You address is .... Placing bids ... Waiting for incoming storage requests ... Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots. Autonomous robots . Blockchain technology enables the creation of devices which can earn, store, spend and invest digital assets by themselves. If a robot can earn, store, spend and invest she can do everything you can do What is needed is a simple yet powerful state reality tool with the ability to find particular things. cyberd offers minimalistic but continuously self-improving data source that provides necessary tools for programming economically rational robots. According to top-10000 english words the most popular word in English is defined article the that means a pointer to a particular thing. That fact can be explained as the following: particular things are the most important for us. So the nature of our current semantic computing is to find unique things. Hence the understanding of unique things become essential for robots too. Language convergence . A programmer should not care about what language do the user use. We don't need to know about what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for answering can become distributed across different domain-specific areas, including semantic cores of different languages. The unified approach creates an opportunity for cyber\u2022Bahasa. Since the Internet, we observe a process of rapid language convergence. We use more truly global words across the entire planet independently of our nationality, language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree on what means what. However, we have the tools to make that dream come true. It is not hard to predict that the shorter a word, the more its cyber\u2022rank will be. Global publicly available list of symbols, words, and phrases sorted by cyber\u2022rank with corresponding links provided by cyberd can be the foundation for the emergence of genuinely global language everybody can accept. Recent scientific advances in machine translation are breathtaking but meaningless for those who wish to apply them without Google scale trained model. Proposed cyber\u2022rank offers precisely this. This is sure not the exhaustive list of possible applications but very exciting, though.","title":"Possible applications"},{"location":"cyberd/cyberd/#economic-protection-is-smith","text":"About private knowledge on relevance. Explain the difference between private cyberlinks and private relevance machines. The plan for learning the beast. How cyberlink ipfs, wiki, bitcoin and ethereum?","title":"Economic protection is smith"},{"location":"cyberd/cyberd/#ability-to-evolve-is-darwin","text":"About the importance of alternative implementation.","title":"Ability to evolve is darwin"},{"location":"cyberd/cyberd/#turing-is-about-computing-more","text":"Ability to programmatically extend state based on proven knowledge graph is of paramount importance. Thus we consider that WASM programs will be available for execution in cyber consensus computer on top of knowledge graph. Our approach to the economics of consensus computer is that users buy an amount of RAM, CPU, and GPU as they want to execute programs. OpenCypher or GraphQL like language can be provided to explore semantics of the knowledge graph. The following list is simple programs we can envision that can be built on top of simple relevance machine. Self prediction . A consensus computer can continuously build a knowledge graph by itself predicting the existence of cyberlinks and applying these predictions to a state of itself. Hence a consensus computer can participate in the economic consensus of the cyber protocol. Universal oracle. A consensus computer can store the most relevant data in the key-value store, where the key is cid and value is bytes of actual content. She is doing it by making a decision every round about which cid value she want to prune and which she wants to apply based on the utility measure of content addresses in the knowledge graph. To compute utility measure validators check availability and size of content for the top-ranked content address in the knowledge graph, then weight on the size of cids and its ranks. The emergent key-value store will be available to write for consensus computer only and not agents, but values can be used in programs. Proof of location . It is possible to construct cyberlinks with proof-of-location based on some existing protocol such as Foam . So location-based search also can become provable if web3 agents will mine triangulations and attaching proof of location for every link chain. Proof of web3 agent . Agents are a subset of content addresses with one fundamental property: consensus computer can prove the existence of private keys for content addresses for the subset of knowledge graph even if those addresses has never transacted in its own chain. Hence it is possible to compute much provable stuff on top of that knowledge. E.g., some inflation can be distributed to addresses that have never transacted in the cyber network but have the provable link. Motivation for read requests . It would be great to create cybernomics not only for write requests to consensus computer but from read requests also. So read requests can be two order of magnitude cheaper, but guaranteed. Read requests to a search engine can be provided by the second tier of nodes which earn CYB tokens in state channels. We consider implementing state channels based on HTLC and proof verification which unlocks amount earned for already served requests. Prediction markets on link relevance . We can move the idea further by the ranking of knowledge graph based on prediction market on links relevance. An app that allow betting on link relevance can become a unique source of truth for the direction of terms as well as motivate agents to submit more links. Private cyberlinks . Privacy is foundational. While we are committed to privacy achieving implementation of private cyberlinks is unfeasible for our team up to Genesis. Hence it is up to the community to work on wasm programs that can be executed on top of the protocol. The problem is to compute cyberRank based on cyberlink submitted by a web3 agent without revealing neither previous request nor public keys of a web3 agent. Zero-knowledge proofs, in general, are very expensive. We believe that privacy of search should be must by design, but not sure that we know how to implement it. Coda like recursive snarks and mimblewimble constructions, in theory, can solve part of the privacy issue, but they are new, untested and anyway will be more expensive regarding computations than a transparent alternative.","title":"turing is about computing more"},{"location":"cyberd/cyberd/#in-a-search-for-equilibria-is-nash","text":"We need to find answers for a lot of hard questions regarding consensus variables and its default values. So we decide to stick to a community generated feedback on the road to Genesis and continuously adjust them to keep going better. On scalability trilemma ... Decentralization comes with costs and slowness. We want to find a good balance between speed, reliance, and ability to scale, as we believe all three are sensitive for widespread web3 adoption. That is the area of research for us now. We need real economic measurements to apply a scientific method for this class of challenges.","title":"In a search for equilibria is nash"},{"location":"cyberd/cyberd/#on-faster-evolution-at-weiner","text":"The primary purpose of wiener stage is to be able to update the consensus of a network from a consensus computer state using some on-chain upgrade mechanism. Evolvability and governance are connected tightly. Ability to reflect input from the world and output changes of itself is an essential evolutionary feature. Hence, thanks to cosmos-sdk euler implementation have basic but compelling features such as on-chain voting with vetos and abstain that drastically simplified open discussions for a change. So we are going to use this feature from the inception of the network. However, we can go in a different direction than cosmos-sdk offers. Following ideas from Tezos in weiner we can define the current state of a protocol as the immutable content address that included in round merkle root. Also instead of formal governance procedure, we would love to check the hypothesis that changing state of a protocol is possible indeed using relevance machine itself. Starting protocol can be as simple as follows: The closer some content address to QmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh the more probability that it becomes the winning during an upgrade. The closest protocol to cyber-protocol-current is the protocol which is the most relevant to users. Hence it is up to nodes to signal cyber-protocol-current by sending cyberlinks with semantics like <cQmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh> <cid-of-protocol> .","title":"On faster evolution at weiner"},{"location":"cyberd/cyberd/#genesis-is-secure-as-merkle","text":"Before unleashing our creature, we need to have strong assurance that implementations are secure. Merkle is our final genesis release after security audits and more formalism. After this release, the network of relevance machines become fully functional and evolvable.","title":"Genesis is secure as merkle"},{"location":"cyberd/cyberd/#conclusion","text":"We define and implement a protocol for provable communications of consensus computers on relevance. The protocol is based on a simple idea of content defined knowledge graphs which are generated by web3 agents using cyberlinks. Cyberlinks are processed by a consensus computer using a concept we call relevance machine. euler consensus computer is based on CIDv0 and uses go-ipfs and cosmos-sdk as a foundation. IPFS provide significant benefits regarding resources consumption. CIDv0 as primary objects are robust in its simplicity. For every CIDv0 cyber\u2022rank is computed by a consensus computer with no single point of failure. Cyber\u2022rank is CYB weighted PageRank with economic protection from sybil attacks and selfish voting. Every round merkle root of the rank tree is published so every computer can prove to any computer a relevance value for a given CID. Sybil resistance is based on bandwidth limiting. Embedded ability to execute programs offer inspiring apps. Starting primary goal is indexing of peer-to-peer systems with self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorrent, or stateful such as Bitcoin, Ethereum and other blockchains and tangles. Proposed semantics of linking offers a robust mechanism for predicting meaningful relations between objects by a consensus computer itself. The source code of a relevance machine is open source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to process it. The performance of proposed software implementation is sufficient for seamless user interactions. Scalability of proposed implementation is enough to index all self-authenticated data that exist today and serve it to millions of web3 agents. The blockchain is managed by a decentralized autonomous organization which functions under Tendermint consensus algorithm with standard governance module. Thought a system provide necessary utility to offer an alternative for conventional search engines it is not limited to this use case either. The system is extendable for numerous applications and, e.g. makes it possible to design economically rational self-owned robots that can autonomously understand objects around them.","title":"Conclusion"},{"location":"cyberd/cyberd/#references","text":"cyberd Scholarly context adrift Web3 stack Search engines information retrieval in practice Motivating game for adversarial example research An idea of decentralized search IPFS DAT cosmos-sdk CIDv0 Bandwidth in cyber network Thermodynamics of predictions DURA Nebulas Colony Truebit SpringRank PageRank RFC-6962 IBC protocol Tendermint Comparison of web3 browsers Cyb CBD cyberFoundation in Aragon How to become validator in cyber protocol Tolik's article on Satoshi Lottery Top 10000 english words Multilingual neural machine translation Foam Coda Mimblewimble Tezos","title":"References"},{"location":"cyberd/get_CYB/","text":"How to get CYB? \u00b6 There are 3 ways to get tokens that allow you to participate in testnet either as a validator or as an search agent: Get it for free . 1 million Ethereum addresses have been included in the genesis block of cyber protocol based on the ranking defined in paper. You can verify how many tokens had been allocated to your address, by downloading web3 browser Cyb and searching for genesis there. Our roadmap has several distribution events before the genesis of the mainnet so we will probably expand ways to drop tokens in the future. Buy OTC . There is a way to buy CBD tokens from other web3 agents. Go to https://t.me/cybotc group and try to find someone who will sell you some tokens. Earn through coding . Go to Gitcoin and complete a bounty . For every completed bounty we have a 1B CYB prize tokens, as well as CBD prize. It is done so for a reason, that acquiring tokens in the testnetwork is difficult. Participation in the testnet requires certain skills from agents. So early participation make sense only for experienced users. How to get CBD? \u00b6 CBD is an ERC-20 proto token that serves a purpose to make CYB distribution more reliable and convinient for investors, and for the developers of the cyber protocol. Based on CBD balances, CYB balances are computed every new testnetwork according to the allocation model in the roadmap. It is much easier to get CBD than CYB, because they can be traded on any Ethereum DEX easily. The drawback of CBD is that you will need to wait for the next phase of development in order to actually use any part of the CBD value in the testnetwork, either as a validator or as an agent. There are several ways to buy CBD tokens: Buy OTC . Go to https://t.me/cybotc group and try to find someone who will sell you some CBD tokens. Buy on Forkdelta . Go to https://forkdelta.app/#!/trade/0x136c1121f21c29415D8cd71F8Bb140C7fF187033-ETH . Maybe someone is selling some CBD there? Contribute to the community . You can wait a bit, until the cyberFoundation formation event. You will be then, able to get some CBD for donating ETH. All ETH donated during the formation event, will become the ownership of the DAO, managed by the contributors.","title":"Get CYB"},{"location":"cyberd/get_CYB/#how-to-get-cyb","text":"There are 3 ways to get tokens that allow you to participate in testnet either as a validator or as an search agent: Get it for free . 1 million Ethereum addresses have been included in the genesis block of cyber protocol based on the ranking defined in paper. You can verify how many tokens had been allocated to your address, by downloading web3 browser Cyb and searching for genesis there. Our roadmap has several distribution events before the genesis of the mainnet so we will probably expand ways to drop tokens in the future. Buy OTC . There is a way to buy CBD tokens from other web3 agents. Go to https://t.me/cybotc group and try to find someone who will sell you some tokens. Earn through coding . Go to Gitcoin and complete a bounty . For every completed bounty we have a 1B CYB prize tokens, as well as CBD prize. It is done so for a reason, that acquiring tokens in the testnetwork is difficult. Participation in the testnet requires certain skills from agents. So early participation make sense only for experienced users.","title":"How to get CYB?"},{"location":"cyberd/get_CYB/#how-to-get-cbd","text":"CBD is an ERC-20 proto token that serves a purpose to make CYB distribution more reliable and convinient for investors, and for the developers of the cyber protocol. Based on CBD balances, CYB balances are computed every new testnetwork according to the allocation model in the roadmap. It is much easier to get CBD than CYB, because they can be traded on any Ethereum DEX easily. The drawback of CBD is that you will need to wait for the next phase of development in order to actually use any part of the CBD value in the testnetwork, either as a validator or as an agent. There are several ways to buy CBD tokens: Buy OTC . Go to https://t.me/cybotc group and try to find someone who will sell you some CBD tokens. Buy on Forkdelta . Go to https://forkdelta.app/#!/trade/0x136c1121f21c29415D8cd71F8Bb140C7fF187033-ETH . Maybe someone is selling some CBD there? Contribute to the community . You can wait a bit, until the cyberFoundation formation event. You will be then, able to get some CBD for donating ETH. All ETH donated during the formation event, will become the ownership of the DAO, managed by the contributors.","title":"How to get CBD?"},{"location":"cyberd/overview/","text":"://cyber staking and distribution overview \u00b6 Params \u00b6 Module Param Value Comment Staking UnbondingTime 3 weeks time duration of unbonding Staking MaxValidators 146 maximum number of active validators set Staking MaxEntries 7 max entries for either unbonding delegation or redelegation per delegator/validator pair(delegator/validator/validator for redelegation) Consensus MaxBytes 1mb block max bytes limit Rank RankCalcWindow 100 full rank recalculation window Distr CommunityTax 0% community funding tax, not used Distr BaseProposerReward 1% % of block inflation goes to proposer Distr BonusProposerReward 4% addition reward, calculated as % of included votes from validators set Slashing MaxEvidenceAge 3weeks misbehaviour evidence max age Slashing SignedBlocksWindow 30min window to calculate validators liveness Slashing MinSignedPerWindow 70% min singed block for window to not be jailed Slashing DowntimeJailDuration 0 unjail delay Slashing SlashFractionDoubleSign 20% % of stake reduction for double sign Slashing SlashFractionDowntime 0.1% % of stake reduction for being offline Mint TokensPerBlock 0.634195840 Gcyb validators block rewards Bandwidth RecoveryWindow 24h from 0 to max recovery period Bandwidth PriceSlidingWindow 24h price calculated based on network load for selected period Bandwidth PriceAdjustWindow 1m how ofter price is recalculated Bandwidth PriceMin 0.01 minimum price number (1 means normal price) Bandwidth LinkCost 100 link msg cost Bandwidth NonLinkCost 5 * LinkCost link msg cost Bandwidth TxCost 3 * LinkCost tx cost Bandwidth RecoveryWindowTotalBandwidth 2000 * 1000* LinkCost how much all users in average can spend for recover period Staking \u00b6 The cyberd is a public Proof-Of-Stake (PoS) blockchain, meaning that validator's weight is determined by the amount of staking tokens bonded as collateral. These tokens can be staked directly by the validator or delegated to them by token holders. The weight (i.e. total stake) of a validator determines whether or not it is an active validator, and also how frequently this node will have to propose a block and how much revenue it will obtain. Validator \u00b6 Any user in the system can declare its intention to become a validator by sending a create-validator transaction. From there, they become validators. Validator can set commission , that applied on revenue before it is distributed to their delegators. Each validator holds: - All bounded tokens(self and delegators). NOTE: not include distribution rewards. - Own distribution rewards (commission rewards) - Delegators distribution rewards - All delegators shares. Share is not mapped 1-to-1 to tokens. In a case a validator being punished for misbehaviour, bounded tokens will be reduced, while shares remain a same. Delegation \u00b6 Delegators are token holders who cannot, or do not want to run validator operations themselves. A user can delegate tokens to a validator and obtain a part of its revenue in exchange. Upon delegation a user converts his tokens to validator shares in a rate val_tokens/val_shares . Undelegation \u00b6 A user may want to cancel delegation to specific validator. To do so, he/she send undelegate transaction. Depending on current validator state, either user receive his revenue proportion and bounded tokens back immediately (for unbonded validator), or just start process of undelegation. If a validator is in unbonding state, than a user will receive tokens at a validator unbonding time. In last case, a user will wait full UnbondingTime period. Slashing \u00b6 If validators double sign, are frequently offline or do not participate in governance, their staked tokens (including tokens of users that delegated to them) can be destroyed, or 'slashed'. At the beginning of each block, we update the signing info for each validator and check if they've dipped below the liveness threshold MinSignedPerWindow over the tracked window SignedBlocksWindow . If so, their stake will be slashed by SlashFractionDowntime percentage and will be Jailed for DowntimeJailDuration . Distribution \u00b6 All minted tokens goes to fees pool. At each beginblock , the fees received on previous block are allocated to the proposer, community fund, and previous block active validators set according to next scheme: When the validator is the proposer of the round, that validator (and their delegators) receives between BaseProposerReward and BonusProposerReward of fee rewards. The amount of proposer reward is calculated from pre-commits Tendermint messages in order to incentives validators to wait and include additional pre-commits in the block. Community tax is then charged from full fees. The remainder is distributed proportionally by voting power to all bonded validators(and their delegators) independent of whether they voted (social distribution).","title":"Overview"},{"location":"cyberd/overview/#cyber-staking-and-distribution-overview","text":"","title":"://cyber staking and distribution overview"},{"location":"cyberd/overview/#params","text":"Module Param Value Comment Staking UnbondingTime 3 weeks time duration of unbonding Staking MaxValidators 146 maximum number of active validators set Staking MaxEntries 7 max entries for either unbonding delegation or redelegation per delegator/validator pair(delegator/validator/validator for redelegation) Consensus MaxBytes 1mb block max bytes limit Rank RankCalcWindow 100 full rank recalculation window Distr CommunityTax 0% community funding tax, not used Distr BaseProposerReward 1% % of block inflation goes to proposer Distr BonusProposerReward 4% addition reward, calculated as % of included votes from validators set Slashing MaxEvidenceAge 3weeks misbehaviour evidence max age Slashing SignedBlocksWindow 30min window to calculate validators liveness Slashing MinSignedPerWindow 70% min singed block for window to not be jailed Slashing DowntimeJailDuration 0 unjail delay Slashing SlashFractionDoubleSign 20% % of stake reduction for double sign Slashing SlashFractionDowntime 0.1% % of stake reduction for being offline Mint TokensPerBlock 0.634195840 Gcyb validators block rewards Bandwidth RecoveryWindow 24h from 0 to max recovery period Bandwidth PriceSlidingWindow 24h price calculated based on network load for selected period Bandwidth PriceAdjustWindow 1m how ofter price is recalculated Bandwidth PriceMin 0.01 minimum price number (1 means normal price) Bandwidth LinkCost 100 link msg cost Bandwidth NonLinkCost 5 * LinkCost link msg cost Bandwidth TxCost 3 * LinkCost tx cost Bandwidth RecoveryWindowTotalBandwidth 2000 * 1000* LinkCost how much all users in average can spend for recover period","title":"Params"},{"location":"cyberd/overview/#staking","text":"The cyberd is a public Proof-Of-Stake (PoS) blockchain, meaning that validator's weight is determined by the amount of staking tokens bonded as collateral. These tokens can be staked directly by the validator or delegated to them by token holders. The weight (i.e. total stake) of a validator determines whether or not it is an active validator, and also how frequently this node will have to propose a block and how much revenue it will obtain.","title":"Staking"},{"location":"cyberd/overview/#validator","text":"Any user in the system can declare its intention to become a validator by sending a create-validator transaction. From there, they become validators. Validator can set commission , that applied on revenue before it is distributed to their delegators. Each validator holds: - All bounded tokens(self and delegators). NOTE: not include distribution rewards. - Own distribution rewards (commission rewards) - Delegators distribution rewards - All delegators shares. Share is not mapped 1-to-1 to tokens. In a case a validator being punished for misbehaviour, bounded tokens will be reduced, while shares remain a same.","title":"Validator"},{"location":"cyberd/overview/#delegation","text":"Delegators are token holders who cannot, or do not want to run validator operations themselves. A user can delegate tokens to a validator and obtain a part of its revenue in exchange. Upon delegation a user converts his tokens to validator shares in a rate val_tokens/val_shares .","title":"Delegation"},{"location":"cyberd/overview/#undelegation","text":"A user may want to cancel delegation to specific validator. To do so, he/she send undelegate transaction. Depending on current validator state, either user receive his revenue proportion and bounded tokens back immediately (for unbonded validator), or just start process of undelegation. If a validator is in unbonding state, than a user will receive tokens at a validator unbonding time. In last case, a user will wait full UnbondingTime period.","title":"Undelegation"},{"location":"cyberd/overview/#slashing","text":"If validators double sign, are frequently offline or do not participate in governance, their staked tokens (including tokens of users that delegated to them) can be destroyed, or 'slashed'. At the beginning of each block, we update the signing info for each validator and check if they've dipped below the liveness threshold MinSignedPerWindow over the tracked window SignedBlocksWindow . If so, their stake will be slashed by SlashFractionDowntime percentage and will be Jailed for DowntimeJailDuration .","title":"Slashing"},{"location":"cyberd/overview/#distribution","text":"All minted tokens goes to fees pool. At each beginblock , the fees received on previous block are allocated to the proposer, community fund, and previous block active validators set according to next scheme: When the validator is the proposer of the round, that validator (and their delegators) receives between BaseProposerReward and BonusProposerReward of fee rewards. The amount of proposer reward is calculated from pre-commits Tendermint messages in order to incentives validators to wait and include additional pre-commits in the block. Community tax is then charged from full fees. The remainder is distributed proportionally by voting power to all bonded validators(and their delegators) independent of whether they voted (social distribution).","title":"Distribution"},{"location":"cyberd/rpc/","text":"API reference \u00b6 Cyberd provides a JSON-RPC API. Http endpoint is served under localhost:20657 . WebSockets are the preferred transport for cyberd RPC and are used by applications such as cyb. Default WebSocket connection endpoint for cyberd is ws://localhost:20657/websocket . There are test endpoints available at http://earth.cybernode.ai:34657 and ws://earth.cybernode.ai:34657/websocket . Standard Methods \u00b6 Query Example \u00b6 Query http endpoint using curl: curl --data '{\"method\":\"status\",\"params\":[],\"id\":\"1\",\"jsonrpc\":\"2.0\"}' \\ -H \"Content-Type: application/json\" -X POST earth.cybernode.ai:34657 Query ws endpoint from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"status\" , \"params\" : [], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" })); Method Overview \u00b6 The following is an overview of the RPC methods and their current status. Click the method name for further details such as parameter and return information. # Method Description 1 status Get node info, pubkey, latest block hash, app hash, block height and time. 2 account Get account nonce, pubkey, number, and coins. 3 account_bandwidth Get account bandwidth info for current height. 4 is_link_exist Return true, if given link exist. 5 current_bandwidth_price Returns current bandwidth credit price. 6 index_stats Returns current index entities count. Method Details \u00b6 Method status Parameters None Description Get node info, pubkey, latest block hash, app hash, block height and time. Return to Overview Method account Parameters 1. address (string, required) Description Get account nonce, pubkey, number, and coins. Return to Overview Method account_bandwidth Parameters 1. address (string, required) Description Get account bandwidth info for current height. Return to Overview Method is_link_exist Parameters 1. from (cid, required) 2. to (cid, required) 3. address (string, required) Description Return true, if given link exist. Return to Overview Method current_bandwidth_price Parameters None Description Returns current bandwidth credit price. Return to Overview Method index_stats Parameters None Description Returns current index entities count. Return to Overview Notifications (WebSocket-specific) \u00b6 Cyberd uses standard JSON-RPC notifications to notify clients of changes, rather than requiring clients to poll cyberd for updates. JSON-RPC notifications are a subset of requests, but do not contain an ID. The notification type is categorized by the query params field. Subscribe Example \u00b6 Subscribe for new blocks header from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"subscribe\" , \"params\" : [ \"tm.event='NewBlockHeader'\" ], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" })); Events Overview \u00b6 # Event Description 1 NewBlockHeader Sends block header notification when a new block is committed. 2 CoinsReceived Sends a notification when a new coins is arrived to given address. 3 CoinsSend Sends a notification when a new coins is send from given address. 4 \u0421idsLinked Notification of link created by given address. 5 SignedTxCommitted Notify when any tx for given signer is committed. Events Details \u00b6 NewBlockHeader \u00b6 Event NewBlockHeader Description Sends block header notification when a new block is committed. Query tm.event='NewBlockHeader' Return to Overview CoinsReceived \u00b6 Event CoinsReceived Description Sends a notification when a new coins is arrived to given address. Query tm.event='EventTx' AND recipient='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview CoinsSend \u00b6 Event CoinsSend Description Sends a notification when a new coins is send from given address. Query tm.event='EventTx' AND sender='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview \u0421idsLinked \u00b6 Event \u0421idsLinked Description Notification of link created by given address. Query tm.event='EventTx' AND signer='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' AND action='link' Return to Overview SignedTxCommitted \u00b6 Event SignedTxCommitted Description Notify when any tx for given signer is committed. Query tm.event='EventTx' AND signer='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview","title":"RPC"},{"location":"cyberd/rpc/#api-reference","text":"Cyberd provides a JSON-RPC API. Http endpoint is served under localhost:20657 . WebSockets are the preferred transport for cyberd RPC and are used by applications such as cyb. Default WebSocket connection endpoint for cyberd is ws://localhost:20657/websocket . There are test endpoints available at http://earth.cybernode.ai:34657 and ws://earth.cybernode.ai:34657/websocket .","title":"API reference"},{"location":"cyberd/rpc/#standard-methods","text":"","title":"Standard Methods"},{"location":"cyberd/rpc/#query-example","text":"Query http endpoint using curl: curl --data '{\"method\":\"status\",\"params\":[],\"id\":\"1\",\"jsonrpc\":\"2.0\"}' \\ -H \"Content-Type: application/json\" -X POST earth.cybernode.ai:34657 Query ws endpoint from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"status\" , \"params\" : [], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" }));","title":"Query Example"},{"location":"cyberd/rpc/#method-overview","text":"The following is an overview of the RPC methods and their current status. Click the method name for further details such as parameter and return information. # Method Description 1 status Get node info, pubkey, latest block hash, app hash, block height and time. 2 account Get account nonce, pubkey, number, and coins. 3 account_bandwidth Get account bandwidth info for current height. 4 is_link_exist Return true, if given link exist. 5 current_bandwidth_price Returns current bandwidth credit price. 6 index_stats Returns current index entities count.","title":"Method Overview"},{"location":"cyberd/rpc/#method-details","text":"Method status Parameters None Description Get node info, pubkey, latest block hash, app hash, block height and time. Return to Overview Method account Parameters 1. address (string, required) Description Get account nonce, pubkey, number, and coins. Return to Overview Method account_bandwidth Parameters 1. address (string, required) Description Get account bandwidth info for current height. Return to Overview Method is_link_exist Parameters 1. from (cid, required) 2. to (cid, required) 3. address (string, required) Description Return true, if given link exist. Return to Overview Method current_bandwidth_price Parameters None Description Returns current bandwidth credit price. Return to Overview Method index_stats Parameters None Description Returns current index entities count. Return to Overview","title":"Method Details"},{"location":"cyberd/rpc/#notifications-websocket-specific","text":"Cyberd uses standard JSON-RPC notifications to notify clients of changes, rather than requiring clients to poll cyberd for updates. JSON-RPC notifications are a subset of requests, but do not contain an ID. The notification type is categorized by the query params field.","title":"Notifications (WebSocket-specific)"},{"location":"cyberd/rpc/#subscribe-example","text":"Subscribe for new blocks header from js: let websocket = new WebSocket ( \"ws://earth.cybernode.ai:34657/websocket\" ); websocket . send ( JSON . stringify ({ \"method\" : \"subscribe\" , \"params\" : [ \"tm.event='NewBlockHeader'\" ], \"id\" : \"1\" , \"jsonrpc\" : \"2.0\" }));","title":"Subscribe Example"},{"location":"cyberd/rpc/#events-overview","text":"# Event Description 1 NewBlockHeader Sends block header notification when a new block is committed. 2 CoinsReceived Sends a notification when a new coins is arrived to given address. 3 CoinsSend Sends a notification when a new coins is send from given address. 4 \u0421idsLinked Notification of link created by given address. 5 SignedTxCommitted Notify when any tx for given signer is committed.","title":"Events Overview"},{"location":"cyberd/rpc/#events-details","text":"","title":"Events Details"},{"location":"cyberd/rpc/#newblockheader","text":"Event NewBlockHeader Description Sends block header notification when a new block is committed. Query tm.event='NewBlockHeader' Return to Overview","title":"NewBlockHeader"},{"location":"cyberd/rpc/#coinsreceived","text":"Event CoinsReceived Description Sends a notification when a new coins is arrived to given address. Query tm.event='EventTx' AND recipient='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview","title":"CoinsReceived"},{"location":"cyberd/rpc/#coinssend","text":"Event CoinsSend Description Sends a notification when a new coins is send from given address. Query tm.event='EventTx' AND sender='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview","title":"CoinsSend"},{"location":"cyberd/rpc/#idslinked","text":"Event \u0421idsLinked Description Notification of link created by given address. Query tm.event='EventTx' AND signer='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' AND action='link' Return to Overview","title":"\u0421idsLinked"},{"location":"cyberd/rpc/#signedtxcommitted","text":"Event SignedTxCommitted Description Notify when any tx for given signer is committed. Query tm.event='EventTx' AND signer='cbd1sk3uvpacpjm2t3389caqk4gd9n9gkzq2054yds' Return to Overview","title":"SignedTxCommitted"},{"location":"cyberd/run_validator/","text":"Join Cyberd Network As Validator \u00b6 Note . Currently active dev testnet is euler-1 (substitute with that value). Prepare your server \u00b6 First, you have to setup a server. You are supposed to run your validator node all time, so you will need a reliable server to keep it running. Also, you may consider to use any cloud services like AWS. Cyberd is based on Cosmos SDK written in Go. It should work on any platform which can compile and run programs in Go. However, I strongly recommend running the validator node on a Linux server. Rank calculation on a cyberd is benefit GPU computation. They easy to parallelize that why is the best way is to use GPU. Minimal requirements for the next two weeks (until the middle of February): CPU: 4 cores RAM: 16 GB SSD: 256 GB Connection: 100Mb, Fiber, Stable and low-latency connection GPU: GeForce 1070-1080, CUDA Software: Docker, Ubuntu 16.04/18.04 LTS Recommended requirements: CPU: 6 cores RAM: 64 GB SSD: 512 GB Connection: 100Mb, Fiber, Stable and low-latency connection GPU: GeForce 1070-1080, CUDA Software: Docker, Ubuntu 16.04/18.04 LTS Validator setup \u00b6 Third-party software \u00b6 Cyberd main distribution unit is a docker container. All images are located in default Dockerhub registry . In order to access GPU from the container, Nvidia drivers version 410+ and Nvidia docker runtime should be installed on the host system. For great user experience, we propose you to use portainer - docker containers manager. You can skip any subsection of this if you already had and configured necessary software. Docker installation \u00b6 Update the apt package index: sudo apt-get update Install packages to allow apt to use a repository over HTTPS: sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common May require curl installation apt-get install curl Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" Update the apt package index. sudo apt-get update Install the latest version of Docker CE and containerd, or go to the next step to install a specific version: sudo apt-get install docker-ce docker-ce-cli containerd.io If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. Create the docker group. sudo groupadd docker Add your user to the docker group. sudo usermod -aG docker $USER 8. Reboot the system for the changes to take effect. Portainer installation \u00b6 Before installing Portainer, download the Portainer image from the DockerHub using the docker pull command below. docker pull portainer/portainer Now run Portainer using the simple docker command below. docker run -d --restart always -p 9000 :9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Open your browser and go to: localhost:9000 Set password, chose local tab and click connect . All containers will be available at containers tab. Nvidia drivers installation \u00b6 To proceed first add the ppa:graphics-drivers/ppa repository into your system: sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update Next, identify your graphic card model and recommended driver: ubuntu-drivers devices You should see something like this: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001BA1sv00001462sd000011E4bc03sc00i00 vendor : NVIDIA Corporation model : GP104M [ GeForce GTX 1070 Mobile ] driver : nvidia-driver-390 - third-party free driver : nvidia-driver-410 - third-party free driver : nvidia-driver-396 - third-party free driver : nvidia-driver-415 - third-party free recommended driver : xserver-xorg-video-nouveau - distro free builtin 3. We need 410+ drivers release. As we see v415 is recommended. The command below will install the recommended version of drivers. sudo ubuntu-drivers autoinstall Drivers will install due approximately 10 minutes. DKMS: install completed. Setting up libxdamage1:i386 ( 1 :1.1.4-3 ) ... Setting up libxext6:i386 ( 2 :1.3.3-1 ) ... Setting up libxfixes3:i386 ( 1 :5.0.3-1 ) ... Setting up libnvidia-decode-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up build-essential ( 12 .4ubuntu1 ) ... Setting up libnvidia-gl-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libnvidia-encode-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up nvidia-driver-415 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libxxf86vm1:i386 ( 1 :1.1.4-1 ) ... Setting up libglx-mesa0:i386 ( 18 .0.5-0ubuntu0~18.04.1 ) ... Setting up libglx0:i386 ( 1 .0.0-2ubuntu2.2 ) ... Setting up libgl1:i386 ( 1 .0.0-2ubuntu2.2 ) ... Setting up libnvidia-ifr1-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libnvidia-fbc1-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Processing triggers for libc-bin ( 2 .27-3ubuntu1 ) ... Processing triggers for initramfs-tools ( 0 .130ubuntu3.1 ) ... update-initramfs: Generating /boot/initrd.img-4.15.0-45-generic Reboot the system for the changes to take effect. Check installed drivers nvidia-smi You should see this: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 415.27 Driver Version: 415.27 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1070 Off | 00000000:01:00.0 On | N/A | | N/A 54C P0 36W / N/A | 445MiB / 8117MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 882 G /usr/lib/xorg/Xorg 302MiB | | 0 1046 G /usr/bin/gnome-shell 139MiB | +-----------------------------------------------------------------------------+ Install Nvidia container runtime for docker \u00b6 Add the package repositories curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list You should see this: deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) / Install nvidia-docker2 and reload the Docker daemon configuration sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Test nvidia-smi with the latest official CUDA image docker run --runtime = nvidia --rm nvidia/cuda:10.0-base nvidia-smi Output logs must should coincide as earlier: Unable to find image 'nvidia/cuda:10.0-base' locally 10.0-base: Pulling from nvidia/cuda 38e2e6cd5626: Pull complete 705054bc3f5b: Pull complete c7051e069564: Pull complete 7308e914506c: Pull complete 5260e5fce42c: Pull complete 8e2b19e62adb: Pull complete Digest: sha256:625491db7e15efcc78a529d3a2e41b77ffb5b002015983fdf90bf28955277d68 Status: Downloaded newer image for nvidia/cuda:10.0-base Fri Feb 1 05:41:12 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 415.27 Driver Version: 415.27 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1070 Off | 00000000:01:00.0 On | N/A | | N/A 55C P0 31W / N/A | 445MiB / 8117MiB | 38% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ Your machine is ready to launch fullnode. Cyberd fullnode launching \u00b6 Create folders for keys and data storing where you want: mkdir cyberd mkdir cyberdcli Run fullnode docker run -d --name = cyberd --restart always --runtime = nvidia -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 -v /<path_to_cyberd>/cyberd:/root/.cyberd -v /<path_to_cyberdcli>/cyberdcli:/root/.cyberdcli cyberd/cyberd:<testnet_chain_id> 3. After successful container pulling and launch run to check if your node is connected to the testnet: docker exec cyberd cyberdcli status The possible output looks like this: {\"node_info\":{\"protocol_version\":{\"p2p\":\"6\",\"block\":\"9\",\"app\":\"0\"},\"id\":\"93b776d3eb3f3ce9d9bda7164bc8af3acacff7b6\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"euler-1\",\"version\":\"0.29.1\",\"channels\":\"4020212223303800\",\"moniker\":\"anonymous\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"sync_info\":{\"latest_block_hash\":\"686B4E65415D4E56D3B406153C965C0897D0CE27004E9CABF65064B6A0ED4240\",\"latest_app_hash\":\"0A1F6D260945FD6E926785F07D41049B8060C60A132F5BA49DD54F7B1C5B2522\",\"latest_block_height\":\"45533\",\"latest_block_time\":\"2019-02-01T09:49:19.771375108Z\",\"catching_up\":false},\"validator_info\":{\"address\":\"66098853CF3B61C4313DD487BA21EDF8DECACDF0\",\"pub_key\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"uZrCCdZTJoHE1/v+EvhtZufJgA3zAm1bN4uZA3RyvoY=\"},\"voting_power\":\"0\"}} Your node has started to sync. The syncing process you can see in the terminal. Open a new tab and run following command: docker logs cyberd --follow Or go to localhost:9000 and open logs at cyberd container: Syncing has started. Syncing time depends on your internet bandwidth, connection and blockchain height. As at 2019/02/03 syncing time approximately 15-20 minutes. Once you see in logs that blocks syncing for 1 second your node is synced. Additional information available by API endpoint at localhost:26657 f.e. the number of active validators available here localhost:26657/validators Validator start \u00b6 After your node successful synced you can run validator. Prepare stake address \u00b6 If you already have address with CYB and know seed phrase or private key just restore it into your local keystore. docker exec -ti cyberd cyberdcli keys add <your_key_name> --recover docker exec cyberd cyberdcli keys show <your_key_name> If you have been lucky enought and your Ethereum address has been included in genesis you can import ethereum private key Please, do not import high value Ethereum accounts. This can not be safe! cyberd software is a new software and is not battle tested yet. docker exec -ti cyberd cyberdcli keys add import_private <your_key_name> docker exec cyberd cyberdcli keys show <your_key_name> If you want to create new acccount use the command below. Also, you should send coins to that address to bound them later during validator submitting. docker exec -ti cyberd cyberdcli keys add <your_key_name> docker exec cyberd cyberdcli keys show <your_key_name> is any name you pick to represent this key pair. You have to refer to this later when you use the keys to sign transactions. It will ask you to enter your password twice to encrypt the key. You also need to enter your password when you use your key to sign any transaction. The command returns the address, public key and a seed phrase which you can use it to recover your account if you forget your password later. Keep the seed phrase in a safe place in case you have to use them. The address showing here is your account address. Let\u2019s call this . It stores your assets. Send create validator transaction \u00b6 Validators are actors on the network committing new blocks by submitting their votes. It refers to the node itself, not a single person or a single account. Therefore, the public key here is referring to the node public key, not the public key of the address you have just created. To get the node public key, run the following command: docker exec cyberd cyberd tendermint show-validator It will return a bech32 public key. Let\u2019s call it . The next step you have to declare a validator candidate. The validator candidate is the account which stake the coins. So the validator candidate is an account this time. To declare a validator candidate, run the following command adjusting stake amount and other fields. docker exec -ti cyberd cyberdcli tx staking create-validator \\ --amount = 10000000cyb \\ --pubkey = <your_node_pubkey> \\ --moniker = <your_node_nickname> \\ --trust-node \\ --from = <your_key_name> \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\" \\ --chain-id = <testnet_chain_id> Verify that you validating \u00b6 docker exec -ti cyberd cyberdcli query staking validators --trust-node = true If you see your <your_node_nickname> with status Bonded and Jailed false everything must be good. You are validating the network. Maintenance of validator \u00b6 jailing \u00b6 If your validator go under slashing conditions it first go to jail. After this event operator must unjail it manually. docker exec -ti cyberd cyberdcli tx slashing unjail --from = <your_key_name> --chain-id = <testnet_chain_id> Upgrading of validator \u00b6 Updating is easy as pulling the new docker container and launching it again docker stop cyberd docker rm cyberd docker pull cyberd/cyberd:<testnet_chain_id> docker run -d --name = cyberd --restart always --runtime = nvidia \\ -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 \\ -v /root/cyberd:/root/.cyberd \\ -v /root/cyberdcli:/root/.cyberdcli \\ cyberd/cyberd:<testnet_chain_id> Don't forget to unjail if you was jailed during update.","title":"Run Validator"},{"location":"cyberd/run_validator/#join-cyberd-network-as-validator","text":"Note . Currently active dev testnet is euler-1 (substitute with that value).","title":"Join Cyberd Network As Validator"},{"location":"cyberd/run_validator/#prepare-your-server","text":"First, you have to setup a server. You are supposed to run your validator node all time, so you will need a reliable server to keep it running. Also, you may consider to use any cloud services like AWS. Cyberd is based on Cosmos SDK written in Go. It should work on any platform which can compile and run programs in Go. However, I strongly recommend running the validator node on a Linux server. Rank calculation on a cyberd is benefit GPU computation. They easy to parallelize that why is the best way is to use GPU. Minimal requirements for the next two weeks (until the middle of February): CPU: 4 cores RAM: 16 GB SSD: 256 GB Connection: 100Mb, Fiber, Stable and low-latency connection GPU: GeForce 1070-1080, CUDA Software: Docker, Ubuntu 16.04/18.04 LTS Recommended requirements: CPU: 6 cores RAM: 64 GB SSD: 512 GB Connection: 100Mb, Fiber, Stable and low-latency connection GPU: GeForce 1070-1080, CUDA Software: Docker, Ubuntu 16.04/18.04 LTS","title":"Prepare your server"},{"location":"cyberd/run_validator/#validator-setup","text":"","title":"Validator setup"},{"location":"cyberd/run_validator/#third-party-software","text":"Cyberd main distribution unit is a docker container. All images are located in default Dockerhub registry . In order to access GPU from the container, Nvidia drivers version 410+ and Nvidia docker runtime should be installed on the host system. For great user experience, we propose you to use portainer - docker containers manager. You can skip any subsection of this if you already had and configured necessary software.","title":"Third-party software"},{"location":"cyberd/run_validator/#docker-installation","text":"Update the apt package index: sudo apt-get update Install packages to allow apt to use a repository over HTTPS: sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common May require curl installation apt-get install curl Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" Update the apt package index. sudo apt-get update Install the latest version of Docker CE and containerd, or go to the next step to install a specific version: sudo apt-get install docker-ce docker-ce-cli containerd.io If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. Create the docker group. sudo groupadd docker Add your user to the docker group. sudo usermod -aG docker $USER 8. Reboot the system for the changes to take effect.","title":"Docker installation"},{"location":"cyberd/run_validator/#portainer-installation","text":"Before installing Portainer, download the Portainer image from the DockerHub using the docker pull command below. docker pull portainer/portainer Now run Portainer using the simple docker command below. docker run -d --restart always -p 9000 :9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer Open your browser and go to: localhost:9000 Set password, chose local tab and click connect . All containers will be available at containers tab.","title":"Portainer installation"},{"location":"cyberd/run_validator/#nvidia-drivers-installation","text":"To proceed first add the ppa:graphics-drivers/ppa repository into your system: sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update Next, identify your graphic card model and recommended driver: ubuntu-drivers devices You should see something like this: == /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 == modalias : pci:v000010DEd00001BA1sv00001462sd000011E4bc03sc00i00 vendor : NVIDIA Corporation model : GP104M [ GeForce GTX 1070 Mobile ] driver : nvidia-driver-390 - third-party free driver : nvidia-driver-410 - third-party free driver : nvidia-driver-396 - third-party free driver : nvidia-driver-415 - third-party free recommended driver : xserver-xorg-video-nouveau - distro free builtin 3. We need 410+ drivers release. As we see v415 is recommended. The command below will install the recommended version of drivers. sudo ubuntu-drivers autoinstall Drivers will install due approximately 10 minutes. DKMS: install completed. Setting up libxdamage1:i386 ( 1 :1.1.4-3 ) ... Setting up libxext6:i386 ( 2 :1.3.3-1 ) ... Setting up libxfixes3:i386 ( 1 :5.0.3-1 ) ... Setting up libnvidia-decode-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up build-essential ( 12 .4ubuntu1 ) ... Setting up libnvidia-gl-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libnvidia-encode-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up nvidia-driver-415 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libxxf86vm1:i386 ( 1 :1.1.4-1 ) ... Setting up libglx-mesa0:i386 ( 18 .0.5-0ubuntu0~18.04.1 ) ... Setting up libglx0:i386 ( 1 .0.0-2ubuntu2.2 ) ... Setting up libgl1:i386 ( 1 .0.0-2ubuntu2.2 ) ... Setting up libnvidia-ifr1-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Setting up libnvidia-fbc1-415:i386 ( 415 .27-0ubuntu0~gpu18.04.1 ) ... Processing triggers for libc-bin ( 2 .27-3ubuntu1 ) ... Processing triggers for initramfs-tools ( 0 .130ubuntu3.1 ) ... update-initramfs: Generating /boot/initrd.img-4.15.0-45-generic Reboot the system for the changes to take effect. Check installed drivers nvidia-smi You should see this: +-----------------------------------------------------------------------------+ | NVIDIA-SMI 415.27 Driver Version: 415.27 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1070 Off | 00000000:01:00.0 On | N/A | | N/A 54C P0 36W / N/A | 445MiB / 8117MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 882 G /usr/lib/xorg/Xorg 302MiB | | 0 1046 G /usr/bin/gnome-shell 139MiB | +-----------------------------------------------------------------------------+","title":"Nvidia drivers installation"},{"location":"cyberd/run_validator/#install-nvidia-container-runtime-for-docker","text":"Add the package repositories curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\ sudo apt-key add - distribution = $( . /etc/os-release ; echo $ID$VERSION_ID ) curl -s -L https://nvidia.github.io/nvidia-docker/ $distribution /nvidia-docker.list | \\ sudo tee /etc/apt/sources.list.d/nvidia-docker.list You should see this: deb https://nvidia.github.io/libnvidia-container/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-container-runtime/ubuntu18.04/$(ARCH) / deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) / Install nvidia-docker2 and reload the Docker daemon configuration sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Test nvidia-smi with the latest official CUDA image docker run --runtime = nvidia --rm nvidia/cuda:10.0-base nvidia-smi Output logs must should coincide as earlier: Unable to find image 'nvidia/cuda:10.0-base' locally 10.0-base: Pulling from nvidia/cuda 38e2e6cd5626: Pull complete 705054bc3f5b: Pull complete c7051e069564: Pull complete 7308e914506c: Pull complete 5260e5fce42c: Pull complete 8e2b19e62adb: Pull complete Digest: sha256:625491db7e15efcc78a529d3a2e41b77ffb5b002015983fdf90bf28955277d68 Status: Downloaded newer image for nvidia/cuda:10.0-base Fri Feb 1 05:41:12 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 415.27 Driver Version: 415.27 CUDA Version: 10.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 1070 Off | 00000000:01:00.0 On | N/A | | N/A 55C P0 31W / N/A | 445MiB / 8117MiB | 38% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| +-----------------------------------------------------------------------------+ Your machine is ready to launch fullnode.","title":"Install Nvidia container runtime for docker"},{"location":"cyberd/run_validator/#cyberd-fullnode-launching","text":"Create folders for keys and data storing where you want: mkdir cyberd mkdir cyberdcli Run fullnode docker run -d --name = cyberd --restart always --runtime = nvidia -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 -v /<path_to_cyberd>/cyberd:/root/.cyberd -v /<path_to_cyberdcli>/cyberdcli:/root/.cyberdcli cyberd/cyberd:<testnet_chain_id> 3. After successful container pulling and launch run to check if your node is connected to the testnet: docker exec cyberd cyberdcli status The possible output looks like this: {\"node_info\":{\"protocol_version\":{\"p2p\":\"6\",\"block\":\"9\",\"app\":\"0\"},\"id\":\"93b776d3eb3f3ce9d9bda7164bc8af3acacff7b6\",\"listen_addr\":\"tcp://0.0.0.0:26656\",\"network\":\"euler-1\",\"version\":\"0.29.1\",\"channels\":\"4020212223303800\",\"moniker\":\"anonymous\",\"other\":{\"tx_index\":\"on\",\"rpc_address\":\"tcp://0.0.0.0:26657\"}},\"sync_info\":{\"latest_block_hash\":\"686B4E65415D4E56D3B406153C965C0897D0CE27004E9CABF65064B6A0ED4240\",\"latest_app_hash\":\"0A1F6D260945FD6E926785F07D41049B8060C60A132F5BA49DD54F7B1C5B2522\",\"latest_block_height\":\"45533\",\"latest_block_time\":\"2019-02-01T09:49:19.771375108Z\",\"catching_up\":false},\"validator_info\":{\"address\":\"66098853CF3B61C4313DD487BA21EDF8DECACDF0\",\"pub_key\":{\"type\":\"tendermint/PubKeyEd25519\",\"value\":\"uZrCCdZTJoHE1/v+EvhtZufJgA3zAm1bN4uZA3RyvoY=\"},\"voting_power\":\"0\"}} Your node has started to sync. The syncing process you can see in the terminal. Open a new tab and run following command: docker logs cyberd --follow Or go to localhost:9000 and open logs at cyberd container: Syncing has started. Syncing time depends on your internet bandwidth, connection and blockchain height. As at 2019/02/03 syncing time approximately 15-20 minutes. Once you see in logs that blocks syncing for 1 second your node is synced. Additional information available by API endpoint at localhost:26657 f.e. the number of active validators available here localhost:26657/validators","title":"Cyberd fullnode launching"},{"location":"cyberd/run_validator/#validator-start","text":"After your node successful synced you can run validator.","title":"Validator start"},{"location":"cyberd/run_validator/#prepare-stake-address","text":"If you already have address with CYB and know seed phrase or private key just restore it into your local keystore. docker exec -ti cyberd cyberdcli keys add <your_key_name> --recover docker exec cyberd cyberdcli keys show <your_key_name> If you have been lucky enought and your Ethereum address has been included in genesis you can import ethereum private key Please, do not import high value Ethereum accounts. This can not be safe! cyberd software is a new software and is not battle tested yet. docker exec -ti cyberd cyberdcli keys add import_private <your_key_name> docker exec cyberd cyberdcli keys show <your_key_name> If you want to create new acccount use the command below. Also, you should send coins to that address to bound them later during validator submitting. docker exec -ti cyberd cyberdcli keys add <your_key_name> docker exec cyberd cyberdcli keys show <your_key_name> is any name you pick to represent this key pair. You have to refer to this later when you use the keys to sign transactions. It will ask you to enter your password twice to encrypt the key. You also need to enter your password when you use your key to sign any transaction. The command returns the address, public key and a seed phrase which you can use it to recover your account if you forget your password later. Keep the seed phrase in a safe place in case you have to use them. The address showing here is your account address. Let\u2019s call this . It stores your assets.","title":"Prepare stake address"},{"location":"cyberd/run_validator/#send-create-validator-transaction","text":"Validators are actors on the network committing new blocks by submitting their votes. It refers to the node itself, not a single person or a single account. Therefore, the public key here is referring to the node public key, not the public key of the address you have just created. To get the node public key, run the following command: docker exec cyberd cyberd tendermint show-validator It will return a bech32 public key. Let\u2019s call it . The next step you have to declare a validator candidate. The validator candidate is the account which stake the coins. So the validator candidate is an account this time. To declare a validator candidate, run the following command adjusting stake amount and other fields. docker exec -ti cyberd cyberdcli tx staking create-validator \\ --amount = 10000000cyb \\ --pubkey = <your_node_pubkey> \\ --moniker = <your_node_nickname> \\ --trust-node \\ --from = <your_key_name> \\ --commission-rate = \"0.10\" \\ --commission-max-rate = \"0.20\" \\ --commission-max-change-rate = \"0.01\" \\ --chain-id = <testnet_chain_id>","title":"Send create validator transaction"},{"location":"cyberd/run_validator/#verify-that-you-validating","text":"docker exec -ti cyberd cyberdcli query staking validators --trust-node = true If you see your <your_node_nickname> with status Bonded and Jailed false everything must be good. You are validating the network.","title":"Verify that you validating"},{"location":"cyberd/run_validator/#maintenance-of-validator","text":"","title":"Maintenance of validator"},{"location":"cyberd/run_validator/#jailing","text":"If your validator go under slashing conditions it first go to jail. After this event operator must unjail it manually. docker exec -ti cyberd cyberdcli tx slashing unjail --from = <your_key_name> --chain-id = <testnet_chain_id>","title":"jailing"},{"location":"cyberd/run_validator/#upgrading-of-validator","text":"Updating is easy as pulling the new docker container and launching it again docker stop cyberd docker rm cyberd docker pull cyberd/cyberd:<testnet_chain_id> docker run -d --name = cyberd --restart always --runtime = nvidia \\ -p 26656 :26656 -p 26657 :26657 -p 26660 :26660 \\ -v /root/cyberd:/root/.cyberd \\ -v /root/cyberdcli:/root/.cyberdcli \\ cyberd/cyberd:<testnet_chain_id> Don't forget to unjail if you was jailed during update.","title":"Upgrading of validator"},{"location":"cybernode/cybernode/","text":"Client for p2p node system","title":"About cybernode"},{"location":"cybernode/dev-setup/","text":"","title":"Dev-setup"},{"location":"cybernode/k8s-cheat-sheet/","text":"K8s cheat sheet \u00b6 k8s dashboard \u00b6 Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous Reset GKE Node \u00b6 gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko Forward pod port on localhost \u00b6 kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332 Elassandra commands \u00b6 Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"Cheat-sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-cheat-sheet","text":"","title":"K8s cheat sheet"},{"location":"cybernode/k8s-cheat-sheet/#k8s-dashboard","text":"Local dashboard proxy: kubectl proxy Get cluster access token: kubectl config view | grep -A10 \"name: $( kubectl config current-context ) \" | awk '$1==\"access-token:\"{print $2}' Get logs of previously running container (if it failed and then restarts): kubectl logs mypod --previous","title":"k8s dashboard"},{"location":"cybernode/k8s-cheat-sheet/#reset-gke-node","text":"gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko","title":"Reset GKE Node"},{"location":"cybernode/k8s-cheat-sheet/#forward-pod-port-on-localhost","text":"kubectl port-forward -n monitoring prometheus-kube-prometheus-0 9090 Forward port of first found pod with specific namespace and label: kubectl port-forward -n chains $( kubectl get pod -n chains -l chain = bitcoind-btc -o jsonpath = \"{.items[0].metadata.name}\" ) 8332","title":"Forward pod port on localhost"},{"location":"cybernode/k8s-cheat-sheet/#elassandra-commands","text":"Get nodes status: kubectl exec -n search elassandra-0 -- nodetool status Open cqlsh CLI tool kubectl exec -it -n search elassandra-0 -- cqlsh Dive into elassandra docker container shell(index stats, delete index commands examples) kubectl exec -n search -it elassandra-0 bash curl -XGET 'localhost:9200/_cat/indices?v&pretty' curl -XDELETE 'localhost:9200/twitter?pretty'","title":"Elassandra commands"},{"location":"cybernode/requirements/","text":"Requirements \u00b6 For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop First run initialization \u00b6 For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion? cybernode chains subcomands \u00b6 User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains cybernode p2p subcomands \u00b6 User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired apps|platforms subcomands \u00b6 User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities. cybernode status \u00b6 Should display to user cybernode running|stopped entities. Also should be available for all subcommands. cybernode help \u00b6 Should display help message to the user. Also should be available for all subcommands. cybernode settings \u00b6 Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"Requirements"},{"location":"cybernode/requirements/#requirements","text":"For near future(1.0 release) cybernode will act as a helping tool around docker images to instantiate p2p systems and app around them. For the first release, we consider to do next: 1 ) cybernode settings cybernode help 2 ) cybernode status 3 ) cybernode chains ethereum | ethereum_kovan | ethereum_classic start ( --light --autostartup ) cybernode chains ethereum stop cybernode chains bitcoin help cybernode chains bitcoin start ( --autostartup ) cybernode chains bitcoin stop cybernode chains status cybernide chains help 4 ) cybernode p2p ipfs start ( --autostartup ) cybernode p2p ipfs stop 5 ) cybernode apps | platforms cyb start cybernode apps | platforms cyb stop","title":"Requirements"},{"location":"cybernode/requirements/#first-run-initialization","text":"For first run of cybernode, user should specify where to store data for applications: cybernode.data.location = ~/.cybernode/data By default, all cybernode configs|state will be stored in ~/.cybernode path. Nonchangeble. Research: - [] Path autocompletion?","title":"First run initialization"},{"location":"cybernode/requirements/#cybernode-chains-subcomands","text":"User shoud be able to run chains with specified options. Research: - [] UI for chains - [] Light clients for chains","title":"cybernode chains subcomands"},{"location":"cybernode/requirements/#cybernode-p2p-subcomands","text":"User shoud be able to run p2p systems with specified options. Research: - [] Key pair managment if chain reuqired","title":"cybernode p2p subcomands"},{"location":"cybernode/requirements/#appsplatforms-subcomands","text":"User should be able to run DApps. Each DApp also have dependencies on p2p or chains entities. This subcommand should start only missing entities.","title":"apps|platforms subcomands"},{"location":"cybernode/requirements/#cybernode-status","text":"Should display to user cybernode running|stopped entities. Also should be available for all subcommands.","title":"cybernode status"},{"location":"cybernode/requirements/#cybernode-help","text":"Should display help message to the user. Also should be available for all subcommands.","title":"cybernode help"},{"location":"cybernode/requirements/#cybernode-settings","text":"Should print current settings. Research: - [] Possible solutions for autocomplete for MacOS, Linux systems - [] Language and framework for CLI - [] Package distribution for various platforms(brew, npm, deb etc) - [] Initial Message??","title":"cybernode settings"},{"location":"cybernode/chains/bitcoin/","text":"","title":"Bitcoin"},{"location":"cybernode/chains/ethereum/","text":"Known issues \u00b6 For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Ethereum"},{"location":"cybernode/chains/ethereum/#known-issues","text":"For 2806264 block there is a transaction with size more than 1mb, general rule, size of item can be >1mb.","title":"Known issues"},{"location":"cybernode/components/chain-nodes-components/","text":"Chain Nodes Components \u00b6 Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain-nodes"},{"location":"cybernode/components/chain-nodes-components/#chain-nodes-components","text":"Component Cluster Address Parity 1.9.6 eth parity-eth.chains.svc:8545 Parity 1.9.6 etc parity-et\u0441.chains.svc:8545 Bitcoind 0.16.0 bitcoind-btc.chains.svc:8332","title":"Chain Nodes Components"},{"location":"cybernode/components/components-requirments/","text":"Components requirements \u00b6 Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Requirements"},{"location":"cybernode/components/components-requirments/#components-requirements","text":"Resources requirements to run components in gcloud cluster Component Min CPU Max CPU Min RAM Max RAM Kafka Broker 1.5 2 10GB 10GB Kafka Exporter 0.1 0.2 1GB 2GB Kafka Manager 0.1 0.2 1GB 2GB Zoo Keeper 0.1 0.2 1.25GB 1.25GB Elassandra 3.5 3.5 55GB 55GB Grafana 0.1 0.2 100MB 200MB Prometheus 0.25 1 3GB 3GB Prometheus Operator 0.1 0.2 100MB 200MB Parity ETH 3 3 15GB 20GB Parity ETC 1 1.5 10GB 15GB ETH Pump 1.5 3 5GB 6GB ETH Cassandra Dump 1.5 3 3.75GB 3.75GB ETH Contract Summary 1.5 3 3.75GB 3.75GB ETC Pump 1.5 3 3.75GB 3.75GB ETC Cassandra Dump 1.5 3 3.75GB 3.75GB ETC Contract Summary 1.5 3 3.75GB 3.75GB Bitcoind 2 3 20GB 30GB BTC Pump* 2.5 2.5 22GB 25GB BTC Dump 3 3 3.75GB 3.75GB BTC Contract Summary 1.5 3 3.75GB 3.75GB Search Api 0.5 1 3.75GB 3.75GB Search Api Docs 0.1 0.2 100MB 200MB * - With cache size = 10GB","title":"Components requirements"},{"location":"cybernode/components/monitoring-components/","text":"Monitoring Components \u00b6 During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y Prometheus \u00b6 Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s). Prometheus Operator \u00b6 To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. Default Service Monitor \u00b6 To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters. Grafana \u00b6 To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards. Grafana Alerts \u00b6 Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Monitoring"},{"location":"cybernode/components/monitoring-components/#monitoring-components","text":"During lifetime cybernode collects various metrics using following components: Component Description Cluster Address External Prometheus Operator(PO) Manages Prometheus Configuration Prometheus Metrics Storage prometheus.monitoring.svc:9090 Default Service Monitor Default Service Monitor for PO Grafana Metrics Alerts and Web UI grafana.monitoring.svc:3000 y","title":"Monitoring Components"},{"location":"cybernode/components/monitoring-components/#prometheus","text":"Prometheus is used as a main metrics storage. Components collect metrics and expose them via HTTP endpoint, that Prometheus pulls at configured interval(our is 15s).","title":"Prometheus"},{"location":"cybernode/components/monitoring-components/#prometheus-operator","text":"To deploy Prometheus atop Kubernetes, we use Prometheus Operator . It consists of Prometheus Operator itself, that introduced to cluster CRDs : Prometheus, Service Monitor. PO automatically(dynamically) generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.","title":"Prometheus Operator"},{"location":"cybernode/components/monitoring-components/#default-service-monitor","text":"To enable dynamic mapping for new pods to prometheus, service monitor(SM) is used. Each SM defines set of kubernetes label filters.","title":"Default Service Monitor"},{"location":"cybernode/components/monitoring-components/#grafana","text":"To visualize metrics collected by Prometheus we use Grafana . In order to configure Grafana, three config maps are used: grafana-datasources . Define Prometheus datasource. grafana-dashboards . Gather predefined dashboards into single folder. grafana-dashboards-providers . Configuration for importing dashboards.","title":"Grafana"},{"location":"cybernode/components/monitoring-components/#grafana-alerts","text":"Also Grafana supports checking metrics satisfied defined criteria. If criteria is violated, than notification is raised to setup alert channels(in our case Telegram Group). To view list of available alerts check \"{grafana_host}/alerting/list\" page.","title":"Grafana Alerts"},{"location":"cybernode/staging/chains/","text":"Current chains nodes tables \u00b6 App data path port current size parity /cyberdata/parity 34545 174 gb parity-kovan /cyberdata/parity-kovan 34645 30 gb Local forwarding port for chains \u00b6 ssh -L 8545:localhost:34545 -L 8546:localhost:34546 earth@earth.cybernode.ai -p 33324 Commands used to run chain and live probe \u00b6 Parity \u00b6 Run: docker run -d -p 34546:8546 -p 34545:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:34545 Parity --chain kovan \u00b6 Run: docker run -d -p 34646:8546 -p 34645:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity-kovan --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on --chain kovan Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:34645","title":"Chains"},{"location":"cybernode/staging/chains/#current-chains-nodes-tables","text":"App data path port current size parity /cyberdata/parity 34545 174 gb parity-kovan /cyberdata/parity-kovan 34645 30 gb","title":"Current chains nodes tables"},{"location":"cybernode/staging/chains/#local-forwarding-port-for-chains","text":"ssh -L 8545:localhost:34545 -L 8546:localhost:34546 earth@earth.cybernode.ai -p 33324","title":"Local forwarding port for chains"},{"location":"cybernode/staging/chains/#commands-used-to-run-chain-and-live-probe","text":"","title":"Commands used to run chain and live probe"},{"location":"cybernode/staging/chains/#parity","text":"Run: docker run -d -p 34546:8546 -p 34545:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}'\\ http://127.0.0.1:34545","title":"Parity"},{"location":"cybernode/staging/chains/#parity-chain-kovan","text":"Run: docker run -d -p 34646:8546 -p 34645:8545 -v /cyberdata/parity:/cyberdata \\ -v /home/earth/.local/share/io.parity.ethereum/jsonrpc.ipc:/home/parity/.local/share/io.parity.ethereum/jsonrpc.ipc \\ --name parity-kovan --restart always --ipc=host parity/parity:stable \\ --db-compaction ssd --unsafe-expose --db-path /cyberdata --tracing=on --fat-db=on --chain kovan Probe: curl -X POST -H \"Content-Type: application/json\" \\ --data '{\"jsonrpc\":\"2.0\",\"method\":\"eth_getBlockByNumber\",\"params\":[\"0x1\", true],\"id\":1}' \\ http://127.0.0.1:34645","title":"Parity --chain kovan"},{"location":"cybernode/staging/continuous-delivery/","text":"Circle Ci job example \u00b6 To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all. Example: build and deploy for all commits in master \u00b6 aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"CD"},{"location":"cybernode/staging/continuous-delivery/#circle-ci-job-example","text":"To enable auto staging-redeploy for your application you should: Enable staging docker images build. For example, you can use dockerhub tag \"STAGING\". Add to the repo you image running script. Add a CircleCi job, depended on docker build&push job. It's all.","title":"Circle Ci job example"},{"location":"cybernode/staging/continuous-delivery/#example-build-and-deploy-for-all-commits-in-master","text":"aliases: - &staging_filter filters: tags: only: /.*/ branches: only: master jobs: build_and_push_image_staging_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - checkout - setup_remote_docker: version: 17.05.0-ce - run: name: Build and upload staging images command: | docker build -t build/cs-search-api -f ./devops/search-api/search-api ./ docker login -u $DOCKER_USER -p $DOCKER_PASS docker tag build/cs-search-api cybernode/cs-search-api:staging docker push cybernode/cs-search-api:staging deploy_stagin_image: working_directory: ~/build docker: - image: docker:17.05.0-ce-git steps: - run: name: Rerun image on staging command: >- ssh mars@staging.cyber.fund -p 33322 -o \"StrictHostKeyChecking no\" 'cd /cyberdata/cybernode && git pull && sh /cyberdata/cybernode/up.search.sh' workflows: version: 2 staging_cd: jobs: - build_and_push_image_staging_image: <<: *staging_filter - deploy_stagin_image: <<: *staging_filter requires: - build_and_push_image_staging_image","title":"Example: build and deploy for all commits in master"},{"location":"cybernode/staging/kubernetes/","text":"Prerequisites \u00b6 Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used. Running minikube for local testing \u00b6 $ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser... Run single container in a cluster \u00b6 $ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Kubernetes"},{"location":"cybernode/staging/kubernetes/#prerequisites","text":"Install kubectl for interacting with Kubernetes. Install minikube if you want to test setup locally. Make sure to install driver for your OS or else slow VirtualBox will be used.","title":"Prerequisites"},{"location":"cybernode/staging/kubernetes/#running-minikube-for-local-testing","text":"$ minikube start --vm-driver kvm2 Starting local Kubernetes v1.9.0 cluster... Starting VM... Getting VM IP address... Moving files into cluster... Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. Check status. $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.39.6 Run dashboard. $ minikube dashboard Opening kubernetes dashboard in default browser...","title":"Running minikube for local testing"},{"location":"cybernode/staging/kubernetes/#run-single-container-in-a-cluster","text":"$ kubectl run miniecho --image=k8s.gcr.io/echoserver:1.9 --port=8080 This adds deployment called miniecho to cluster, runs Docker echoserver image in it and tells docker to open port 8080. The port is not accessible until we create service . $ kubectl expose deployment miniecho --type=NodePort service \"miniecho\" exposed This makes miniecho service accessible from your machine on random port. $ minikube service list |-------------|----------------------|---------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|---------------------------| | default | kubernetes | No node port | | default | miniecho | http://192.168.39.6:30426 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.39.6:30000 | |-------------|----------------------|---------------------------|","title":"Run single container in a cluster"},{"location":"cybernode/staging/run/","text":"Port mapping \u00b6 Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api Docker data \u00b6 Run chains \u00b6 Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh Run portainer.io \u00b6 Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer Run components \u00b6 sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run"},{"location":"cybernode/staging/run/#port-mapping","text":"Ports that are not accessible from internet. Use ssh forwarding. 8332 - chain btc 18332 - chain bth 8545 - chain eth 18545 - chain etc 2181 - zookeper 9092 - kafka 9042 - elassandra search 9043 - elassandra markets 9200 - elastic rest search 9201 - elastic rest markets 9300 - elastic transport search 9301 - elastic transport markets Ports open for internet. 32001 - portainer 32002 - grafana (monitoring) 32500 - browser-ui 32600 - chaingear 1.0 api 32800 - markets rest api 32801 - markets stream api 32901 - search api","title":"Port mapping"},{"location":"cybernode/staging/run/#docker-data","text":"","title":"Docker data"},{"location":"cybernode/staging/run/#run-chains","text":"Chains should be run manually once and ideally should not require addition work. To run chains, execute: sudo bash /cyberdata/cybernode/chains/up.sh","title":"Run chains"},{"location":"cybernode/staging/run/#run-portainerio","text":"Run once docker run -d -p 32001:9000 --name portainer --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /cyberdata/portainer:/data portainer/portainer","title":"Run portainer.io"},{"location":"cybernode/staging/run/#run-components","text":"sudo bash /cyberdata/cybernode/up.browser.sh sudo bash /cyberdata/cybernode/up.chaingear.sh sudo bash /cyberdata/cybernode/up.ethereum.index.sh sudo bash /cyberdata/cybernode/up.search.api.sh Then go into each kafka , elassandra and run docker-compose up -d To access monitoring dashboard, run docker-compose up -d in monitoring as well.","title":"Run components"},{"location":"cybernode/staging/setup/","text":"Mars is our staging server. You may reuse its config for your own. Current Mars storage setup \u00b6 Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata /cyberdata contents \u00b6 /cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode Useful commands \u00b6 Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Setup"},{"location":"cybernode/staging/setup/#current-mars-storage-setup","text":"Name Type Size LABEL Mapping nvme0n1p1 ssd 1 tb SDD1 /cyberdata/ssd1tb nvme2n1p1 ssd 500 gb SDD2 /cyberdata/ssd05tb sdc1 hdd 3.5 tb HDD3 /cyberdata/elassandra-markets sdb1 hdd 3.5 tb HDD2 /cyberdata/elassandra-search sda1 hdd 3.5 tb HDD1 /backupsData sdd1 hdd 3.5 tb cyberdata /cyberdata","title":"Current Mars storage setup"},{"location":"cybernode/staging/setup/#cyberdata-contents","text":"/cyberdata/portainer/ - portainer data directory /cyberdata/cybernode/ - clone of repo https://github.com/cyberFund/cybernode","title":"/cyberdata contents"},{"location":"cybernode/staging/setup/#useful-commands","text":"Format partition: sudo mkfs -t ext4 -L SSD1 /dev/nvme0n1p1 sudo mkfs -t ext4 -L SSD2 /dev/nvme2n1p1 /etc/fstab addition lines: LABEL=cyberdata /cyberdata ext4 defaults 0 0 LABEL=SSD2 /cyberdata/ssd05tb ext4 nofail 0 0 LABEL=SSD1 /cyberdata/ssd1tb ext4 nofail 0 0 LABEL=HDD1 /backupsData ext4 defaults 0 0 LABEL=HDD2 /cyberdata/elassandra-search ext4 defaults 0 0 LABEL=HDD3 /cyberdata/elassandra-markets ext4 defaults 0 0 Copy data: sudo cp -R --verbose /backupsData/chain/eth/eth /cyberdata/ssd05tb/eth/ sudo cp -R --verbose /cyberdata/bitcoind /cyberdata/ssd05tb/bitcoind/","title":"Useful commands"},{"location":"\u0441haingear/Changelog/","text":"Changelog \u00b6 All major changes of chaingear will be documented in this file. Sprint 9: \u00b6 1. Started frontend migration to new architecture (spring 9 - initialial relealse contracts's epic) 2. Changed architecture: - Splitting Registry creation in two stages - creation (in Chaingear) and post-initialization with EntryCore's bytecode (in Registry via admin/token holder) - Splitting Entry creation in two states - creation/deletion in Registry (with token minting/burning) and updating/reading in EntryCore 3. Improved policies: - Improved policies for Chaingear - Improved policies for Registry 3. Added advanced tests: - Improved and added more tests to Chaingear (Registry Creation, Chaingear Settings, policies) - Improved and added more tests to Registry (EntryCore initialization, CRUD, policies, Registry settings) 4. Initial gas estimation: - First meaningfull estimates of gas consumption for Chaingear creation, Registry creation, Registry initialization (with user generated EntryCore bytecode), Entry adding 5. Refactoring: - Policies refactoring 5. Improved documentation: - Improved NatSpec 6. Iceboxing after Research and Refactoring: - Funding of Chaingear registries - Funding of Registry entries Sprint 7-8: \u00b6 1. Initial frontend with Web3 and Metamask/Truffle integration 2. Created initial version of contract of Chaingear/Registry 3. Initial research/integration of ERC721 to tokenize registries/entries in Chaingear/Registry 4. Initial realization of policies for Chaingear/Registry 5. Initial research/realization of cross-linking Registries tokens in Chaingear with side Registries smart-contracts","title":"Changelog"},{"location":"\u0441haingear/Changelog/#changelog","text":"All major changes of chaingear will be documented in this file.","title":"Changelog"},{"location":"\u0441haingear/Changelog/#sprint-9","text":"1. Started frontend migration to new architecture (spring 9 - initialial relealse contracts's epic) 2. Changed architecture: - Splitting Registry creation in two stages - creation (in Chaingear) and post-initialization with EntryCore's bytecode (in Registry via admin/token holder) - Splitting Entry creation in two states - creation/deletion in Registry (with token minting/burning) and updating/reading in EntryCore 3. Improved policies: - Improved policies for Chaingear - Improved policies for Registry 3. Added advanced tests: - Improved and added more tests to Chaingear (Registry Creation, Chaingear Settings, policies) - Improved and added more tests to Registry (EntryCore initialization, CRUD, policies, Registry settings) 4. Initial gas estimation: - First meaningfull estimates of gas consumption for Chaingear creation, Registry creation, Registry initialization (with user generated EntryCore bytecode), Entry adding 5. Refactoring: - Policies refactoring 5. Improved documentation: - Improved NatSpec 6. Iceboxing after Research and Refactoring: - Funding of Chaingear registries - Funding of Registry entries","title":"Sprint 9:"},{"location":"\u0441haingear/Changelog/#sprint-7-8","text":"1. Initial frontend with Web3 and Metamask/Truffle integration 2. Created initial version of contract of Chaingear/Registry 3. Initial research/integration of ERC721 to tokenize registries/entries in Chaingear/Registry 4. Initial realization of policies for Chaingear/Registry 5. Initial research/realization of cross-linking Registries tokens in Chaingear with side Registries smart-contracts","title":"Sprint 7-8:"},{"location":"\u0441haingear/contracts/","text":"Contracts Overview \u00b6 Design rationale \u00b6 Main design principle goes from ERC721 NFT tokenization of Registries (in Chaingear/Metaregistry) and Entries (in custom Registry). In reason of Registries tokenization in Chaingear which allows token holder acts as administrator of their Registry, Chaingear acts to Registry as owner, which sets holder as administrator on creation phase, changes administrator when holder transfers token to another user, and transfers ownership when user unregister Registry in Chaingear, giving them full control to contract. Registry deep-linked to Chaingear registry token. In other words, token ownership means control of Registry. Also, Chaingear supports multiple Registry Creators (fabrics of registries), and allows Chaingear owners provide different kind and versioning of Registries. In reason of providing user functionality to describe their custom registry data structures and CRUD operations, the user creates their custom smart-contract, which implements the EntryInterface interface. This contract acts as inner storage, defines schema, and Registry acts them on token operations (creating and deleting). A user may deploy erroneous or vulnerable EntryCore contract, but this should not crash Chaingear-Registry NFT-token logic and Registry inner entry NFT-logic too. Even if this happens Registry crashing should not affect Chaingear/metaregistry contract. We proceed from the premise that the creator of the registry (administrator) is positive and it does not make sense for them to break his registry by initializing it with an incorrect contract. This brings us to tokenized ( C RU D ) operations (and inner private in EntryCore) in Registry-EntryCore and public ( C R UD ) actions/direct EntroCore tokenized ( CR U D ) operations. Chaingear inheritance \u00b6 Registry inheritance \u00b6 PS: OZ stands to Open Zeppelin contracts \u00b6 /chaingear \u00b6 Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Tokenized registries Entries may collect funds by users. Chaingear supports multiple benefitiaries witch have access to collected fees. ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, adding multiple registry creator contracts based on versioning or/and functionality. RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the code of specified version of Registry. This code used by Chaingear for Registry creation process. Registry Creator should be added with specified version and description to Chaingear Registry Creators inner registry. Chaingear contract should be added as builder to Registry Creator with reason to allow creation calls only by Chaingear contact. /common \u00b6 RegistySafe allows creator contract transfer ETHs to them and claim from, accounting logic holded by owner contract. SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares. /registry \u00b6 Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, description, tags, entry base address. EntryInterface interface for EntryCore . Holds entry metainformation and interfaces of functions ( C RU D ) which should be implemented in EntryCore . Uses for interaction between Registry and EntryCore. EntryCore partially code-generated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then initializes in Registry by their creator (as admin) and completes Registry setup process. Provides public ( C RU D ) actions for users and inner ( C RU D ) tokenized actions for Registry. Registry goes as owner of contract (and acts as proxy) with entries creating, token-based transferring and deleting. Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create tokenized empty entries according to entry access policy setted in Registry. Registry provides tokenized ( C RU D ) actions, after creation of token and empty registry object, user should initialize them in EntryCore. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry token can claim funds. RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyAdministrator, AllUsers. Chaingear acts as owner of Registry and creator of registry acts of administrator with separated policies to Registry functions. EntryInterface interface (should be implemented in user EntryCore contract) \u00b6 contract EntryInterface { function entriesAmount () external view returns ( uint256 ); function createEntry () external returns ( uint256 ); function deleteEntry ( uint256 ) external ; } Example EntryCore (with example custom structure and required functions) \u00b6 pragma solidity 0.4.24 ; import \"../common/EntryInterface.sol\" ; import \"openzeppelin-solidity/contracts/ownership/Ownable.sol\" ; //This is Example of EntryCore contract EntryCore is EntryInterface , Ownable { struct Entry { address expensiveAddress ; uint256 expensiveUint ; int128 expensiveInt ; string expensiveString ; } mapping ( string => bool ) internal entryExpensiveStringIndex ; Entry [] internal entries ; function () external {} function createEntry () external onlyOwner returns ( uint256 ) { Entry memory m = ( Entry ( { expensiveAddress : address ( 0 ), expensiveUint : uint256 ( 0 ), expensiveInt : int128 ( 0 ), expensiveString : \"\" })); uint256 newEntryID = entries . push ( m ) - 1 ; return newEntryID ; } function updateEntry ( uint256 _entryID , address _newAddress , uint256 _newUint , int128 _newInt , string _newString ) external { require ( owner . call ( bytes4 ( keccak256 ( \"checkAuth(uint256, address)\" )), _entryID , msg . sender )); // for uniq check example require ( entryExpensiveStringIndex [ _newString ] == false ); Entry memory m = ( Entry ({ expensiveAddress : _newAddress , expensiveUint : _newUint , expensiveInt : _newInt , expensiveString : _newString })); entries [ _entryID ] = m ; // for uniq check example entryExpensiveStringIndex [ _newString ] = true ; require ( owner . call ( bytes4 ( keccak256 ( \"updateEntryTimestamp(uint256)\" )), _entryID )); } function deleteEntry ( uint256 _entryIndex ) external onlyOwner { uint256 lastEntryIndex = entries . length - 1 ; Entry storage lastEntry = entries [ lastEntryIndex ]; entries [ _entryIndex ] = lastEntry ; delete entries [ lastEntryIndex ]; entries . length -- ; } function entriesAmount () external view returns ( uint256 entryID ) { return entries . length ; } function entryInfo ( uint256 _entryID ) external view returns ( address , uint256 , int128 , string ) { return ( entries [ _entryID ]. expensiveAddress , entries [ _entryID ]. expensiveUint , entries [ _entryID ]. expensiveInt , entries [ _entryID ]. expensiveString ); } }","title":"Contracts"},{"location":"\u0441haingear/contracts/#contracts-overview","text":"","title":"Contracts Overview"},{"location":"\u0441haingear/contracts/#design-rationale","text":"Main design principle goes from ERC721 NFT tokenization of Registries (in Chaingear/Metaregistry) and Entries (in custom Registry). In reason of Registries tokenization in Chaingear which allows token holder acts as administrator of their Registry, Chaingear acts to Registry as owner, which sets holder as administrator on creation phase, changes administrator when holder transfers token to another user, and transfers ownership when user unregister Registry in Chaingear, giving them full control to contract. Registry deep-linked to Chaingear registry token. In other words, token ownership means control of Registry. Also, Chaingear supports multiple Registry Creators (fabrics of registries), and allows Chaingear owners provide different kind and versioning of Registries. In reason of providing user functionality to describe their custom registry data structures and CRUD operations, the user creates their custom smart-contract, which implements the EntryInterface interface. This contract acts as inner storage, defines schema, and Registry acts them on token operations (creating and deleting). A user may deploy erroneous or vulnerable EntryCore contract, but this should not crash Chaingear-Registry NFT-token logic and Registry inner entry NFT-logic too. Even if this happens Registry crashing should not affect Chaingear/metaregistry contract. We proceed from the premise that the creator of the registry (administrator) is positive and it does not make sense for them to break his registry by initializing it with an incorrect contract. This brings us to tokenized ( C RU D ) operations (and inner private in EntryCore) in Registry-EntryCore and public ( C R UD ) actions/direct EntroCore tokenized ( CR U D ) operations.","title":"Design rationale"},{"location":"\u0441haingear/contracts/#chaingear-inheritance","text":"","title":"Chaingear inheritance"},{"location":"\u0441haingear/contracts/#registry-inheritance","text":"","title":"Registry inheritance"},{"location":"\u0441haingear/contracts/#ps-oz-stands-to-open-zeppelin-contracts","text":"","title":"PS: OZ stands to Open Zeppelin contracts"},{"location":"\u0441haingear/contracts/#chaingear","text":"Chaingear allows any user to create his own registry. Building fee is collecting by new registry creation. All builded registries are tokenized with ERC721 NFT token standard and saved in Chaingear metaregistry with registry metainformation. Creator of registry may transfer tokenized ownership of registry and destroy registry with token burning. Tokenized registries Entries may collect funds by users. Chaingear supports multiple benefitiaries witch have access to collected fees. ChaingearCore holds general logic of Chaingear. Allows change chaingear's metainformation, amount of registration fee, adding multiple registry creator contracts based on versioning or/and functionality. RegistryBase holds struct of data with describes registry metainformation which associated with token, views function for registry metainformation. RegistryCreator contains the code of specified version of Registry. This code used by Chaingear for Registry creation process. Registry Creator should be added with specified version and description to Chaingear Registry Creators inner registry. Chaingear contract should be added as builder to Registry Creator with reason to allow creation calls only by Chaingear contact.","title":"/chaingear"},{"location":"\u0441haingear/contracts/#common","text":"RegistySafe allows creator contract transfer ETHs to them and claim from, accounting logic holded by owner contract. SplitPaymentChangeable allows add beneficiaries to contract (addresses and shares amount) and change payee address. Beneficiaries can claim ETHs from contract proportionally to their shares.","title":"/common"},{"location":"\u0441haingear/contracts/#registry","text":"Chaingeareable holds basic logic of Registry as registry basic information, balance and fees amount. Contains getters and setters for registry name, description, tags, entry base address. EntryInterface interface for EntryCore . Holds entry metainformation and interfaces of functions ( C RU D ) which should be implemented in EntryCore . Uses for interaction between Registry and EntryCore. EntryCore partially code-generated contract where registry creator setup their custom entry structure and setters/getters. EntryCore then initializes in Registry by their creator (as admin) and completes Registry setup process. Provides public ( C RU D ) actions for users and inner ( C RU D ) tokenized actions for Registry. Registry goes as owner of contract (and acts as proxy) with entries creating, token-based transferring and deleting. Registry contract witch tokenize entries as NFT tokens via ERC721 standard. Users can create tokenized empty entries according to entry access policy setted in Registry. Registry provides tokenized ( C RU D ) actions, after creation of token and empty registry object, user should initialize them in EntryCore. Also users can fund entries with ETHs which send to RegistrySafe where owner of entry token can claim funds. RegistryAccessControl holds logic of controlling registry and accessing to entries creation. Policy options to entries creation are OnlyAdministrator, AllUsers. Chaingear acts as owner of Registry and creator of registry acts of administrator with separated policies to Registry functions.","title":"/registry"},{"location":"\u0441haingear/contracts/#entryinterface-interface-should-be-implemented-in-user-entrycore-contract","text":"contract EntryInterface { function entriesAmount () external view returns ( uint256 ); function createEntry () external returns ( uint256 ); function deleteEntry ( uint256 ) external ; }","title":"EntryInterface interface (should be implemented in user EntryCore contract)"},{"location":"\u0441haingear/contracts/#example-entrycore-with-example-custom-structure-and-required-functions","text":"pragma solidity 0.4.24 ; import \"../common/EntryInterface.sol\" ; import \"openzeppelin-solidity/contracts/ownership/Ownable.sol\" ; //This is Example of EntryCore contract EntryCore is EntryInterface , Ownable { struct Entry { address expensiveAddress ; uint256 expensiveUint ; int128 expensiveInt ; string expensiveString ; } mapping ( string => bool ) internal entryExpensiveStringIndex ; Entry [] internal entries ; function () external {} function createEntry () external onlyOwner returns ( uint256 ) { Entry memory m = ( Entry ( { expensiveAddress : address ( 0 ), expensiveUint : uint256 ( 0 ), expensiveInt : int128 ( 0 ), expensiveString : \"\" })); uint256 newEntryID = entries . push ( m ) - 1 ; return newEntryID ; } function updateEntry ( uint256 _entryID , address _newAddress , uint256 _newUint , int128 _newInt , string _newString ) external { require ( owner . call ( bytes4 ( keccak256 ( \"checkAuth(uint256, address)\" )), _entryID , msg . sender )); // for uniq check example require ( entryExpensiveStringIndex [ _newString ] == false ); Entry memory m = ( Entry ({ expensiveAddress : _newAddress , expensiveUint : _newUint , expensiveInt : _newInt , expensiveString : _newString })); entries [ _entryID ] = m ; // for uniq check example entryExpensiveStringIndex [ _newString ] = true ; require ( owner . call ( bytes4 ( keccak256 ( \"updateEntryTimestamp(uint256)\" )), _entryID )); } function deleteEntry ( uint256 _entryIndex ) external onlyOwner { uint256 lastEntryIndex = entries . length - 1 ; Entry storage lastEntry = entries [ lastEntryIndex ]; entries [ _entryIndex ] = lastEntry ; delete entries [ lastEntryIndex ]; entries . length -- ; } function entriesAmount () external view returns ( uint256 entryID ) { return entries . length ; } function entryInfo ( uint256 _entryID ) external view returns ( address , uint256 , int128 , string ) { return ( entries [ _entryID ]. expensiveAddress , entries [ _entryID ]. expensiveUint , entries [ _entryID ]. expensiveInt , entries [ _entryID ]. expensiveString ); } }","title":"Example EntryCore (with example custom structure and required functions)"},{"location":"\u0441haingear/development/","text":"Configuring, development and deploying \u00b6 Development environment \u00b6 Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts change imports this way: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\" ; Truffle + Ganache workflow \u00b6 Install Ganache (with UI) from latest release or npm package => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 ( in first tab/or run UI-client ) truffle migrate --network development --reset ( in second tab ) truffle console --network development ( in second tab ) Creating new registry/CRU entries (truffle console way/no Remix IDE) \u00b6 var chaingear = Chaingear.at ( Chaingear.address ) var beneficiaries = [] var shares = [] var buildingFee = 100000 var gas = 10000000 chaingear.registerRegistry ( \"V1\" , beneficiaries, shares, \"BlockchainRegistry\" , \"BLR\" , { value: 100000 , gas: 10000000 }) var registryAddress = chaingear.registryInfo.call ( 0 ) var registry = Registry.at ( 'insert_registry_address_here' ) registry.initializeRegistry ( \"IPFS_HASH\" , EntryCore.bytecode ) registry.createEntry () var entryCoreAddress = registry.getEntriesStorage () var entryCore = EntryCore.at ( 'insert_entry_core_address_here' ) entryCore.updateEntry ( 0 , '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , 256 , -127, \"helloworld\" ) entryCore.entryInfo ( 0 ) // --->>> [ '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , BigNumber { s: 1 , e: 2 , c: [ 256 ] } , BigNumber { s: -1, e: 2 , c: [ 127 ] } , 'helloworld' ] Linting: \u00b6 npm install -g solium solium -d contracts Testing: \u00b6 truffle test PS: script will run separate tests flow for each file, also produce gas report. Temp solution, for while we don't fix problem with tests falls when running for all files at one time what in truffle by default. Deploying (for example kovan): \u00b6 parity ui --chain = kovan truffle migrate --network = kovan PS: approve transaction in parity ui ( http://127.0.0.1:8180/ ) (Optional) Build contract in file: \u00b6 truffle-flattener contracts/registry/Registry.sol > Registry_full.sol","title":"Development"},{"location":"\u0441haingear/development/#configuring-development-and-deploying","text":"","title":"Configuring, development and deploying"},{"location":"\u0441haingear/development/#development-environment","text":"Recommending to use Remix Ethereum Online IDE or desktop electron-based Remix IDE PS: to import to IDE open-zeppelin contacts change imports this way: import \"github.com/openZeppelin/zeppelin-solidity/contracts/ownership/Ownable.sol\" ;","title":"Development environment"},{"location":"\u0441haingear/development/#truffle-ganache-workflow","text":"Install Ganache (with UI) from latest release or npm package => npm install -g ganache-cli Configure development config in truffle.js and launch Ganache (configure them too if needed) and: ganache-cli -p 7545 ( in first tab/or run UI-client ) truffle migrate --network development --reset ( in second tab ) truffle console --network development ( in second tab )","title":"Truffle + Ganache workflow"},{"location":"\u0441haingear/development/#creating-new-registrycru-entries-truffle-console-wayno-remix-ide","text":"var chaingear = Chaingear.at ( Chaingear.address ) var beneficiaries = [] var shares = [] var buildingFee = 100000 var gas = 10000000 chaingear.registerRegistry ( \"V1\" , beneficiaries, shares, \"BlockchainRegistry\" , \"BLR\" , { value: 100000 , gas: 10000000 }) var registryAddress = chaingear.registryInfo.call ( 0 ) var registry = Registry.at ( 'insert_registry_address_here' ) registry.initializeRegistry ( \"IPFS_HASH\" , EntryCore.bytecode ) registry.createEntry () var entryCoreAddress = registry.getEntriesStorage () var entryCore = EntryCore.at ( 'insert_entry_core_address_here' ) entryCore.updateEntry ( 0 , '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , 256 , -127, \"helloworld\" ) entryCore.entryInfo ( 0 ) // --->>> [ '0xa2f0dde51cb715f9cc7c12763fef90270bd50f70' , BigNumber { s: 1 , e: 2 , c: [ 256 ] } , BigNumber { s: -1, e: 2 , c: [ 127 ] } , 'helloworld' ]","title":"Creating new registry/CRU entries (truffle console way/no Remix IDE)"},{"location":"\u0441haingear/development/#linting","text":"npm install -g solium solium -d contracts","title":"Linting:"},{"location":"\u0441haingear/development/#testing","text":"truffle test PS: script will run separate tests flow for each file, also produce gas report. Temp solution, for while we don't fix problem with tests falls when running for all files at one time what in truffle by default.","title":"Testing:"},{"location":"\u0441haingear/development/#deploying-for-example-kovan","text":"parity ui --chain = kovan truffle migrate --network = kovan PS: approve transaction in parity ui ( http://127.0.0.1:8180/ )","title":"Deploying (for example kovan):"},{"location":"\u0441haingear/development/#optional-build-contract-in-file","text":"truffle-flattener contracts/registry/Registry.sol > Registry_full.sol","title":"(Optional) Build contract in file:"},{"location":"\u0441haingear/overview/","text":"Overview \u00b6 This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation. Features \u00b6 Chaingear \u00b6 Metaregistry with Registries entries, where each entry are ERC721 token Fee-based Registry creation Creating Registries with different functionality Token-based ownership/administration for Registry Funding in ETH for Registries Custom registry \u00b6 Custom data structure for Registry (EntryCore) Each Entry is ERC721 token Fee-based Entry creation Token-based ownership Entry management Entry creation policies (Administrator, Whitelist, AllUsers) Chaingear UI (browser/stand-alone web3 DApp) \u00b6 Web3/Metamask/Truffle/IPFS based Full Chaingear control interface Full custom Registry control interface Simple smart-contract EntryCore code generation in client Registries ABI and metainformation savings in IPFS","title":"Overview"},{"location":"\u0441haingear/overview/#overview","text":"This project allows you to create your own Registry of general purpose entries on Ethereum blockchain. Entry type is defined during creation, so you can put into entry any custom logic you want (validation, entry-level permission control). Entries are tokenized as NFT. Your creating your registry in Chaingear - metaregistry, which are one point of access to all registries. Registries on chaingear level are tokenized as NFT. Chaingear is most expensive registry, so you should pay for your registry creation.","title":"Overview"},{"location":"\u0441haingear/overview/#features","text":"","title":"Features"},{"location":"\u0441haingear/overview/#chaingear","text":"Metaregistry with Registries entries, where each entry are ERC721 token Fee-based Registry creation Creating Registries with different functionality Token-based ownership/administration for Registry Funding in ETH for Registries","title":"Chaingear"},{"location":"\u0441haingear/overview/#custom-registry","text":"Custom data structure for Registry (EntryCore) Each Entry is ERC721 token Fee-based Entry creation Token-based ownership Entry management Entry creation policies (Administrator, Whitelist, AllUsers)","title":"Custom registry"},{"location":"\u0441haingear/overview/#chaingear-ui-browserstand-alone-web3-dapp","text":"Web3/Metamask/Truffle/IPFS based Full Chaingear control interface Full custom Registry control interface Simple smart-contract EntryCore code generation in client Registries ABI and metainformation savings in IPFS","title":"Chaingear UI (browser/stand-alone web3 DApp)"},{"location":"\u0441haingear/pipelines/","text":"General Chaingear/Registry pipeline \u00b6 Registry CRUD/tokenized Entry/Funds pipeline \u00b6 Chaingear tokenized Registries pipeline \u00b6","title":"Pipelines"},{"location":"\u0441haingear/pipelines/#general-chaingearregistry-pipeline","text":"","title":"General Chaingear/Registry pipeline"},{"location":"\u0441haingear/pipelines/#registry-crudtokenized-entryfunds-pipeline","text":"","title":"Registry CRUD/tokenized Entry/Funds pipeline"},{"location":"\u0441haingear/pipelines/#chaingear-tokenized-registries-pipeline","text":"","title":"Chaingear tokenized Registries pipeline"}]}